1
00:00:00,000 --> 00:00:15,600
Hey everyone. Welcome to the Drive podcast. I'm your host, Peter Attia. This podcast,

2
00:00:15,600 --> 00:00:19,360
my website, and my weekly newsletter all focus on the goal of translating the science of

3
00:00:19,360 --> 00:00:24,360
longevity into something accessible for everyone. Our goal is to provide the best content in

4
00:00:24,360 --> 00:00:28,440
health and wellness, full stop, and we've assembled a great team of analysts to make

5
00:00:28,440 --> 00:00:32,720
this happen. If you enjoy this podcast, we've created a membership program that brings you

6
00:00:32,720 --> 00:00:36,920
far more in-depth content if you want to take your knowledge of this space to the next level.

7
00:00:36,920 --> 00:00:41,560
At the end of this episode, I'll explain what those benefits are, or if you want to learn more now,

8
00:00:41,560 --> 00:00:48,080
head over to PeterAttiaMD.com forward slash subscribe. Now, without further delay, here's

9
00:00:48,080 --> 00:00:55,160
today's episode. I guess this week is Dr. Eric Topol. Eric is a very famous cardiologist,

10
00:00:55,160 --> 00:01:01,000
geneticist, and digital medicine researcher slash pioneer. He's the founder and director

11
00:01:01,000 --> 00:01:05,480
of the Scripps Research Translational Institute, TSRI. Prior to coming to Scripps,

12
00:01:05,480 --> 00:01:09,720
he served as the chairman of cardiovascular medicine at the Cleveland Clinic, a post he

13
00:01:09,720 --> 00:01:16,640
held for about 15 years. We actually start the interview by talking about the story that led

14
00:01:16,640 --> 00:01:20,920
to him leaving the Cleveland Clinic. Actually, we spend quite a bit of time on this story,

15
00:01:20,920 --> 00:01:26,920
which is something that I certainly remember following during its unfolding in the early

16
00:01:26,920 --> 00:01:32,160
part of the 2000s. He's also the editor-in-chief of Medscape, and in 2012, he published a book

17
00:01:32,160 --> 00:01:38,320
called The Creative Destruction of Medicine. What we talk about today, though, is his, I believe,

18
00:01:38,320 --> 00:01:42,160
his third book, but it could be his fourth, called Deep Medicine. This is something I've been wanting

19
00:01:42,160 --> 00:01:48,360
to talk with Eric about for some time, because it really goes into, in a non-sci-fi way, the

20
00:01:48,360 --> 00:01:54,000
application of artificial intelligence, deep learning, machine learning in medicine, which

21
00:01:54,000 --> 00:02:00,560
is, as you probably realize, a field that is at times upsettingly slow to adopt to technical change.

22
00:02:00,560 --> 00:02:05,460
We talk about a lot of things in this episode, and in a surprisingly brief period of time for one

23
00:02:05,460 --> 00:02:09,680
of my podcasts, actually, we talk a lot about the gut biome and actually have a great and

24
00:02:09,680 --> 00:02:13,760
spirited discussion about it, because as some of you may know, I'm kind of a skeptic of this

25
00:02:13,760 --> 00:02:18,560
whole gut biome is going to be the answer to all of our woes. But I think in the end, Eric and I

26
00:02:18,560 --> 00:02:23,840
really kind of end up being much more closely aligned in our views of the utility of this tool

27
00:02:23,840 --> 00:02:27,920
as a way to provide predictive insights. In fact, there are a number of things that I came out of

28
00:02:27,920 --> 00:02:33,760
this episode with some follow-up notes for myself as far as people I want to connect with, researchers

29
00:02:33,760 --> 00:02:38,960
that I want to connect with, to better understand how I can utilize that information for some of my

30
00:02:38,960 --> 00:02:43,640
own clinical interests. I think what comes across in the end of this discussion is that doctors

31
00:02:43,640 --> 00:02:50,000
aren't going anywhere. And in fact, Eric has a slightly contrarian view of what the impact of AI

32
00:02:50,000 --> 00:02:54,600
in medicine will be. He argues that it's not at all the doctors are going to go away, it's just the

33
00:02:54,600 --> 00:02:59,320
doctors are going to change their focus and frankly focus on the one thing that doctors and humans in

34
00:02:59,320 --> 00:03:04,920
general can do far better than machines. So with that said, I hope you enjoy my conversation with

35
00:03:04,920 --> 00:03:15,200
Dr. Eric Topol. Eric, thanks so much for coming over on a Friday afternoon. Great to be with you,

36
00:03:15,200 --> 00:03:20,960
Peter. This is kind of funny because we've both lived in San Diego for over 10 years and your name

37
00:03:20,960 --> 00:03:25,040
comes up all the time. Everybody says to me, you must know Eric Topol. And I say, well, of course,

38
00:03:25,040 --> 00:03:29,760
I know of him, but no, I don't know him. And where it really comes up is every time I'm at Dexcom.

39
00:03:30,640 --> 00:03:34,600
And obviously you're on the board there and I know Kevin Sayer very well and I met several

40
00:03:34,600 --> 00:03:38,400
folks on the team. So it's hard to believe this is the first time we're meeting.

41
00:03:38,400 --> 00:03:43,320
It is. I've heard a lot about you over the years, Peter. Now you mentioned Dexcom,

42
00:03:43,320 --> 00:03:50,400
I've been on their board for nine years and I've watched this early medical wireless world of

43
00:03:50,400 --> 00:03:55,880
sensors be transformed. So that's been really a privilege. Whereas most people think about

44
00:03:56,000 --> 00:04:03,360
steps as a wearable sensor and not glucose. So it's been a great company that started nine years

45
00:04:03,360 --> 00:04:09,320
ago, at least when I started with them, they were really having a rough time to get people with type

46
00:04:09,320 --> 00:04:12,640
one diabetes to use continuous glucose. And that's really changed a lot.

47
00:04:12,640 --> 00:04:19,040
Yeah, Kevin and I met about four years ago on an airplane. I still refer to it as certainly one of

48
00:04:19,040 --> 00:04:24,000
the top two luckiest seating assignments I ever had to be sitting next to Kevin and immediately

49
00:04:24,000 --> 00:04:28,280
clicked and I've never taken the sensor off since. You can see I'm wearing my G6 right now.

50
00:04:28,280 --> 00:04:29,080
Right. Wow.

51
00:04:29,080 --> 00:04:34,120
And I agree. I mean, it's sort of comical when people think about the number of steps one's

52
00:04:34,120 --> 00:04:39,520
taking as a quote unquote wearable or a valuable insight when you think about what could be

53
00:04:39,520 --> 00:04:43,680
measured in the interstitial fluid and glucose of course is just the thin end of the wedge on that.

54
00:04:43,680 --> 00:04:49,840
Exactly. I think a lot of people haven't realized how word this is headed. The more concern of course

55
00:04:49,840 --> 00:04:55,480
is using in the right people. That's like for example, the Apple watch for heart rhythm where

56
00:04:55,480 --> 00:05:02,080
so many people are using it and it's a recipe for false positives. But if you use these sort of

57
00:05:02,080 --> 00:05:05,600
things, these more advanced sensors properly, they can be really a big difference.

58
00:05:05,600 --> 00:05:12,560
You're one of the earliest adopters of mobile telemetry and mobile devices or outside of

59
00:05:12,560 --> 00:05:17,320
hospital devices, ambulatory devices to be able to measure heart rhythm. I wasn't even planning

60
00:05:17,320 --> 00:05:21,080
to ask you about that, but I can't resist at this moment. Give people a little bit of background

61
00:05:21,080 --> 00:05:24,520
about you. Obviously you're a cardiologist. We're going to talk about what you've done at the

62
00:05:24,520 --> 00:05:28,360
Cleveland Clinic and what you've done here at Scripps, but what interested you in cardiology

63
00:05:28,360 --> 00:05:32,400
in the first place? Well, it wasn't really cardiology that got me into medicine. It was

64
00:05:32,400 --> 00:05:39,240
the interest in endocrinology. My father had been a type one diabetic and had every complication you

65
00:05:39,240 --> 00:05:48,120
can imagine was blind by age 49. So I decided that since that seemed to be such a primitive area as

66
00:05:48,120 --> 00:05:53,840
far as no prevention and lack of treatments outside of insulin, that maybe that would be the way to go.

67
00:05:53,840 --> 00:06:00,080
And so when I went to UC San Francisco for my residency, it was really to get geared up to be

68
00:06:00,080 --> 00:06:06,760
a diabetologist. And what happened there was I was completely transfixed by what was going on in

69
00:06:06,760 --> 00:06:12,920
cardiology, particularly kind of strategy who's been a medical hero of mine, had a big influence

70
00:06:12,920 --> 00:06:19,480
in really changing the path. It was also a very remarkable time in that it was the first balloon

71
00:06:19,480 --> 00:06:24,960
angioplasty, the coronary arteries, the first clot dissolving therapy for heart attack, and so many

72
00:06:24,960 --> 00:06:30,120
other things that it was captivating. So I've never regretted that change, but it wasn't what

73
00:06:30,120 --> 00:06:37,520
I had initially in mind. The field of cardiology today is so specialized. To say that one would do

74
00:06:37,520 --> 00:06:42,880
a residency in medicine followed by a fellowship in cardiology would be as broad as doing a

75
00:06:42,880 --> 00:06:49,480
residency in general surgery today, where the field has maybe stated in another way, an

76
00:06:49,480 --> 00:06:55,880
interventional cardiologist versus a lipidologist would have virtually nothing in common outside of

77
00:06:55,880 --> 00:07:00,080
their foundational training in cardiology. I mean, I don't know that many people actually appreciate

78
00:07:00,080 --> 00:07:05,960
that outside of medicine. You know, you're really right, Peter. There's the sub specialties. It could

79
00:07:05,960 --> 00:07:12,560
be a heart failure or prevention or the plumbers, interventional or the electricians, electrophysiology,

80
00:07:12,560 --> 00:07:18,760
and on and on. So it's a very broad discipline. The field has matured so much in the last decade

81
00:07:18,760 --> 00:07:23,540
or two. There are obvious benefits to that. What do you think are the limitations of that

82
00:07:23,540 --> 00:07:29,580
stratification? Well, the major limitation is that you get this ice pick view of the patient.

83
00:07:29,580 --> 00:07:33,660
You know, when you see a patient as an interventional cardiologist thinking about what

84
00:07:33,660 --> 00:07:39,580
arteries can I fix and the electricians are thinking about the person as having an arrhythmia.

85
00:07:39,580 --> 00:07:46,380
So the general cardiologist, which is the group of people that would be the advocates to prevent

86
00:07:46,380 --> 00:07:51,780
unnecessary procedures, they don't get enough respect. It's just like primary cure internal

87
00:07:51,780 --> 00:07:57,820
medicine doctors. And so we really want to boost them up because they are the ones that are really

88
00:07:57,820 --> 00:08:03,900
caring for patients and looking out for their overall cardiovascular health. How long ago did

89
00:08:03,900 --> 00:08:12,180
you sort of get the sense that we didn't have to be in the hospital with a 12 lead EKG on a patient

90
00:08:12,180 --> 00:08:17,700
to appreciate what was happening in the conduction system of their heart? And I feel like I even

91
00:08:17,740 --> 00:08:23,980
remember seeing you on TV 10 years ago on CNN or Good Morning America for all I know. I honestly

92
00:08:23,980 --> 00:08:28,860
can't even remember. But you were in a clinic and you were sort of saying, look, there's going to be

93
00:08:28,860 --> 00:08:35,100
a day when a patient at home is going to wear a device and it's going to send me a warning sign

94
00:08:35,100 --> 00:08:40,660
that something is going on. Right. Well, it's interesting you bring that up because it was kind

95
00:08:40,660 --> 00:08:47,740
of a serendipitous connection to San Diego. So it was I think 1999, I had gotten to know the

96
00:08:47,740 --> 00:08:53,780
folks at Kleiner Perkins pretty well. And this guy, Brooke Byers contacted me and he said, you

97
00:08:53,780 --> 00:08:58,860
know, we're looking at this company called CardioNet. We don't really understand this thing

98
00:08:58,860 --> 00:09:05,380
about being able to do electrocardiogram monitoring over the internet. Could you look at this thing?

99
00:09:05,620 --> 00:09:12,700
I'm going to send you the slide deck. So it was, you know, 1999, 20 years ago, I looked at this thing,

100
00:09:12,700 --> 00:09:16,180
I said, whoa, this is an eye opener. And it was a San Diego based company.

101
00:09:16,180 --> 00:09:18,100
And you were still in Cleveland at the time.

102
00:09:18,100 --> 00:09:23,740
Yes, yes. And the whole idea was things were starting to really converge of the idea,

103
00:09:23,740 --> 00:09:30,300
at least of medicine and the internet. But the sense that you could monitor people remotely,

104
00:09:30,300 --> 00:09:35,460
continuously for multiple leads of their cardiogram was really exciting because up

105
00:09:35,460 --> 00:09:41,300
until that time, the only way we could do that was to put on one of these bulky Holter monitors.

106
00:09:41,300 --> 00:09:46,140
This is Norman Holter from the 1950 version. We still talk about his name.

107
00:09:46,140 --> 00:09:50,940
They're still using it. And you know, that is you can't exercise really,

108
00:09:50,940 --> 00:09:56,300
you can't take a shower, you know, it's you can't wear this too long. And it isn't real time.

109
00:09:56,300 --> 00:10:01,420
You then send it in and you know, get or you go back to the clinic and take it off.

110
00:10:01,420 --> 00:10:07,340
And so it's so antiquated when you think about it. So the idea that we could transcend that era

111
00:10:07,340 --> 00:10:12,380
with this mobile continuous monitoring. And by the way, there are people who have been on a

112
00:10:12,380 --> 00:10:17,140
Holter who died suddenly. And the Holter of course serves no purpose other than to tell you what

113
00:10:17,140 --> 00:10:21,860
arrhythmia killed them. Exactly. So now it's a whole different world. And then you start to wait,

114
00:10:21,860 --> 00:10:27,460
wait a minute, why don't we just do everything? Not just a cardiogram, we could do all the vital

115
00:10:27,460 --> 00:10:34,700
science. We get rid of hospitals or at least those hospital patients who are not in an intensive

116
00:10:34,700 --> 00:10:40,700
care unit. And I think that's where we're headed. That is 20 years ago was kind of the entry point

117
00:10:40,700 --> 00:10:46,940
with this first dedicated wireless company card unit. And it's just where we're going to build on

118
00:10:47,020 --> 00:10:53,340
to ultimately eradicate the need for most hospital rooms, which is a pretty big deal. And it's

119
00:10:53,340 --> 00:10:59,460
probably the most transformative aspect of where we're headed because that's the number one item

120
00:10:59,460 --> 00:11:05,980
for healthcare costs. It's not just the facilities, but the personnel. And so they account for a third

121
00:11:05,980 --> 00:11:12,980
of our $3.6 trillion annual healthcare budget. Medicine seems to always take longer to adopt

122
00:11:12,980 --> 00:11:18,780
things. I mean, I think in general, as people, we tend to have optimism that exceeds the pace at

123
00:11:18,780 --> 00:11:26,100
which technology moves forward. I mean, in some ways engineering examples get overused. We talk

124
00:11:26,100 --> 00:11:30,180
about the Manhattan Project, which is kind of remarkable when you really stop to think about it,

125
00:11:30,180 --> 00:11:36,420
that in such a short period of time, they could go from a proof of concept in the early 40s to a

126
00:11:36,420 --> 00:11:42,820
finished product in 44. Even the space race is kind of remarkable in terms of the accuracy with

127
00:11:42,820 --> 00:11:47,660
which they were able to sort of project and map out the steps. It doesn't seem that healthcare

128
00:11:47,660 --> 00:11:51,100
follows that curve. It doesn't just follow a straight Moore's law.

129
00:11:51,100 --> 00:11:59,660
Well, it actually defies Moore's law. If you plot that out, you look at, wow, the cost of chips has

130
00:11:59,660 --> 00:12:06,860
gotten so incredibly low over the course of 50 plus years and healthcare costs are going the opposite

131
00:12:06,860 --> 00:12:15,500
direction. So their lack of embracement of the digital era and also the lack of it having the

132
00:12:15,500 --> 00:12:23,540
impact of lowering costs is notable. It's palpable. It's a general resistance. I liken it to a sclerotic

133
00:12:23,540 --> 00:12:29,900
or ossified nature of the medical community, very resistant to change. The only time you see lack of

134
00:12:29,900 --> 00:12:36,660
resistance when it is tied to markedly improved reimbursement, for example, the adoption of robots

135
00:12:36,940 --> 00:12:44,820
like DaVinci or something like that. But otherwise there's just no real incentive to change. And of

136
00:12:44,820 --> 00:12:48,820
course we want to be careful because we don't want to adopt a significant change when it isn't

137
00:12:48,860 --> 00:12:56,260
validated or proven. But when we see things that are unquestionably advances and still unwillingness

138
00:12:56,260 --> 00:12:58,900
to move in that direction, that's disconcerting.

139
00:12:59,100 --> 00:13:04,620
Yeah, it is. And there are a few problems I've contemplated where no matter how much time and

140
00:13:04,620 --> 00:13:09,660
energy I put into it, I really can't even see the direction of the solution. I think there are some

141
00:13:09,660 --> 00:13:13,020
problems, even the political system, when you think about how broken our political system is, I don't

142
00:13:13,020 --> 00:13:17,220
think you, you don't have to be a student of political science to appreciate that. But I think

143
00:13:17,220 --> 00:13:22,220
most people who spend a lot of time thinking about it, if given a magic wand, would know how to move

144
00:13:22,220 --> 00:13:26,740
in the right direction, right? If you stop gerrymandering, if you maybe discarded the electoral

145
00:13:26,740 --> 00:13:32,860
college, like there are like five things that you could do structurally that would bring politics

146
00:13:32,860 --> 00:13:38,900
back into a sort of more civilized era. But if you said to me, wave a magic wand, what how do you fix

147
00:13:39,140 --> 00:13:46,700
medicine? The only idea I've ever had is one that involves changing behaviors directly, which of

148
00:13:46,700 --> 00:13:51,540
course becomes a bit of a tautology. But it seems to be that the disconnect between the driver of

149
00:13:51,540 --> 00:13:56,380
demand and the one who pays the bill is the biggest problem. Does that kind of resonate with you? In

150
00:13:56,380 --> 00:14:01,660
other words, in this system, which is not a budget driven healthcare system like it is in the UK, it's a

151
00:14:01,660 --> 00:14:08,380
demand driven system. So the system will rise to the cost of the demand. The demand is mostly driven

152
00:14:08,380 --> 00:14:12,820
by the patient and the physician, the patient requiring the care of the physician ordering the

153
00:14:12,820 --> 00:14:17,900
treatment, but they bear less than, I mean, they bear maybe I think the last thing I saw said about

154
00:14:17,900 --> 00:14:23,700
11% of the total cost is borne by those driving demand, which is sort of like you walking into a

155
00:14:23,700 --> 00:14:29,900
car dealership and knowing you only have to pay 11% of the car price, it's going to completely

156
00:14:29,900 --> 00:14:37,500
uncouple any reality. To me, that seems like the elephant in the room. And that's why I think the

157
00:14:37,500 --> 00:14:44,580
problem is, as you stated, which is why are hospitals so expensive? Well, if people actually saw the

158
00:14:44,580 --> 00:14:49,860
bill of, you know, a hospital stay and realized what the, you know, the gauze and the pillowcase

159
00:14:49,860 --> 00:14:54,300
were being charged for, I mean, they'd, they'd scream, but of course they don't have to pay it

160
00:14:54,300 --> 00:15:01,020
directly. They're only paying it indirectly. Right. It's a really messed up system in so many

161
00:15:01,020 --> 00:15:11,100
respects. You touched on one big one, but the basis is this absurd healthcare charges. It's just

162
00:15:11,100 --> 00:15:17,460
unfathomable. And all the things that have been done to date, like Obamacare and the debate now

163
00:15:17,460 --> 00:15:22,460
about Medicare for all or whatever, doesn't get to the root of the problem, which is the cost.

164
00:15:23,100 --> 00:15:28,220
That's right. It's politically in vogue to deal with access, which is an important problem.

165
00:15:28,220 --> 00:15:33,740
Absolutely. But if you expand access without reducing costs, you trade one bad problem for

166
00:15:33,740 --> 00:15:34,540
another bad problem.

167
00:15:34,660 --> 00:15:42,060
That was so educational for me because over the past year and a half, I worked with the NHS to

168
00:15:42,620 --> 00:15:49,700
review their health system, in particular, the impact of technology, AI, digital medicine,

169
00:15:49,700 --> 00:15:56,420
genomics. And as you already mentioned, Peter, they have a system which can be changed like a

170
00:15:56,420 --> 00:16:03,060
light switch. They don't have the single payer and they have far better outcomes than we do at

171
00:16:03,060 --> 00:16:10,420
about a third of the cost per person. And what's interesting is they have the will to make these

172
00:16:10,420 --> 00:16:16,100
changes. They're adopting things at a rate that is, there's no comparison to the US. Like for

173
00:16:16,180 --> 00:16:21,220
example, they already have places in the UK where they've gotten rid of keyboards instead of

174
00:16:21,220 --> 00:16:27,700
doctors typing and being data clerks. But they're also interested in making things more efficient

175
00:16:27,700 --> 00:16:32,980
than they already are more efficient than we are. But the difference is the incentives, just as you

176
00:16:32,980 --> 00:16:40,260
outlined, this is not employer based health care. This is not copay related. This is health care for

177
00:16:40,260 --> 00:16:47,140
everyone. And we're going to make it as good as we can and as least expensive as we can. So that

178
00:16:47,140 --> 00:16:52,900
country is in many respects a different model. Canada is like it and many countries in Europe

179
00:16:52,900 --> 00:16:58,020
are similar and we're so remotely disparate. It's just unfortunate.

180
00:16:58,660 --> 00:17:02,820
You know, I grew up in Canada, so I have sort of mixed feelings about the discussion of a single

181
00:17:02,820 --> 00:17:08,580
payer system because I've seen the advantages of it and you've outlined them. I don't know as much

182
00:17:08,580 --> 00:17:14,740
about the NHS, which may be better than in Canada. It's not national. It's done by each province,

183
00:17:14,740 --> 00:17:18,820
but it's still universal coverage within each province. But that said, my whole family is still

184
00:17:18,820 --> 00:17:27,460
in Canada. And I will say the following, when they get care, it seems to be pretty good. But boy,

185
00:17:27,460 --> 00:17:31,540
is it hard for them to get care sometimes. It depends what you need, right? If you need

186
00:17:32,580 --> 00:17:37,300
a coronary artery bypass and you have critical stenosis, you'll get excellent care in Toronto.

187
00:17:37,940 --> 00:17:42,180
It'll be no worse than it would be in the best hospital in New York or San Francisco.

188
00:17:43,220 --> 00:17:48,660
But if you have a torn ACL, it might take you six months to get that MRI.

189
00:17:49,380 --> 00:17:54,980
Now we could debate whether or not in the long run that matters as much, but I've always

190
00:17:55,780 --> 00:18:01,540
found it interesting that at least a country like Canada, and again, you can speak to the UK for me,

191
00:18:01,540 --> 00:18:06,100
there's great resistance to have a second layer of private insurance on top of the public

192
00:18:06,100 --> 00:18:13,140
that would allow that, which to me seems like the best hybrid solution, which is you have to have

193
00:18:13,140 --> 00:18:18,340
a safety net that provides universal coverage for everyone. But if an individual decides,

194
00:18:18,340 --> 00:18:22,500
look, I'm willing to pay an extra $10,000 a year, which by the way, is still a fraction of what I

195
00:18:22,500 --> 00:18:28,900
pay to ensure my family, you now have a separate queue that you can go into. And it's like Disneyland,

196
00:18:28,900 --> 00:18:32,260
right? It's like in Disneyland, you can do the special pass where you don't have to wait in line,

197
00:18:32,260 --> 00:18:37,140
you pay an extra whatever it is. Why do you think that places like Canada for sure,

198
00:18:37,140 --> 00:18:41,700
but maybe even the UK have resistance to secondary insurance?

199
00:18:41,700 --> 00:18:47,620
Well, they have secondary insurance. People have that in many respects, it works like you just

200
00:18:47,620 --> 00:18:48,340
described.

201
00:18:48,340 --> 00:18:52,260
I see. So the NHS has a second tier that one can buy privately.

202
00:18:52,260 --> 00:18:57,460
Yes. The main difference is there's a philosophy that if you're a citizen, as you said,

203
00:18:57,460 --> 00:18:59,220
healthcare is a right.

204
00:18:59,220 --> 00:19:00,340
It's right, not a privilege.

205
00:19:00,820 --> 00:19:07,300
Yeah. And then there are people that have this added insurance. It does separate a small

206
00:19:07,300 --> 00:19:14,580
fraction of people into this other class of getting access to more rapidly, to perhaps

207
00:19:14,580 --> 00:19:20,500
a different queue as you outlined it. But for the most part, it's a small proportion of the people

208
00:19:20,500 --> 00:19:27,300
in the UK. And I think it's somewhat similar in Canada. But I think the difference really is that

209
00:19:27,300 --> 00:19:34,260
there's two big poles of problems. If you look at in the US, there's the indigent who either don't

210
00:19:34,260 --> 00:19:39,060
have access or if they do, they're just not getting the kind of care that you would like to see.

211
00:19:39,700 --> 00:19:44,740
And then there's the affluent who get too much, they get overcooked, they get executive

212
00:19:44,740 --> 00:19:50,020
physicals and they get all this stuff that shouldn't be done. And the outgrowth of that

213
00:19:50,020 --> 00:19:55,780
is bad outcomes. They get incidental omens. You don't see that in the UK and in a lot of other

214
00:19:55,780 --> 00:20:02,100
countries. So we have this problem at both ends. And most of the recognition has been on the end

215
00:20:02,100 --> 00:20:08,340
of the underrepresented in indigent, not on the people who are getting overcooked. And that's a

216
00:20:08,340 --> 00:20:14,740
problem. I interviewed Marty Macquarie a little while ago, and he's written very eloquently about

217
00:20:14,740 --> 00:20:19,540
this problem, specifically with respect to pharmacotherapy. You mentioned executive

218
00:20:19,540 --> 00:20:24,580
physical. Well, your alma mater, of course, is one of the places that certainly has to be

219
00:20:24,580 --> 00:20:28,340
regarded as one of the finest hospital centers in the United States. And then by extension,

220
00:20:28,340 --> 00:20:33,700
the world. And I feel like I've had half a dozen patients come back from their executive

221
00:20:33,700 --> 00:20:38,500
physicals there or more so ask me if they should go and get it. So when did you, when did you get

222
00:20:38,500 --> 00:20:43,940
to Cleveland early nineties? Yeah, I got there in 91. And what was the Cleveland Clinic in 91?

223
00:20:44,660 --> 00:20:51,620
It wasn't as well known as it is now. It was particularly well known for bypass surgery.

224
00:20:52,340 --> 00:20:57,060
It was the place where Renee Favallaro essentially invented bypass surgery.

225
00:20:57,700 --> 00:21:03,380
And also Floyd Loop had really brightened that internal mammary artery, which was a big advance.

226
00:21:03,940 --> 00:21:08,900
It also had some other traditions. I mean, Mason Sones had been there, discovered coronary

227
00:21:08,900 --> 00:21:14,980
angiography. Was it considered at that time ahead of Minnesota, which was also arguably one of the

228
00:21:14,980 --> 00:21:19,220
earliest pioneers, you know, Stanford and Minnesota were really these huge pioneers in earlier

229
00:21:19,220 --> 00:21:24,180
cardiovascular medicine. Well, I think in cardiovascular, that was its signature contribution.

230
00:21:24,180 --> 00:21:28,660
I mean, obviously there were others, but it was little high. It of course was, you know,

231
00:21:28,660 --> 00:21:32,980
some way in little high, we're kind of these gods. Exactly. I think the main, you know,

232
00:21:32,980 --> 00:21:39,140
for carner disease, because there had been so many a cluster of remarkable innovations. That's

233
00:21:39,140 --> 00:21:46,260
where it had its biggest footprint. And when I went there, there was my far more bypass surgery

234
00:21:46,260 --> 00:21:51,460
done at Cleveland Clinic than anywhere in the world. And you had gone after UCSF training,

235
00:21:51,460 --> 00:21:55,860
you went to University of Michigan? Well, there was one stop in between. That was at Johns Hopkins,

236
00:21:55,860 --> 00:22:00,500
where I did cardiology. Oh, I don't think I knew that. Yeah. So we have this one overlap in our

237
00:22:00,500 --> 00:22:05,460
background. Yeah. And then I went to, I was seven years at University of Michigan. My first job,

238
00:22:05,460 --> 00:22:11,620
my second job at Cleveland Clinic, it was almost 14 years. But when I went there in 91, it wasn't

239
00:22:11,860 --> 00:22:19,860
a really strong academic center. It was, in fact, I'll never forget my chief of medicine at Michigan.

240
00:22:19,860 --> 00:22:23,700
When I told him I'm going to Cleveland Clinic, he said, well, that's the end of your academic

241
00:22:23,700 --> 00:22:29,060
career. It was viewed as almost going into private practice. It was viewed as well, high volume

242
00:22:29,060 --> 00:22:34,660
factory medicine, you know, high quality, but you know, did you have a strong residency fellowship

243
00:22:34,660 --> 00:22:39,220
system underneath you? Were you going to be heavily involved in training? There was a medicine

244
00:22:39,220 --> 00:22:43,460
fellowship system, but I don't know that I would qualify it as strong academically, you know,

245
00:22:43,460 --> 00:22:49,620
large, lots of them, but they weren't doing cutting edge research. And it wasn't that kind

246
00:22:49,620 --> 00:22:58,180
of scholarly environment. So my mission when I went there to refute the Yamada's view that it

247
00:22:58,180 --> 00:23:05,460
was the end of a career was actually to do just the opposite and enliven it and wake up the curiosity

248
00:23:05,460 --> 00:23:10,580
and the innovations. So that's what we did. And you know, it was a big transformation because it

249
00:23:10,580 --> 00:23:16,420
involved a whole new team, you know, bringing it was like an exchange transfusion because they

250
00:23:16,420 --> 00:23:22,420
hadn't written a paper in the cardiology division in a couple of years. Really? Yeah. It was very

251
00:23:22,420 --> 00:23:29,540
much dominated by cardiac surgery and it was just limited high productivity in academic side. So it

252
00:23:29,540 --> 00:23:33,140
really comes down to incentives again. I mean, today we see the opposite problem, of course,

253
00:23:33,220 --> 00:23:38,980
where we have the proliferation of total nonsense journals and absolute horrible things that don't

254
00:23:38,980 --> 00:23:44,580
pass for science being written constantly because of course the pendulum on the incentive is you have

255
00:23:44,580 --> 00:23:49,860
to publish. Right. And so presumably at Cleveland, that was simply not the pendulum was the exact

256
00:23:49,860 --> 00:23:55,060
opposite where you're, you were probably compensated based on clinical productivity and nothing more.

257
00:23:55,060 --> 00:23:59,300
Yeah. I mean, I think the cardiologist, when I talked to them, when I was interviewing and then

258
00:23:59,380 --> 00:24:04,260
when I got there, peached in, they said, you know, we're the handmaidens of the surgeons.

259
00:24:04,260 --> 00:24:08,420
And they were so busy taking care of the patients because the surgeons didn't really see the

260
00:24:08,420 --> 00:24:13,060
patients outside of the operating room and they needed this high volume of patients,

261
00:24:13,060 --> 00:24:16,580
need a lot of care and there weren't that many cardiologists. So the cardiologists would run the

262
00:24:16,580 --> 00:24:21,380
critical care and the step down units and everything. Everything. Wow. So they really were

263
00:24:21,380 --> 00:24:26,180
given great care, but they were consumed by that. So they didn't really have the time and

264
00:24:26,180 --> 00:24:31,940
nor did a lot of them since it was highly inbred then really have the knack of asking questions

265
00:24:31,940 --> 00:24:37,620
and chasing them down and whatnot. So we brought in a whole group. I mean, I started, there were 30

266
00:24:37,620 --> 00:24:42,340
cardiologists. When I left, there were over 90. Were you brought in as the chairman of cardiology?

267
00:24:42,340 --> 00:24:47,700
Right. Right. No, I was age 35. Actually, it was really funny, Peter, when you think about it,

268
00:24:47,700 --> 00:24:53,380
Bill Belichick and I started the same day. Bill Belichick was the youngest football coach in the

269
00:24:53,380 --> 00:24:58,500
NFL history head coach and I was the youngest chairman in the history of Cleveland Clinic. So

270
00:24:58,500 --> 00:25:02,340
we got to know each other a little bit. It was a very different era for Bill Belichick.

271
00:25:02,340 --> 00:25:07,060
Who's my favorite coach, by the way? Is that right? Oh, I mean, I'm obsessed with Bill Belichick.

272
00:25:08,020 --> 00:25:15,780
I met Tony Gonzalez recently. Oh, wow. And he told the absolute funniest story about his

273
00:25:15,780 --> 00:25:19,940
experience with Bill Belichick at the Pro Bowl one year, which I won't restate now, but in the

274
00:25:19,940 --> 00:25:26,340
show notes, we'll link to a video of Tony telling that story along with an article that was written

275
00:25:26,340 --> 00:25:31,700
up about it at some point. But I'm fundamentally just obsessed with with Belichick. Well, he's a

276
00:25:31,700 --> 00:25:36,660
really easy guy. It's a bucket list for me to meet him at some point. Wow. I know he's a kind of

277
00:25:36,660 --> 00:25:42,420
fascinating figure for many reasons. Actually, one of the most memorable things that happened

278
00:25:42,420 --> 00:25:48,820
regarding Bill Belichick was he benched Bernie Cozor. That didn't go over well. And Art Modell,

279
00:25:48,820 --> 00:25:53,300
who was the chairman of the board of Cleveland Clinic, we were good friends and we were for

280
00:25:53,300 --> 00:25:58,180
dinner at our house. This had all happened and it was an uproar, you know, with the dog pound and

281
00:25:58,180 --> 00:26:02,740
everything. And so Art and Pat were over and they said, you know, well, we had to put a sign in front

282
00:26:02,740 --> 00:26:10,340
of our house. Bill Belichick doesn't live here. But, you know, there was as much fear of what was

283
00:26:10,340 --> 00:26:16,500
happening then is when, you know, Art Modell moved the Browns to Baltimore. Yeah. So during those

284
00:26:16,500 --> 00:26:22,100
almost 14 years, it was great to see, you know, this kind of renaissance of- And you were supported.

285
00:26:22,100 --> 00:26:27,220
Yeah. I mean, because I mean, presumably you would not have left Michigan without an explicit

286
00:26:27,220 --> 00:26:32,180
understanding that you were not coming to implement the status quo. You were coming to rattle it.

287
00:26:32,180 --> 00:26:38,180
Exactly. And, you know, it was really because of Floyd, as we knew him, Fred Loop. He was a very

288
00:26:38,180 --> 00:26:43,140
progressive CEO of Cleveland Clinic. He, even though he'd been a cardiac surgeon throughout his

289
00:26:43,140 --> 00:26:48,900
career, and in fact, in the early years was still operating, he wanted to see cardiology thrive.

290
00:26:48,900 --> 00:26:54,580
He's not alive anymore. No, no. Unfortunately, he died of a rare cancer a few years back, but

291
00:26:54,580 --> 00:26:59,060
at a young age, because it was a surprise, he had such longevity in his family.

292
00:26:59,940 --> 00:27:04,900
He said, Eric, you know, I want you to come in and, you know, just completely get this place

293
00:27:04,900 --> 00:27:10,740
supercharged, make cardiology the greatest anywhere. And I'll back you a hundred percent.

294
00:27:10,740 --> 00:27:14,980
And not only that, but in 2000, in the year 2001, I was thinking about leaving

295
00:27:15,860 --> 00:27:20,580
actually to go to Stanford. He said, why do you want to go to Stanford medical school? Just start

296
00:27:20,580 --> 00:27:25,860
one here. And so that gave me the green light to work with Case Western to get a new medical

297
00:27:25,860 --> 00:27:32,180
school. And there hadn't been one in Cleveland or in the country for 26 years. So that was the show

298
00:27:32,180 --> 00:27:37,700
of Loop to be a great leader. I mean, he wasn't threatened by cardiology. He wasn't threatened

299
00:27:37,700 --> 00:27:44,180
by making it a far more academic environment. He actually saw those odds pluses. He was,

300
00:27:44,180 --> 00:27:48,020
you know, some extraordinary leader. We would have had that second overlap if you'd come to

301
00:27:48,020 --> 00:27:52,260
Stanford because I graduated from Stanford med school in 01. Oh, wow. When I was looking

302
00:27:52,260 --> 00:27:57,860
here to be the Dean, it was just after the divorce. It was like a low time morale.

303
00:27:57,860 --> 00:28:02,820
It was. So, so what was his name on blank? And there was a dermatologist who was the Dean,

304
00:28:03,700 --> 00:28:08,820
Eugene Bauer. Exactly. He was the one. He was the one that I think basically in the failure

305
00:28:08,820 --> 00:28:13,220
of that merger, it made sense that there was going to be a regime change. I don't know if

306
00:28:13,220 --> 00:28:18,340
Phyllis Gardner was ever in the running for it, but I always liked her. She was top drawer. I was

307
00:28:18,340 --> 00:28:25,380
super impressed by her, but a pediatrician. Yes. From Boston. And he was there for a number of years.

308
00:28:25,380 --> 00:28:28,180
And I know who you're talking about. Last name begins with the P I don't recall.

309
00:28:28,180 --> 00:28:33,140
Yeah. Pizzo Pizzo. Yeah. Yeah. Yeah. He had been in Boston infectious disease,

310
00:28:33,140 --> 00:28:39,380
pediatrics, and he was kind of opposite of the Stanford way. He was anti entrepreneurial,

311
00:28:39,380 --> 00:28:44,740
anti in many respects, innovation. So it was an interesting to see how that worked out. But

312
00:28:45,300 --> 00:28:50,580
before I had decided I didn't want to go there mainly. So the job you were potentially going

313
00:28:50,580 --> 00:28:54,100
to take a Stanford was to be the Dean, not to be the division chief of medicine or cardiology.

314
00:28:54,420 --> 00:28:59,940
Yeah. No. And I was at the time I thought it was a dream job. I thought Stanford, even though it

315
00:28:59,940 --> 00:29:06,260
was coming at a tough time in the wake of this UCSF, Stanford breakup, I thought, you know, hey,

316
00:29:06,260 --> 00:29:10,820
it's unilateral. It can only get better. Absolutely. But of course, knowing what I know,

317
00:29:10,820 --> 00:29:14,340
the little bit that I know about what it means to be the Dean of a hospital, it seems like that

318
00:29:14,340 --> 00:29:19,860
would have not allowed you to thrive in the way that you ended up finding a second home.

319
00:29:19,860 --> 00:29:24,020
That's an astute point, you know, you have to know what you're good at. And that might not

320
00:29:24,020 --> 00:29:29,300
have been a good fit in retrospect, but I was restless. I was looking for a change. And in fact,

321
00:29:29,860 --> 00:29:35,220
working on getting the new medical school at Cleveland, which we basically got in 2002,

322
00:29:35,220 --> 00:29:40,900
and the first class came in 2004, that kept me busy. I need, I always need a kind of big project

323
00:29:40,900 --> 00:29:47,140
to, to know something that's a reach to keep me going. And so that was important that it actually

324
00:29:47,140 --> 00:29:52,260
was a four year run to on top of the other things I was doing is to get that new med school off the

325
00:29:52,260 --> 00:29:57,060
ground. And there's something else that happened in the twilight of your career at Cleveland Clinic

326
00:29:57,060 --> 00:30:01,300
that I want to talk about, because it's on a personal level, it was very near and dear to me,

327
00:30:01,300 --> 00:30:07,140
which was your involvement in the uncovering or elucidation of the challenges with a medication

328
00:30:07,140 --> 00:30:15,220
called Vioxx. This is such an interesting example of it's a case study in so many things, right?

329
00:30:15,220 --> 00:30:20,100
Because I'll state for you at the outset, my bias in this entire story, and then I want to go into

330
00:30:20,100 --> 00:30:27,140
the story in detail. If I could go back in time and be czar for a month or a day or a year, I would

331
00:30:27,140 --> 00:30:32,100
have put a black box warning on Vioxx. I would have left it on the market for most people who

332
00:30:32,100 --> 00:30:37,140
could have tolerated it and made sure that it was very transparent that this is going to increase

333
00:30:37,140 --> 00:30:42,020
the risk of a subset of the population. And everybody's happy. Unfortunately, that's not

334
00:30:42,100 --> 00:30:50,020
what happened. Merck, I think in the hubris of wanting to deny that there was any potential

335
00:30:50,020 --> 00:30:56,340
patient subset that could be harmed by this drug ended up spending by my calculation at least four

336
00:30:56,340 --> 00:31:01,460
years, probably concealing data. You'll tell us the story and it may be longer. And in the end,

337
00:31:01,460 --> 00:31:06,980
a lot of people lost what I still consider to be probably the best Cox two inhibitor that was ever

338
00:31:06,980 --> 00:31:11,460
out there. So that's my bias. I could be wrong. I agree with everything you said.

339
00:31:11,860 --> 00:31:14,420
Totally. And we never discussed it. We never met before.

340
00:31:14,420 --> 00:31:19,060
No, no. So now let's talk about the story. So tell people what a Cox two inhibitor was and why was

341
00:31:19,060 --> 00:31:23,940
it such a big deal when these drugs came out in the late nineties? Well, this was at that time

342
00:31:23,940 --> 00:31:29,540
viewed as the most important blockbuster in medicine by oxen, Celebrex. They were competing

343
00:31:29,540 --> 00:31:34,500
with each other. They, I think, introduced right around 99 and it was a race because there's

344
00:31:34,500 --> 00:31:41,700
multi billions of dollars for each drug. The promise was instead of the Advil and Aleve and

345
00:31:41,700 --> 00:31:46,740
other nonsteroidals that they would replace, they would spare the stomach. They would be more potent

346
00:31:46,740 --> 00:31:52,660
to relieve pain and better anti-inflammatories. That was how they were built. And the reason

347
00:31:52,660 --> 00:31:57,780
because they were selective. So these cyclooxygenase enzymes that the Aleves of the world

348
00:31:57,780 --> 00:32:02,420
indiscriminately block them. And one of the problems is yes, you get the anti-inflammation

349
00:32:02,420 --> 00:32:08,340
that relieves your pain, but you also rip apart sort of the gastric lining and a whole bunch of

350
00:32:08,340 --> 00:32:12,580
other things in the wake. And of course, as you said, Celebrex and Vioxx came along and said,

351
00:32:12,580 --> 00:32:18,340
we're going to selectively target just cyclooxygenase two, which almost seemed too good to be true,

352
00:32:18,340 --> 00:32:24,020
by the way, in medicine, it doesn't often work that that happens, that you can selectively hit

353
00:32:24,020 --> 00:32:28,420
one of these two enzymes, but nevertheless, that was the, I mean, they did have some selectivity,

354
00:32:28,500 --> 00:32:34,180
but not as much as advertised, but nonetheless, I wasn't really paying attention to this because...

355
00:32:34,180 --> 00:32:37,540
Right. You're not a rheumatologist or an orthopedist. This is out of your wheelhouse.

356
00:32:37,540 --> 00:32:42,260
No, and I'm not even into drug safety. That was not the kind of thing I was into. And in fact,

357
00:32:42,260 --> 00:32:48,180
it was only because this remarkable fellow of ours, Deb Mukherjee, who now is the chief

358
00:32:48,180 --> 00:32:53,300
of cardiology in Texas. But at that time he came to me and said, Dr. Topol, I'm looking at this

359
00:32:53,300 --> 00:33:01,620
data from the FDA and what they're saying is that Vioxx is really not at all causing any heart

360
00:33:01,620 --> 00:33:07,460
problems. It's actually that the comparator in an approximate was the one that is decreasing.

361
00:33:07,460 --> 00:33:09,700
Providing a benefit. That was the argument.

362
00:33:09,700 --> 00:33:15,780
And I said, well, Deb, this FDA, they approved this drug, this is a year plus after they approved it.

363
00:33:15,780 --> 00:33:20,820
And I said, how did you get this data? Because back then, to get into the bowels of the FDA website

364
00:33:20,820 --> 00:33:25,940
wasn't so easy, but he did it on his own. So I give him credit. And I looked at it. First,

365
00:33:25,940 --> 00:33:28,420
I didn't believe it, but then we spent quite a bit of time.

366
00:33:28,420 --> 00:33:31,860
Now, do you remember the numbers? Because I remember that it was naproxen, but do you

367
00:33:31,860 --> 00:33:37,540
remember what the absolute risk difference was between naproxen and Vioxx in that first cohort?

368
00:33:37,540 --> 00:33:42,180
Yeah, there was this trial called VIGOR. I don't remember the exact numbers, but

369
00:33:42,900 --> 00:33:49,540
it was something like an excess of heart attacks in the Vioxx arm, Rofococcib,

370
00:33:50,180 --> 00:33:53,860
that was not trivial. If you look at it per hundred people.

371
00:33:53,860 --> 00:33:58,260
My recollection, I can be wrong, and we will link to all of this in great detail. So it'll be,

372
00:33:58,260 --> 00:34:01,780
for those of you listening, this will be completely accurate in the show notes.

373
00:34:01,780 --> 00:34:07,860
I want to say it was like 15 deaths per 10,000, but I don't remember what the baseline,

374
00:34:07,860 --> 00:34:09,220
I don't remember what the naproxen number was.

375
00:34:09,220 --> 00:34:10,900
Yeah, I think it was 15 per thousand.

376
00:34:10,980 --> 00:34:11,780
15 per thousand.

377
00:34:11,780 --> 00:34:16,420
Or up to 20 per thousand, depending on how you interpret the data, but there was a definite gap.

378
00:34:17,060 --> 00:34:21,860
And so first I was questioning Deb, and then we raked over the data and I said,

379
00:34:21,860 --> 00:34:23,460
you know what, you're onto something here.

380
00:34:23,460 --> 00:34:27,940
But at that time, Eric, did you think that naproxen provided cardio protection?

381
00:34:27,940 --> 00:34:30,340
No, actually, I said, where did that come from?

382
00:34:30,340 --> 00:34:32,500
Okay, so in other words, you sort of questioned the premise.

383
00:34:32,500 --> 00:34:36,980
Very peculiar, because there was no data to really support that. And it seemed like a very

384
00:34:36,980 --> 00:34:43,220
odd explanation for this excess of heart attacks. We went over the data and I said,

385
00:34:43,220 --> 00:34:47,060
you know what, we've got to publish this, because this is really important. And

386
00:34:47,700 --> 00:34:51,380
so we put together a paper and went to JAMA. It was published.

387
00:34:51,860 --> 00:34:53,700
This was 01, 02.

388
00:34:53,700 --> 00:34:57,940
No, it was published in 01. In fact, it was, I think, August 30th, 01.

389
00:34:57,940 --> 00:35:02,660
You know what, I remember it so well. I remember where I was reading it.

390
00:35:02,660 --> 00:35:04,420
Yeah, it was the summer of 01.

391
00:35:04,420 --> 00:35:08,180
It was on the front page of the Wall Street Journal. It was on a lot of other front pages,

392
00:35:08,180 --> 00:35:14,340
but there it quoted me as saying we could be facing a public health disaster.

393
00:35:15,140 --> 00:35:20,660
Now, did I ever know that that would be the case? Did I ever know that it would be three years later

394
00:35:20,660 --> 00:35:27,780
to the date, September 01, that Merck had this abrupt withdrawal? And in the process, by the way,

395
00:35:27,780 --> 00:35:34,100
Peter, before we published in JAMA, Merck came out to try to intimidate us, to withdraw the

396
00:35:34,100 --> 00:35:34,980
paper. Once they heard it-

397
00:35:34,980 --> 00:35:37,940
How did they know? Did the reviewers give it to them for comment?

398
00:35:37,940 --> 00:35:44,580
The reviewers apparently communicated to them that there was this hatchet job on Vioxx coming.

399
00:35:44,580 --> 00:35:49,060
And so they came to us and tried to intimidate us. And they also then, what I learned from the

400
00:35:49,060 --> 00:35:56,100
editor, then the editor, they tried to intimidate her, that they would sue JAMA and it was unfounded.

401
00:35:56,100 --> 00:35:58,820
And as soon as we published the paper and that wasn't-

402
00:35:58,820 --> 00:36:01,060
What did they actually say to you guys?

403
00:36:01,060 --> 00:36:06,660
Well, they said that we did data dredging. They had all the lines. They basically said that we

404
00:36:06,660 --> 00:36:07,780
were hacks and didn't hack-

405
00:36:07,780 --> 00:36:09,220
P-hacking data dredging, et cetera.

406
00:36:09,220 --> 00:36:14,500
Yeah. All we did was basically review the data that was filed on the FDA. And by the way,

407
00:36:15,220 --> 00:36:19,380
some of the things that didn't get out in the public, there were other small studies

408
00:36:20,020 --> 00:36:24,900
that never really got in the spotlight that also showed the excess of heart attack. So the signal-

409
00:36:24,900 --> 00:36:29,780
Did your JAMA study include a meta-analysis of those smaller ones as well as the original FDA-

410
00:36:29,780 --> 00:36:30,020
Yes.

411
00:36:30,100 --> 00:36:30,900
As the vigor trial.

412
00:36:30,900 --> 00:36:32,900
Exactly. And we saw this consistent-

413
00:36:32,900 --> 00:36:33,860
And you saw this pattern.

414
00:36:33,860 --> 00:36:39,780
It wasn't a question. And we also saw a lesser signal for Celebrex. It was in the paper. But

415
00:36:39,780 --> 00:36:46,660
the one that was just so consistent and you couldn't deny it was with Vioxx. And that was

416
00:36:46,660 --> 00:36:51,380
not just compared to naproxen, it was compared to other things and including placebo.

417
00:36:51,380 --> 00:36:57,540
And so your feeling at that point was the naproxen comparison is a red herring. And

418
00:36:57,540 --> 00:37:02,260
whether you're doing this against a placebo or Advil. And by the way, was there a belief at

419
00:37:02,260 --> 00:37:07,060
the time that just general ibuprofen had slight prevention or was neutral?

420
00:37:07,060 --> 00:37:07,540
Neutral.

421
00:37:07,540 --> 00:37:07,860
Neutral.

422
00:37:07,860 --> 00:37:12,980
Neutral at best. There wasn't any hint that naproxen afforded benefit or protection.

423
00:37:12,980 --> 00:37:15,300
So that whole premise was off base.

424
00:37:15,300 --> 00:37:19,140
And so we were talking about a difference of one in 100 and absolute risk.

425
00:37:19,140 --> 00:37:19,780
Two in 100.

426
00:37:19,780 --> 00:37:25,460
Two in 100. So one in 50 additional. And at that point in time, because I think later on,

427
00:37:25,460 --> 00:37:31,300
we knew more. But in 01, did you have a sense of which patients were the ones that were at risk?

428
00:37:31,300 --> 00:37:36,100
No, I think that we still don't know that who was at risk. We do know that 80 million people

429
00:37:36,100 --> 00:37:37,780
took Vioxx, which is a lot of people.

430
00:37:37,780 --> 00:37:42,900
Yeah. But it wasn't necessarily those with hypertension or those with dyslipidemia. I mean,

431
00:37:42,900 --> 00:37:45,220
were we able to sort of stratify it at all?

432
00:37:45,220 --> 00:37:49,220
No, in fact, that's the hardest thing is that when there were all these lawsuits of people that had

433
00:37:49,220 --> 00:37:53,860
heart attacks, you know, Merck defended it saying, well, it could have been their hyperlipidemia and

434
00:37:53,860 --> 00:37:59,300
their high blood pressure. And it's very hard, an individual person to ascribe to a hit to Vioxx.

435
00:38:00,100 --> 00:38:07,060
That's difficult because most people that have severe osteoarthritis are also having comorbidities

436
00:38:07,060 --> 00:38:12,260
that would put them at risk for heart attack. The signal kept showing up, though, like when

437
00:38:12,260 --> 00:38:17,540
Kaiser looked at their patient base database, they saw it everywhere. It looked it was a heart attack

438
00:38:17,540 --> 00:38:20,900
problem and stroke problem, by the way, but heart attacks especially.

439
00:38:20,900 --> 00:38:23,140
And the strokes were hypercoagulable strokes?

440
00:38:23,220 --> 00:38:29,620
As best we can tell. Yeah. In fact, when I did the 60 minutes segment, it brings me to that idea

441
00:38:29,620 --> 00:38:35,140
about we talked about it was right after Vioxx withdrawal. And I was upset because Merck was

442
00:38:35,140 --> 00:38:40,660
claiming they did everything right. And I knew much better that it wasn't true. In fact, we had

443
00:38:40,660 --> 00:38:45,380
called this three years before and they still never took it seriously. And as you said, they could have

444
00:38:45,380 --> 00:38:50,260
just admitted there was a problem. It was in all their emails. It was clearly they knew about it.

445
00:38:50,260 --> 00:38:52,740
Wait, there's evidence they knew before your paper?

446
00:38:52,740 --> 00:38:54,820
Oh, absolutely. Oh, I don't think I realized. Yeah.

447
00:38:54,820 --> 00:39:01,220
I thought it was your paper came out in 01. That was the shot across the bow. Then they just

448
00:39:01,940 --> 00:39:08,660
completely denied it, concealed data until it became undeniable by 04. I didn't realize prior

449
00:39:08,660 --> 00:39:14,500
to 01 internally they had seen the same signal. Absolutely. No, they had emails. They recovered

450
00:39:14,500 --> 00:39:21,540
from the all the way back to from 99 when the FDA approved the drug, 2000 well before our paper.

451
00:39:21,700 --> 00:39:26,900
Because they did make the argument in 99 that naproxen was risk lowering. And that's why there

452
00:39:26,900 --> 00:39:34,180
was no signal. Yeah. In fact, the term signal was used as the head scientific officer and all the

453
00:39:34,180 --> 00:39:38,820
people involved in the Vioxx development said, well, let's turn it on. Let's flip it to the

454
00:39:38,820 --> 00:39:43,860
communications experts showed up and yeah. Yeah. Now, the whole thing was just so incredibly

455
00:39:43,860 --> 00:39:49,460
contrived and it was all clear that they were in this race with Pfizer with Cellebrex. They didn't

456
00:39:49,460 --> 00:39:55,700
want to lose it. $5 billion was on the line and whatnot. But when I went to 60 minutes to discuss

457
00:39:55,700 --> 00:40:01,620
this right after the term turbulence of the withdrawal, the interviewer, he had just had

458
00:40:01,620 --> 00:40:06,740
a stroke on Vioxx. He never revealed it on the show. I said, well, why don't you, you know,

459
00:40:06,740 --> 00:40:12,100
in the, just like we're talking before we actually went on the air, I said, why didn't you tell

460
00:40:12,100 --> 00:40:16,100
people that? He says, well, I'm not part of the story. I said, well, you had a stroke. I mean,

461
00:40:16,100 --> 00:40:22,340
that's kind of a big deal. Ed Bradley. You know, I think there was a lot of hits out there. It's a

462
00:40:22,340 --> 00:40:29,540
shame because, you know, up until that time, Merck had been. What finally sunk the ship in 04?

463
00:40:30,180 --> 00:40:36,260
Well, when they withdrew the drug, there was another new trial. And this one, again,

464
00:40:36,260 --> 00:40:41,060
the same exact signal. This was a phase four. This was a, yeah. Actually, I think it was phase three

465
00:40:41,140 --> 00:40:46,580
for an expanded indication. Whereas the early one was in, you know, one condition, this was in

466
00:40:46,580 --> 00:40:52,180
another, it was a large trial. The heart attack thing was right there again. And you just couldn't

467
00:40:52,180 --> 00:40:56,900
deny it anymore, especially on top of everything they'd been trying to suppress for years. So they

468
00:40:56,900 --> 00:41:02,420
just pulled a plug on it. But was there any ramification? No, that's actually when you mention

469
00:41:02,420 --> 00:41:08,340
it, you know why I never should have been involved with this. I regret it because you do regret it.

470
00:41:08,980 --> 00:41:13,140
Absolutely. Because nothing ever happened. I mean, no one at Merck ever.

471
00:41:13,140 --> 00:41:17,460
In other words, you believe that had you not written the paper in 01, they still would have

472
00:41:17,460 --> 00:41:22,660
withdrawn it in 04? They might have because after we wrote the paper and published it,

473
00:41:22,660 --> 00:41:27,780
others started to come alive like Kaiser and others about this signal. So it was getting more

474
00:41:27,780 --> 00:41:32,260
and more undeniable. So I don't know that our paper, even though it was the first one and was

475
00:41:32,260 --> 00:41:37,060
in a high profile journal, I think they still would have had a hard time keeping that drug.

476
00:41:37,060 --> 00:41:41,700
Well, they might have done what you suggested, Peter, which is put on a warning and keep

477
00:41:41,700 --> 00:41:45,700
marketing, which is what they should have done. It was a good drug, but the problem was that the

478
00:41:45,700 --> 00:41:51,140
doses that they were recommending, certain people were getting exposed. And you say, well, two out

479
00:41:51,140 --> 00:41:55,300
of a hundred, not a lot, but when you have tens of millions of people, there's a lot.

480
00:41:55,300 --> 00:42:00,900
Yeah. One out of 50 absolute increase in risk for a heart outcome like mortality is a huge deal,

481
00:42:00,900 --> 00:42:05,700
especially if you can't know who that patient is. So this is where, again, because I never,

482
00:42:05,940 --> 00:42:09,140
I was in the middle of my residency when this was going on and I was a surgeon, so it's not like

483
00:42:09,140 --> 00:42:13,860
this was top of mind. I just had a personal interest because I remember using Vioxx and

484
00:42:13,860 --> 00:42:21,460
finding it so efficacious and finding it to be personally much better than Celebrex and much

485
00:42:21,460 --> 00:42:25,940
better than single day dosing. I think he took 50 milligrams once a day. I mean, it was like,

486
00:42:26,900 --> 00:42:32,340
and I had just had a horrible back injury in 2001, which is actually another story where they'd

487
00:42:32,340 --> 00:42:37,060
operated on the wrong side and I had multiple trips to the OR. So I was really debilitated.

488
00:42:37,060 --> 00:42:43,940
And in the midst of a surgical residency, Vioxx was the saving grace for me. But my recollection

489
00:42:43,940 --> 00:42:49,860
was, oh, but there's a subset of patients in whom you could sort of carve out to not take it. And

490
00:42:49,860 --> 00:42:52,580
that would have been the interesting question. That would have been the clinical question,

491
00:42:52,580 --> 00:42:57,220
which is like, for example, like if you, if you look at drugs that cause birth defects,

492
00:42:57,220 --> 00:43:01,460
something like Avodart or Dutasteride or something like that, like don't take it. If there's a

493
00:43:01,460 --> 00:43:06,100
pregnant woman nearby kind of thing becomes a very clear and obvious way, not that that causes

494
00:43:06,100 --> 00:43:10,020
birth defects, but that interferes with androgens. I don't know. So it's interesting to hear you say

495
00:43:10,020 --> 00:43:13,220
that, that basically, I don't want to put words in your mouth, but it almost sounds like you said,

496
00:43:13,220 --> 00:43:18,660
if you go back in time, you wouldn't have done it. No, because it wound up being a horrible phase in

497
00:43:18,660 --> 00:43:25,940
my career, the true nadir, not only during that time after the withdrawal, were there threats

498
00:43:25,940 --> 00:43:31,380
from whether it was Merck or friends of Merck, you know, calling up saying, if you don't stop

499
00:43:31,380 --> 00:43:36,180
talking about this, you know, bad things are going to happen to you. I remember being out of town one

500
00:43:36,180 --> 00:43:41,860
night and my wife got a call like that. You better stop. You better tell your husband to stop saying

501
00:43:41,860 --> 00:43:45,540
things about Merck or you're going to regret it. It's hard for people to believe what you're saying,

502
00:43:45,540 --> 00:43:50,340
right? It sounds like the sort of thing you'd see in a mob movie. Yeah. No, it was the worst

503
00:43:50,340 --> 00:43:56,820
experience. And then I even had my own institution, unbeknownst to me, the chairman of the board

504
00:43:57,380 --> 00:44:03,620
of Cleveland Clinic, a fellow named Malachi, who was CEO of Invacare, but he and Gil Martin,

505
00:44:03,620 --> 00:44:09,140
the CEO of Merck were best friends from Harvard Business School. And so he and the CEO of Cleveland

506
00:44:09,140 --> 00:44:16,020
Clinic were basically getting up to suppress me and gag me and also to turn on me. So I had my

507
00:44:16,100 --> 00:44:21,460
own institution. I had Merck against me. It was a nightmare. I mean, a terrible nightmare.

508
00:44:22,020 --> 00:44:26,340
But without redemption, when Merck finally pulled the drug, you would think that,

509
00:44:27,460 --> 00:44:32,820
one, it would sort of give people pause to realize that this was, as you said,

510
00:44:32,820 --> 00:44:38,500
probably inevitable. And two, it was the right thing to do. Unfortunately, it was too big a hammer

511
00:44:38,500 --> 00:44:44,500
for, you know, like I said, they were backed into a corner. No, they were in a corner, but, you know,

512
00:44:45,380 --> 00:44:50,660
but nobody came around. You know, to me, it's kind of nowadays, everybody talks about

513
00:44:50,660 --> 00:44:57,940
truth and fake and whatnot. But to me, then was the beginning of seeing that syndrome because

514
00:44:57,940 --> 00:45:05,940
here was truth and it was just being basically turned into fake news by Merck. And they had gone

515
00:45:05,940 --> 00:45:12,660
years of marketing a drug, mass marketing a drug. You couldn't turn on a TV set without seeing ads

516
00:45:12,660 --> 00:45:18,580
for Vioxx. And they never fessed up and they just, every single patient case that went to court,

517
00:45:19,220 --> 00:45:25,540
they basically prevailed eventually, whether it was the original case or the appeals by this whole

518
00:45:25,540 --> 00:45:31,540
inability to proof, for proof in an individual patient. So they didn't pay anything.

519
00:45:31,540 --> 00:45:33,300
There was no restitution whatsoever.

520
00:45:33,300 --> 00:45:38,100
Nothing that I know that's significant. And most importantly, the executives who oversaw this,

521
00:45:38,100 --> 00:45:42,900
who knew exactly what they were doing, they didn't go to jail. They were never indicted.

522
00:45:42,900 --> 00:45:44,740
There was never any charge.

523
00:45:44,740 --> 00:45:46,260
So no civil suits at all?

524
00:45:46,260 --> 00:45:46,820
Nothing.

525
00:45:46,820 --> 00:45:47,140
Nothing.

526
00:45:47,140 --> 00:45:53,060
Which is interesting. It tells you something about how difficult it is when the complication

527
00:45:53,060 --> 00:45:58,980
is a ubiquitous disease. You see, it's different when you're dealing with, well, and of course,

528
00:45:58,980 --> 00:46:03,060
we think of the examples that turned out to be wrong, right? Like the use of silicone,

529
00:46:03,060 --> 00:46:06,900
breast implants and lupus. Well, it turned out to be incorrect, but you at least had a signal

530
00:46:06,900 --> 00:46:11,140
to talk about because lupus was so rare or what other connective tissue disorders they were talking

531
00:46:11,140 --> 00:46:16,820
about. But as you said, like, how can you possibly look at any individual and make that case

532
00:46:16,820 --> 00:46:20,900
probabilistically? You would need a very large trial to determine that.

533
00:46:20,900 --> 00:46:26,340
Oh yeah. I know. You can't single it out. It's almost impossible. If you had assays to show that

534
00:46:26,340 --> 00:46:32,740
the selectivity of the Cox two inhibitor was pro thrombotic making clot in a person and that

535
00:46:32,740 --> 00:46:36,900
person then had a heart attack or stroke. But you know who had that? I mean, these were

536
00:46:36,900 --> 00:46:41,940
sudden events and no one had a proof in that person that their clotting state has changed

537
00:46:41,940 --> 00:46:45,060
from the drug. It seems like that's got to be the most likely mechanism.

538
00:46:45,060 --> 00:46:49,860
Oh yeah. I don't, I don't, the mechanism of how these people went down is not elusive. But

539
00:46:50,420 --> 00:46:56,100
what's, you know, what's sad about this too Peter is I had known Roy Vagelos to some extent.

540
00:46:56,100 --> 00:47:01,220
I had the highest regard for this company. We were doing trials with this company when it happened.

541
00:47:01,220 --> 00:47:08,420
And so just to see a company that was reviewed as one of the most ethical minded companies,

542
00:47:08,420 --> 00:47:14,580
not just in pharma, but you know, abroad across all companies to see it take these tactics of

543
00:47:14,580 --> 00:47:20,100
marketing. And it's really sad. But of course, that's many years ago, you know, that's 2004.

544
00:47:20,100 --> 00:47:25,540
We're 15 years later now. Yeah. But I can hear it in your voice, Eric. It's still quite dramatic.

545
00:47:25,540 --> 00:47:30,660
Yeah. It was almost the end of my career. You know, that's what precipitated leaving Cleveland. And

546
00:47:30,660 --> 00:47:35,220
fortunately, you know, coming to San Diego, which was the greatest thing ever, but who would have

547
00:47:35,220 --> 00:47:40,420
known for a year or two, it was a question of whether there would be a new position and whether

548
00:47:40,420 --> 00:47:46,820
it be suited to things that I would want to do. So, so how did San Diego, I mean, were you a pariah

549
00:47:46,820 --> 00:47:53,300
at the time? Yeah. You know, one of my people who I regarded in extraordinary, the Pope of cardiology,

550
00:47:53,300 --> 00:47:58,580
Gene Brownwall, he was trying to help me. He says, you know what, Eric, you're radioactive right now.

551
00:47:59,140 --> 00:48:05,540
And I was, and I even had people at Cleveland Clinic, by then there was a new CEO and others

552
00:48:05,540 --> 00:48:11,460
who were charged to try to nuke me. That is any place I interviewed for a position, they were

553
00:48:11,460 --> 00:48:18,900
calling them and actively trying to, to take me down. So ultimately, over that course of a year,

554
00:48:18,900 --> 00:48:23,860
when I was looking to move, I started realizing I have to do this in stealth mode, because, you know,

555
00:48:23,860 --> 00:48:29,140
I've got people who are trying to get me. And fortunately, a very close friend of mine here in

556
00:48:29,140 --> 00:48:35,140
San Diego, who I'd known for decades, Paul Tierstein, who is at Scripps, and I had collaborated

557
00:48:35,140 --> 00:48:40,500
some with the people at Scripps Research. And so we started talking, they were excited about what I

558
00:48:40,500 --> 00:48:48,020
had a vision for. And then I was ultimately recruited in fall of 2006.

559
00:48:48,020 --> 00:48:52,660
And what was the role they brought you into at that time? Because at the time, wasn't Scripps,

560
00:48:53,140 --> 00:48:57,220
I mean, it wasn't really a clinical powerhouse, it was a research powerhouse.

561
00:48:57,220 --> 00:49:02,500
Well, kind of both in some respects. The research was the affiliation with UCSD on the clinical.

562
00:49:02,500 --> 00:49:10,660
No, no, purely Scripps Health. So Scripps Health was on the move. They had, Chris Angorder as the

563
00:49:10,660 --> 00:49:16,820
CEO had basically put together, stitched together many different Scripps entities into one called

564
00:49:16,820 --> 00:49:21,940
Scripps Health. They were, and are, completely different entity than Scripps Research.

565
00:49:22,020 --> 00:49:25,940
So TRSI, the Translational Research Institute, that was totally separate.

566
00:49:25,940 --> 00:49:31,780
So totally separate, although prior to 2000, year 2000, they were one entity, but it was Scripps

567
00:49:31,780 --> 00:49:37,060
Clinic then, not this big health system with multiple hospitals and 30 clinics and whatnot.

568
00:49:37,060 --> 00:49:44,580
So what I did was to come in to be a cardiologist at Scripps Clinic, but also to develop a new

569
00:49:44,580 --> 00:49:50,020
institute that was dedicated to translational research, particularly genomics. That's why I

570
00:49:50,020 --> 00:49:54,340
came here. And it was only, you know, within weeks, I realized, wait a minute, what about

571
00:49:54,340 --> 00:49:59,220
wireless? What about digital? Because you don't want to just rely on a genome, even though

572
00:49:59,220 --> 00:50:04,260
back in 06, it was tremendous. Now, what was Craig Venter doing at that time?

573
00:50:04,260 --> 00:50:11,220
So Craig, he had the Venter Institute in Maryland. He was also working in synthetic biology, had a

574
00:50:11,220 --> 00:50:18,260
synthetic biology company here. And I think he was aiming to develop another Venter Institute in San

575
00:50:18,260 --> 00:50:25,700
Diego, but he, as a pioneer of pushing the whole sequencing project, of course, in year 2000,

576
00:50:25,700 --> 00:50:30,900
announcing it with Francis Collins and Bill Clinton at the White House. But he had moved,

577
00:50:30,900 --> 00:50:35,940
not just from sequencing, but also to writing the genome with synthetic biology. That was his

578
00:50:35,940 --> 00:50:41,380
interest at the time. I see. Got it. So I came here to try to make human genomics and genetics

579
00:50:41,380 --> 00:50:46,980
center stage for the two Scripps institutions. Right. And you wanted to translate this as

580
00:50:47,060 --> 00:50:51,860
quickly as possible to basically patient care. Yeah. To change practice, which is, we're still

581
00:50:51,860 --> 00:50:57,700
working on that, but that was the goal. And we basically very quickly, fortunately, we're able

582
00:50:57,700 --> 00:51:05,620
to get a big grant called the CTSA grant, one of the 57 now hubs of that in the country. And we're

583
00:51:05,620 --> 00:51:10,500
the only one that's not a university or without a medical school, but basically Scripps research is

584
00:51:10,500 --> 00:51:16,820
a storied institution with some of the best life science in the world ranked number one in nature

585
00:51:16,820 --> 00:51:23,940
for innovation and influence above some of the very top known center. So it has had a phenomenal

586
00:51:23,940 --> 00:51:29,380
track record and to work with them, this great brain trust of scientists and to try to bridge

587
00:51:29,380 --> 00:51:35,060
that with this big clinical entity Scripps health, which is a dominant player in the San Diego

588
00:51:35,060 --> 00:51:41,140
region, a big region for me, it was perfect. And basically the big grant we were able to get

589
00:51:41,140 --> 00:51:46,420
led to innovation space, you know, just to do whatever you think would be appropriate to

590
00:51:47,060 --> 00:51:50,900
make medicine better. And there's no shortage of ways we could do that.

591
00:51:51,620 --> 00:51:56,180
And you're also the editor in chief of Medscape. Is that right? Right. How did you get involved? And

592
00:51:56,740 --> 00:52:00,820
I think anybody listening to this who's ever gone onto Google and searched for something will

593
00:52:00,820 --> 00:52:06,260
notice Medscape is usually coming up with information. So what is Medscape? Yeah. Well,

594
00:52:06,260 --> 00:52:13,060
Medscape is the professional side for healthcare professionals of WebMD. The way I got into it was

595
00:52:13,060 --> 00:52:20,180
in the mid nineties when the internet was kind of warming up, I started with a couple of friends,

596
00:52:20,180 --> 00:52:27,620
the heart.org, which was the first cardiovascular website for cardiologists and anyone working in

597
00:52:27,700 --> 00:52:33,700
this space. So we started that and, you know, it all about getting great content, getting journalists.

598
00:52:34,580 --> 00:52:42,740
And it was for many years, a big magnet for not just the information, but also a forum for

599
00:52:42,740 --> 00:52:49,780
education and for debates and whatnot. So ultimately Medscape started to cover every

600
00:52:49,780 --> 00:52:56,020
specialty and they acquired the heart.org. And in that acquisition, being the editor in chief of

601
00:52:56,020 --> 00:53:00,820
that, they ultimately asked me would I be the editor in chief of Medscape? And so I've done

602
00:53:00,820 --> 00:53:04,900
that now for several years. It's been great. How much time does that take? I mean, that's,

603
00:53:04,900 --> 00:53:09,380
you must have an editorial staff under you because it's such a voluminous, I mean,

604
00:53:09,380 --> 00:53:14,900
it's like an encyclopedia. Yeah, no, they have an amazing crew of medical journalists and they

605
00:53:14,900 --> 00:53:21,300
cover everything that moves in medicine. I don't do so much day to day. I said general direction.

606
00:53:21,300 --> 00:53:26,740
We have a monthly call to go over features that I usually try to introduce ideas for that. I do a

607
00:53:26,740 --> 00:53:34,180
lot of interviews. I try to find like, you know, this week was the big Wall Street Journal issue

608
00:53:34,180 --> 00:53:40,340
with a Penn medicine, former Dean taking on medical education today, saying that it was

609
00:53:40,900 --> 00:53:48,660
completely off base to nurture students on climate change or gun control or any social injustice.

610
00:53:49,300 --> 00:53:53,700
And of course there was a revolt that, you know, and we're going to have a lot of that in Medscape.

611
00:53:53,700 --> 00:54:00,260
So I tried to bring up my, you know, when I first got involved, the website was much more

612
00:54:00,260 --> 00:54:06,180
pharma oriented. And what I've tried to do is round that out with not just devices and

613
00:54:06,180 --> 00:54:12,420
medical education, but also, you know, the whole genomics and digital medicine and AI and all those

614
00:54:12,660 --> 00:54:18,660
sort of topics. How big is the staff? Oh gosh, there's probably over 30, 35 journalists.

615
00:54:18,660 --> 00:54:24,340
Ivan Oranzky recently joined as a VP for editorial. He runs Retraction Watch,

616
00:54:24,340 --> 00:54:29,780
which is really formidable. But no, it's a big staff. It's a for profit or not for profit?

617
00:54:29,780 --> 00:54:34,820
Well, it's part of WebMD. It's part of WebMD. WebMD used to be a publicly traded company,

618
00:54:34,820 --> 00:54:40,020
but they were acquired about a year ago by a company called Internet Brand. So they're now

619
00:54:40,020 --> 00:54:44,100
a private, but for profit company. Got it. I didn't realize that I should have known that,

620
00:54:44,100 --> 00:54:48,340
I suppose, but I didn't realize Medscape was under that umbrella. Yeah. And I've always tried to

621
00:54:48,340 --> 00:54:56,900
weave in the WebMD side because WebMD has a big reach to consumers, as you pointed out, to kind of

622
00:54:56,900 --> 00:55:03,700
go to search for lots of common things in medicine. And we don't do that enough. I'm hoping that over

623
00:55:03,700 --> 00:55:09,460
time we'll see better crosstalk because we may have some really interesting things on the Medscape

624
00:55:09,460 --> 00:55:14,660
side or the opposite on WebMD. We don't get enough trying to get that mixed audio.

625
00:55:14,660 --> 00:55:20,260
It's funny that it's taken us this long to get to your most recent book, but I think it was a

626
00:55:20,260 --> 00:55:26,420
worthwhile route to get here because I think that the story of Vioxx alone I think is, well, I learned

627
00:55:26,420 --> 00:55:31,540
a lot because I, again, I think I knew parts of it, but I don't think I appreciated the severity

628
00:55:31,540 --> 00:55:37,540
with which you've paid a price. Fortunately, you passed tense and didn't hold. Yeah. And it sounds

629
00:55:37,540 --> 00:55:41,700
like it's worked out for the best, but that's phenomenal. That seems to be one of those

630
00:55:41,700 --> 00:55:47,060
experiences that falls in the category of you're probably better for it, but you never want to

631
00:55:47,060 --> 00:55:52,500
redo it. Exactly. You get much stronger. You learn who your friends are and aren't. And

632
00:55:53,060 --> 00:55:58,740
basically when I got here, it was like being in the Witness Protection Program and you're starting

633
00:55:58,740 --> 00:56:03,700
all over. I remember I had this big lab where we're going to do all the sequencing and I'm sitting

634
00:56:03,780 --> 00:56:09,620
in this big lab and I'm the only one in the lab and I got a lot of recruitment to do. Now we have

635
00:56:09,620 --> 00:56:15,700
just in our group, well over a hundred people, we have one of the largest NIH grants in history to

636
00:56:15,700 --> 00:56:23,060
do all of us, the big million person participants, a diverse group that we're doing so much with over

637
00:56:23,060 --> 00:56:27,860
the years ahead. So things are really humming, so it's been great. So your book is called

638
00:56:27,860 --> 00:56:34,580
Deep Medicine. And the picture on the front really points to AI, but the book is about more

639
00:56:34,580 --> 00:56:38,900
than that. But I want to start with that. Now let's assume for a moment that someone listening

640
00:56:38,900 --> 00:56:46,020
to this has heard the term AI and sort of knows from science fiction movies what it kind of means,

641
00:56:46,020 --> 00:56:50,180
but that's the limit of the knowledge, right? So they don't maybe necessarily know the difference

642
00:56:50,180 --> 00:56:54,100
between machine learning and artificial intelligence or those terms synonymous,

643
00:56:54,740 --> 00:56:59,140
and let alone how would that even factor into medicine and how do you separate out the sci-fi

644
00:56:59,140 --> 00:57:03,940
from what's already happening, right? And to what you think a path looks like. So take that in any

645
00:57:03,940 --> 00:57:08,900
order you like. Well, I mean, I think the problem with AI is it's been around the concept since the

646
00:57:08,900 --> 00:57:16,580
50s, 1950s, and it's diffuse. Yes, there's lots of sci-fi and movies and misunderstandings, but

647
00:57:16,580 --> 00:57:23,860
what we're talking about now is a specific subtype of AI, which got its birth just over 10 years ago.

648
00:57:24,260 --> 00:57:30,980
Called deep learning neural networks that allow for inputs and they could be millions, billions

649
00:57:30,980 --> 00:57:36,500
of data points. Could be images, could be speech, could be text. And then it goes through these

650
00:57:36,500 --> 00:57:41,860
layers of artificial neurons, which are not very much like neurons, but nonetheless, they can

651
00:57:41,860 --> 00:57:48,420
distinguish features progressively as they go through this network. And then you get outputs.

652
00:57:48,500 --> 00:57:53,700
And what's remarkable about this era and why it recently won the Turing prize for

653
00:57:53,700 --> 00:57:59,460
Jeffrey Hinton and his colleagues from University of Toronto, but the thing that's so...

654
00:57:59,460 --> 00:58:02,580
The Turing prize being basically the Nobel prize for computer science.

655
00:58:02,580 --> 00:58:08,260
Yes, I should have mentioned that. Exactly. The reason why this is such a big advance in medicine,

656
00:58:08,260 --> 00:58:13,860
the biggest advance I've ever seen as a student of medicine for many decades now, but it's so big

657
00:58:13,860 --> 00:58:22,180
because you can take, particularly now, images and you can get accurate definition of the image

658
00:58:22,900 --> 00:58:29,300
better than experts, doctors. So whether it's radiologists or dermatologists, pathologists,

659
00:58:29,300 --> 00:58:34,980
cardiologists, I mean, you go down the list, ophthalmologists, and you will see studies now

660
00:58:34,980 --> 00:58:40,820
to show superiority of accuracy, or at least as good through a machine.

661
00:58:40,900 --> 00:58:45,620
Now, to be clear, this is in initial recognition, not comparison. I mean, I think... This is an area

662
00:58:45,620 --> 00:58:49,940
I don't know very much about, Eric, but the last time I thought about this and did some reading

663
00:58:49,940 --> 00:58:56,660
about this, I came away with the impression that if you took an MRI of a person and you showed...

664
00:58:56,660 --> 00:59:00,980
So this is the first time this person's getting an MRI, you get the best radiologist to look at it,

665
00:59:00,980 --> 00:59:06,180
you get the best computer to look at it. The computer still struggled for macro context.

666
00:59:06,180 --> 00:59:10,580
It still didn't even realize that was the liver, per se, but it could certainly,

667
00:59:11,140 --> 00:59:17,940
with greater fidelity and resolution, once told that was the liver, identify and maybe be more

668
00:59:18,500 --> 00:59:23,540
clear about, well, what's a cyst versus what's a hemangioma versus what's a hepatoma.

669
00:59:23,540 --> 00:59:28,500
So it had superiority there. It also had superiority when it came to serial studies.

670
00:59:28,500 --> 00:59:34,020
So, Mrs. Smith had a chest X-ray a year ago. She has a cough now. She has another chest X-ray.

671
00:59:34,020 --> 00:59:37,140
Is there a difference? But am I right in my recollection?

672
00:59:37,140 --> 00:59:42,260
No, actually, I'm really glad you put some anchoring on that because what we have...

673
00:59:42,260 --> 00:59:47,540
Deep learning is, in many respects, extraordinary, but it's very narrow. So if I say,

674
00:59:48,420 --> 00:59:55,140
find me pulmonary nodules in a chest X-ray, that's where I say it can be superior. And clearly,

675
00:59:55,140 --> 01:00:00,580
the best is the combination, the synergy, the symbiosis of what the machine can, quote,

676
01:00:00,580 --> 01:00:07,620
see versus what the doctor could see. So yes, it's a very narrow thing. But what we're talking

677
01:00:07,620 --> 01:00:15,300
about here is there's so many mistakes in medicine because things are missed or are inaccurate. And

678
01:00:15,300 --> 01:00:18,580
this extends through pathology in every different specialty.

679
01:00:18,580 --> 01:00:23,940
Yeah, your thesis is not... I mean, many people have said to me when they talk about this sort

680
01:00:23,940 --> 01:00:28,980
of loosely that the radiologist is the first doctor on the chopping block. That's not really

681
01:00:28,980 --> 01:00:33,700
your thesis. No, I actually think that's completely wrong. Jeffrey Hinton said that once,

682
01:00:33,700 --> 01:00:40,180
and I think he will ultimately regret it. The point being is that it basically tees it up.

683
01:00:40,180 --> 01:00:48,820
That is, you get a different complimentary read of something, and that helps for speed and accuracy,

684
01:00:48,820 --> 01:00:52,980
and it could ultimately lower costs, and it could ultimately improve medicine. The thesis of deep

685
01:00:52,980 --> 01:00:58,740
medicine is if we lean on machines more, in many respects, we can get into that. But if we do that

686
01:00:58,740 --> 01:01:04,900
more, we can free up to have time with patients, and we could get the doctor-patient relationship

687
01:01:04,900 --> 01:01:11,380
back to where it ought to be, where it was some 40 years ago. That's the main premise that is

688
01:01:11,380 --> 01:01:16,980
unique about the book in which I really build up to deep empathy within the last chapter. But

689
01:01:17,780 --> 01:01:25,780
the real thing that's different now is that we have lots of promise, lots of potential for AI.

690
01:01:25,780 --> 01:01:31,700
We haven't actualized that. We haven't proven it for the most part. One of the only randomized

691
01:01:31,700 --> 01:01:37,460
trials to date is in colonoscopy. Because a lot of polyps, particularly if they're flat or sessile

692
01:01:37,460 --> 01:01:43,940
or small, are missed. It's very much operator dependent, how much time they take to do a thorough

693
01:01:43,940 --> 01:01:50,020
colonoscopy. Now there's a Chinese randomized trial that shows, hey, if you use deep learning

694
01:01:50,020 --> 01:01:55,940
machine vision, you can pick up polyps that are routinely missed. Then people say, okay,

695
01:01:56,500 --> 01:02:01,220
so what? Maybe the ones that are missed are not important. Well, that's where we are today.

696
01:02:01,220 --> 01:02:05,460
That's the study, is you look at the denominator of the missed versus the not missed, the machine

697
01:02:05,460 --> 01:02:10,100
caught versus not, and what's the prevalence? Because if the prevalence of pathology in them

698
01:02:10,100 --> 01:02:15,140
is at least the same, you could argue they shouldn't be missed. It might be higher if it's

699
01:02:15,140 --> 01:02:21,780
sessile. Yeah, so 20% of polyps were picked more, were picked up by machine vision, and then we

700
01:02:21,780 --> 01:02:28,340
still don't know how much of that were true disease likely. I've always felt the field,

701
01:02:28,340 --> 01:02:33,940
if radiology is the first pit stop on this journey, I've always felt like the ICU needed

702
01:02:33,940 --> 01:02:40,260
to be a very close second. How much is really being done there? Because A, it's the, obviously,

703
01:02:40,260 --> 01:02:46,500
the most data rich environment after radiology. Radiology also, of course, informing the ICU,

704
01:02:46,500 --> 01:02:51,060
but in terms of just raw numbers coming out about a patient, if you think about a patient on a

705
01:02:51,060 --> 01:02:58,900
ventilator with CVVHD, you pick every device strapped up to a patient. It's not the same

706
01:02:58,900 --> 01:03:04,820
as a Formula One car, but you're in the ballpark of that much data. Yeah, and you're touching on

707
01:03:04,900 --> 01:03:10,740
one of the big deficiencies of AI and deep learning today, which is multimodal data.

708
01:03:11,460 --> 01:03:20,100
When you have all these inputs of varied types, not just their vital signs, but could be machine

709
01:03:20,100 --> 01:03:26,900
vision of their facial recognition. It could be so many different parts about that person, no less

710
01:03:26,900 --> 01:03:34,100
their prior electronic record. We don't do well with that because deep learning today is, as I

711
01:03:34,100 --> 01:03:39,940
say, narrow task. It's like what's in this eye ground for ophthalmologists. Is this diabetic

712
01:03:39,940 --> 01:03:46,980
retinopathy? Is this something else? The ability to take many layers of data, which would be the

713
01:03:46,980 --> 01:03:53,780
ICU story, is in the early stages, even more so than the image recognition. Yeah. Realistically,

714
01:03:53,780 --> 01:03:59,620
where do you think that is in terms of, again, the caveatting it with the, it's always going to take

715
01:03:59,620 --> 01:04:06,180
longer than we think it is. Is this something where, I don't know, in 10 years or in 20 years,

716
01:04:06,820 --> 01:04:16,820
going to an ICU will afford a patient the luxury of a true supercomputer that's basically assimilating

717
01:04:17,540 --> 01:04:26,020
the CVVHD data with the ventilator data, with the swan GANs data, like stuff that as you point out,

718
01:04:26,100 --> 01:04:34,900
it's even the most analytical physician can't really recognize the patterns that are deep within

719
01:04:34,900 --> 01:04:41,060
all of those data. Well, you just touched on with that statement, the essence of why we need

720
01:04:41,060 --> 01:04:47,460
AI support, not just in an ICU patient, but in every patient. There's more data than we can handle,

721
01:04:48,260 --> 01:04:51,780
especially when you say people are wearing sensors, they're going to be wearing more,

722
01:04:51,780 --> 01:04:55,940
people are going to have their genome sequence, or they already have a genome chip or array,

723
01:04:56,740 --> 01:05:02,020
microbiome with a gut. I mean, no less all their records, not just the one healthcare place that

724
01:05:02,020 --> 01:05:09,380
there happened to be visiting that moment. So this flood of data per person, no less the intensive

725
01:05:09,380 --> 01:05:15,540
data collection in an ICU setting, this is overridden human capability. We need machines,

726
01:05:15,540 --> 01:05:20,660
we have to fess up that we can't do this, but we also have to acknowledge that we're not there yet.

727
01:05:20,820 --> 01:05:26,580
So when are we going to get there? Well, Fei-Fei Li and the group at Stanford has done ICU work,

728
01:05:26,580 --> 01:05:30,740
machine vision to see whether- Is it single machine or is it integrated?

729
01:05:30,740 --> 01:05:36,980
Their studies have been mainly single whereby, for example, they're looking to see risk of

730
01:05:36,980 --> 01:05:41,860
extubation so that you don't have to have a nurse in the room all the time that what's going on with

731
01:05:41,860 --> 01:05:48,100
that patient that they're going to self-extubate or others have looked at likelihood of sepsis or

732
01:05:48,100 --> 01:05:54,100
different pieces of the story, but no one has integrated it all yet today. And I think that's

733
01:05:54,100 --> 01:05:59,780
where it's headed. We're seeing these hybrid models of bringing the data together, but a lot

734
01:05:59,780 --> 01:06:06,980
of the problem with this field has been way out of bound type of where it can go. And when I did the

735
01:06:06,980 --> 01:06:11,940
research in the book, which involved a few years of work cumulatively, I spoke to a lot of the real

736
01:06:11,940 --> 01:06:17,940
gurus in the field and they made it clear that we are going to get there eventually, but we're

737
01:06:18,020 --> 01:06:22,820
not there yet. That is the challenge because when you think about other big breakthroughs that we

738
01:06:22,820 --> 01:06:30,900
look back on, we don't realize that they were more discoveries than creations sometimes. So,

739
01:06:30,900 --> 01:06:36,100
for example, look at germ theory, right? This is, again, it's something we take for granted today,

740
01:06:36,740 --> 01:06:41,380
and it's hard to believe there was a day when you wouldn't wash your hands before operating

741
01:06:41,380 --> 01:06:46,660
on a patient or you wouldn't wear sterile gloves. So we acknowledge how that transformed medicine

742
01:06:46,660 --> 01:06:52,500
in a step function manner, but two things are a little bit missed when we contrast it. One is,

743
01:06:53,380 --> 01:07:01,220
we didn't have to build it. We accepted it and discovered it. And two, it didn't happen overnight.

744
01:07:01,220 --> 01:07:07,300
Like there's still a generation that it takes to implement these things. And so that's the best

745
01:07:07,300 --> 01:07:12,900
case scenario, right? It doesn't get any better than that. This is something that has to be built.

746
01:07:13,140 --> 01:07:18,260
I can't think of an example, maybe I'm wrong, and if anyone's going to think of it, it's you. But

747
01:07:18,260 --> 01:07:24,580
is there an example before when we had the idea and the promise and then we set out to engineer

748
01:07:24,580 --> 01:07:30,340
the solution building into it? So, for example, I'll give you a failed example, which is the EMR.

749
01:07:30,340 --> 01:07:31,300
Totally failed, yeah.

750
01:07:31,300 --> 01:07:35,940
Right? So is it like literally the worst thing on the face of the earth? I heard you once talk

751
01:07:35,940 --> 01:07:39,940
about this, and I think it was you actually, but maybe it wasn't. But I'm going to give you credit

752
01:07:39,940 --> 01:07:46,420
for it. But I think you best summarize it by saying, look, the EMR was created as a billing

753
01:07:46,420 --> 01:07:52,020
solution, not a clinical solution. Absolutely. And I couldn't agree with you more. But there's

754
01:07:52,020 --> 01:07:57,140
an example of, okay, we have a problem. Medical records are so cumbersome, so voluminous, although

755
01:07:57,140 --> 01:08:00,820
really they're just a two dimensional problem. It's really not, it's three dimensions if you include

756
01:08:00,820 --> 01:08:07,380
time, I would say. And we now have computers, quote unquote. So computers will solve the problem.

757
01:08:07,380 --> 01:08:12,340
Let's build it. Well, one, it took a lot longer than people expected. It took much longer to

758
01:08:12,340 --> 01:08:18,340
implement it, and it sucks much more than people could have ever imagined it could suck. When I

759
01:08:18,340 --> 01:08:23,220
think of those examples, I keep saying, is there a positive story? Is there a great case study in

760
01:08:23,220 --> 01:08:27,940
medicine where the engineering solution lived up to its expectations?

761
01:08:27,940 --> 01:08:35,300
I think you nailed it. I don't believe there is one. This is a unique story that's being written

762
01:08:35,460 --> 01:08:42,100
as we speak. It's so different than we have a technology and we just want to implement it.

763
01:08:42,100 --> 01:08:48,260
This is one that there's a lot of construction that's still required. We know what the house

764
01:08:48,260 --> 01:08:53,780
is likely going to look like when it's built, but we're still on the foundation stage.

765
01:08:53,780 --> 01:08:58,260
Do you think that these are problems that are going to be solved by the giants? Is IBM,

766
01:08:58,820 --> 01:09:03,220
is Google, I mean, are these the entities that figure this out or is this going to be

767
01:09:04,180 --> 01:09:13,540
solved in more of a pharma model where the early discovery and the early stage, even the stage one,

768
01:09:13,540 --> 01:09:18,580
the safety trials are done by small companies that ultimately get acquired by rolled up into

769
01:09:18,580 --> 01:09:22,900
larger companies? I mean, today, the Merck's and the Pfizer's of the world aren't really doing

770
01:09:22,900 --> 01:09:27,300
drug discovery anymore. They've decided we're going to outsource that to more nimble companies.

771
01:09:27,300 --> 01:09:32,180
And basically the private markets now subsidize that while the public markets subsidize late stage

772
01:09:32,180 --> 01:09:35,700
drug development, do you think that's the way this is going to be or do you think this is going

773
01:09:35,700 --> 01:09:40,180
to have to start and finish within the behemoth companies that have their enormously deep pockets?

774
01:09:40,180 --> 01:09:45,140
I think this is a story of innovation from the outside. I think it's very different than what

775
01:09:45,140 --> 01:09:50,740
you're seeing now with the consolidation in pharma and outsourcing. Here you see the big

776
01:09:50,740 --> 01:09:56,180
titans like Google and Microsoft, Amazon, and the rest of them, they all recognize this is

777
01:09:56,180 --> 01:10:03,700
the greatest opportunity for growth and also a noble mission of improving health. So you have

778
01:10:04,260 --> 01:10:10,420
that group, you have startups that there's no shortage of those. And you also have some

779
01:10:10,420 --> 01:10:15,860
governments like in China, in the UK, and other places that are nurturing this, that are investing

780
01:10:15,860 --> 01:10:23,460
big in this area. So I think this combined force of multiple entities is where we're going to see

781
01:10:23,460 --> 01:10:29,140
this really take off. It's starting to happen much more in China out of need. That is,

782
01:10:29,780 --> 01:10:34,260
the implementation is way ahead of what's going on in the US because they have so few.

783
01:10:34,260 --> 01:10:35,380
What are some examples?

784
01:10:35,380 --> 01:10:40,420
Well, the radiologists, whereas here we're just starting to get a bunch of FDA approved algorithms

785
01:10:40,420 --> 01:10:46,980
for reading various types of scans. They already have that widespread throughout China. They already

786
01:10:46,980 --> 01:10:55,220
are doing many things on the, well, we're here, the only FDA consumer approved or cleared is the

787
01:10:55,220 --> 01:11:00,180
Apple watch for heart arrhythmia, a deep learning algorithm for atrial relation.

788
01:11:00,180 --> 01:11:04,980
So explain how that works. Let's use that example. That's near and dear to everybody's wrist.

789
01:11:04,980 --> 01:11:09,700
And I see you're wearing your Apple watch there as well. So let's just say you went into the Apple

790
01:11:09,700 --> 01:11:14,900
store today for the very first time and you bought an Apple watch. Okay. So first of all,

791
01:11:14,900 --> 01:11:19,220
it's on the back surface of the wrist, the volar surface of the wrist. And what is it

792
01:11:19,220 --> 01:11:23,460
shining through? And I assume it's shining it onto the veins in the back of your arm.

793
01:11:23,460 --> 01:11:28,980
Yeah, no, it's picking up optically each heart rate. And you can see the light that it's used.

794
01:11:29,540 --> 01:11:35,540
And for the deep learning algorithm, which actually was first cleared by a startup, a live

795
01:11:35,540 --> 01:11:39,700
core. And then a year later, Apple, which they didn't even acknowledge that they had been

796
01:11:39,700 --> 01:11:45,860
a year after the first, but nonetheless on their watch, they get heart rate. So at rest,

797
01:11:45,860 --> 01:11:52,980
and then when you are active and then they basically for you, it has your data whereby

798
01:11:52,980 --> 01:12:00,340
when you have heart rate at rest, that's off track for you, it says, get a cardiogram.

799
01:12:00,340 --> 01:12:05,540
And you get a one lead cardiogram when you press the crown on the PC and you get a good quality

800
01:12:05,620 --> 01:12:08,900
cardiogram. And then if it has atrial fibrillation, it's also...

801
01:12:08,900 --> 01:12:11,380
Which lead does it most closely approximate on the 12 lead?

802
01:12:11,380 --> 01:12:18,980
It's a lead one. You get a cardiogram read for atrial fibrillation, which one thing,

803
01:12:18,980 --> 01:12:20,020
it's pretty good for that.

804
01:12:20,020 --> 01:12:24,580
I was about to say, not to minimize that, but AFib seems like about the easiest thing to pick

805
01:12:24,580 --> 01:12:26,900
up because of the irregularity of it, right?

806
01:12:26,900 --> 01:12:32,660
Yeah, although there is some false positives and negatives because sometimes the P-waves

807
01:12:32,900 --> 01:12:39,460
that you're looking to be absent, sometimes you can get faked out. And so it's reasonably good.

808
01:12:39,460 --> 01:12:45,780
And it's in the 90 plus percent accuracy level, but it's all about the Bayes theorem of...

809
01:12:45,780 --> 01:12:47,220
More information.

810
01:12:47,220 --> 01:12:52,660
Well, people who are not risk, a lot of people have an Apple watch who are young and have zero

811
01:12:52,660 --> 01:12:56,580
risk of atrial fibrillation and they get a cardiogram and gets anxious and they may even

812
01:12:56,580 --> 01:13:03,300
get workups by a cardiologist. So this is a problem where we have marketing of an algorithm,

813
01:13:03,300 --> 01:13:05,380
the first deep learning algorithm.

814
01:13:05,380 --> 01:13:10,740
How long does it take, by the way, to learn a person well enough that it would be willing

815
01:13:10,740 --> 01:13:12,500
to make a recommendation like that?

816
01:13:12,500 --> 01:13:14,020
Oh, just a matter of hours.

817
01:13:14,820 --> 01:13:17,220
Or certainly by a couple of days, it's got it down.

818
01:13:18,020 --> 01:13:22,180
But yeah, I mean, you know your resting heart rate by the accelerometers,

819
01:13:22,180 --> 01:13:26,420
it knows that you're not moving. And why did your resting heart rate used to be 60?

820
01:13:26,660 --> 01:13:29,940
Why is it 100 something? And then it'll tell you to get a cardiogram.

821
01:13:29,940 --> 01:13:30,260
But...

822
01:13:30,260 --> 01:13:33,700
And it can't make any other diagnosis. It can't diagnose any ventricular rhythm.

823
01:13:33,700 --> 01:13:34,660
Not now.

824
01:13:34,660 --> 01:13:36,900
Or atrial tachycardia or anything else.

825
01:13:36,900 --> 01:13:41,780
Ultimately, it should be able to, but those algorithms haven't really been validated yet.

826
01:13:41,780 --> 01:13:47,220
But ultimately, we know now I use a six-lead cardiogram. It isn't on the watch,

827
01:13:47,220 --> 01:13:50,100
but you can just do that with sensors and put it on the leg.

828
01:13:50,100 --> 01:13:51,860
Well, how do you do that? That's interesting.

829
01:13:51,860 --> 01:13:55,300
Yeah. It's basically half the size of a credit card.

830
01:13:55,860 --> 01:13:58,820
Where do you get this? It's an aftermarket product or?

831
01:13:58,820 --> 01:14:04,740
No, it's actually marketed now by Livecore, the one that came with this ECG on the watch first.

832
01:14:04,740 --> 01:14:07,620
They actually put it on the Apple Watch, but it was their algorithm.

833
01:14:08,340 --> 01:14:12,900
They came up with a six-lead, which you then put that on your leg, your left leg,

834
01:14:12,900 --> 01:14:14,900
and then you get six, all limb lead.

835
01:14:14,900 --> 01:14:16,500
And you do this with your patients as well?

836
01:14:16,500 --> 01:14:20,260
Yeah, every patient. When I see them, instead of just taking their pulse,

837
01:14:20,260 --> 01:14:24,660
I also do a six-lead cardiogram. It's been remarkably insightful because

838
01:14:24,660 --> 01:14:29,940
it's free. It takes a second. And then I can really be much more certain about if they have

839
01:14:29,940 --> 01:14:33,620
an arrhythmia, but also diagnose conduction system abnormalities.

840
01:14:33,620 --> 01:14:37,460
So it's accurate enough that you can measure your intervals perfectly?

841
01:14:37,460 --> 01:14:42,260
Oh my gosh. The quality is amazing. Yeah. I mean, the six-lead.

842
01:14:42,260 --> 01:14:46,500
Now, can you send them home with the same kit and then can they get a six-lead on

843
01:14:46,500 --> 01:14:49,220
themselves at home and let you see the data?

844
01:14:49,220 --> 01:14:53,620
They could. I haven't done that yet, but that's probably where this is headed.

845
01:14:53,700 --> 01:14:56,980
The reason why this is actually funny, you mentioned it, Peter,

846
01:14:56,980 --> 01:14:59,620
you can even do your own stress test with this.

847
01:14:59,620 --> 01:15:02,980
Yeah, of course. In fact, you could do a real stress test, which is in the actual

848
01:15:02,980 --> 01:15:05,060
environment under which you need to be stressed.

849
01:15:05,060 --> 01:15:10,900
Yeah, I did that the other day. I did a rest ECG and then I got on a bicycle, stationary bicycle,

850
01:15:10,900 --> 01:15:16,020
and went really hard. And then just after I got off and did a six-lead again, so I said, wow,

851
01:15:16,020 --> 01:15:21,460
you can do a stress, electric cardiogram, high quality, six-lead, and never go near a medical

852
01:15:21,540 --> 01:15:23,300
output. Where are you seeing the output?

853
01:15:23,300 --> 01:15:23,940
Oh, on your phone.

854
01:15:25,220 --> 01:15:27,780
And you can make it a PDF and send it off to your doctor.

855
01:15:27,780 --> 01:15:31,300
It makes it automatically, yes. Yeah, it's pretty cool.

856
01:15:33,620 --> 01:15:38,500
So, I mean, that strikes me as proof of concept now.

857
01:15:39,300 --> 01:15:43,620
And that's where we're going to get, you alluded to, when are we going to get all the heart rhythm

858
01:15:43,620 --> 01:15:49,700
abnormalities diagnosed and the heart conduction, which is a precursor to arrhythmia? So,

859
01:15:49,700 --> 01:15:53,460
that's where we're headed because one lead, it's hard to do that. But when you have all the

860
01:15:53,460 --> 01:15:58,180
limits, in fact, with AI, you could impute all 12 leads. You don't even need to get the other six

861
01:15:58,180 --> 01:16:05,140
leads. So, pretty soon, we're going to see that six lead become really valuable entry for what's

862
01:16:05,140 --> 01:16:10,180
going on in a person's heart. In fact, Mayo Clinic just published a series of papers on 12 lead

863
01:16:10,180 --> 01:16:14,980
cardiograms that you could get heart function. You could predict from a cardiogram whether they're

864
01:16:14,980 --> 01:16:19,540
going to have atrial fibrillation. You can get the potassium level of the blood through that,

865
01:16:20,420 --> 01:16:26,660
the amount of data that's sitting in this pattern, which we can't see, is amazing.

866
01:16:26,660 --> 01:16:32,100
Well, think about the number of times I see this once a month, and my practice is really small. So,

867
01:16:32,100 --> 01:16:35,940
if I'm seeing this once a month, let's extrapolate to how many times this happens in the United

868
01:16:35,940 --> 01:16:41,700
States. The blood hemolyzes slightly on a blood draw and the potassium comes back at 5.5.

869
01:16:41,700 --> 01:16:44,340
Oh, or higher even. Yeah. And you don't know what to do.

870
01:16:45,060 --> 01:16:49,940
Well, imagine you had that EKG, you wouldn't have to panic because every time that happens,

871
01:16:49,940 --> 01:16:55,620
you have to call that patient, send them into the ER, get a blood draw, confirm what you know is

872
01:16:55,620 --> 01:17:02,180
likely true, which is the stupid sample hemolyze. Their potassium is really 4.7, but imagine you

873
01:17:02,180 --> 01:17:08,500
didn't have to do that. You could just push this button on your watch. That's a great example.

874
01:17:08,500 --> 01:17:15,940
It exemplifies what we can't see, but having a machine trained by a million cardiogram,

875
01:17:15,940 --> 01:17:22,020
what it can see. And in the book, in Deep Medicine, I have a chapter, it starts out with that story,

876
01:17:22,020 --> 01:17:27,780
how did they discover the potassium story, something we can't, we can tell, the potassium's

877
01:17:27,780 --> 01:17:33,700
really high with the tall T-waves, but we can't get to the decimal point. Right. We can't distinguish

878
01:17:33,700 --> 01:17:39,620
between 4.9, which is do nothing, and 5.6, which is you better be careful. Right. And that's what

879
01:17:39,620 --> 01:17:44,020
machines are good for. And we're going to be seeing a lot more of that kind of stuff. That is,

880
01:17:44,020 --> 01:17:50,500
the eye opening thing to me is to learn about all the things that we humans can't do, that machines

881
01:17:50,500 --> 01:17:56,100
can be trained to do, and they're just going to get better over time. So if you, do you wear a

882
01:17:56,100 --> 01:18:01,380
Dexcom sensor? I have not regularly. I'm not a diabetic, but I have, and I've learned a lot from

883
01:18:01,380 --> 01:18:08,500
it. I've tried Dexcom and the Libra. I mean, I've really found this glucose thing because of

884
01:18:08,500 --> 01:18:14,340
how it interacts with what you eat, with your sleep, with your physical activity. It's amazing.

885
01:18:14,340 --> 01:18:19,700
Yeah. It really is, people ask me why I still wear it because I'm not diabetic,

886
01:18:20,580 --> 01:18:26,020
and even my patients, so about a third of my patients to a half my patients wear this,

887
01:18:26,020 --> 01:18:30,580
none of whom have diabetes. Wow. And I always ask them for 90 days. If every one of my patients

888
01:18:30,660 --> 01:18:34,500
would wear it for 90 days, at least I'd be happy. And then we could decide, but what's in,

889
01:18:34,500 --> 01:18:39,620
what invariably happens is people realize the, they go through the following cycle, which is,

890
01:18:40,420 --> 01:18:44,020
Peter, you've been wearing this thing for four or five years. Haven't you already figured out what

891
01:18:44,020 --> 01:18:50,020
to eat and what not to eat? And I say, well, yes and no, but it's more complicated because like,

892
01:18:50,020 --> 01:18:57,300
for example, let me show you this. I have not eaten anything since 4 PM yesterday. Wow. It's

893
01:18:57,300 --> 01:19:04,020
11 30. So I'm coming up on 20 hours of no food. Yeah. Look at the variability in my glucose for

894
01:19:04,020 --> 01:19:13,060
the last 12 hours. It's been as high. It peaked at one 18, which was, yeah, peaked at one 18,

895
01:19:13,060 --> 01:19:18,100
which was right after a workout this morning. And by the way, it was just weight training. It wasn't

896
01:19:18,100 --> 01:19:20,500
like high intensity interval training. If it's high intensity interval training,

897
01:19:20,500 --> 01:19:29,140
it's going to go much higher. Now it's sitting at 94 and you'd think, well, if knowing that it's 94,

898
01:19:29,140 --> 01:19:34,340
like if I ate a bagel right now, could I predict what it would go to just knowing it's 94? The

899
01:19:34,340 --> 01:19:40,580
answer is not a chance. You see, just knowing it's 94 isn't enough to tell me my response to the bagel.

900
01:19:40,580 --> 01:19:45,140
I have to know how much glycogen I have. I have to know how much cortisol I have. I have to know

901
01:19:45,140 --> 01:19:51,780
how much insulin I have. Like there's so many variables and that's why four or five years later,

902
01:19:51,780 --> 01:19:57,060
there's nothing about this that is boring to me because I'm constantly learning a new physiologic

903
01:19:57,060 --> 01:20:04,100
experiment. I mean, if there's anything that's ripe for AI, it would also be CGM coupled with

904
01:20:04,100 --> 01:20:09,780
other data. So in other words, I don't think the CGM data as the input feed would be sufficient.

905
01:20:09,780 --> 01:20:13,940
You would have to constantly be pairing it with your activity and other sensors.

906
01:20:14,180 --> 01:20:15,540
I totally agree.

907
01:20:15,540 --> 01:20:21,860
If you had the cortisol sensor and the lactate sensor, that starts to become remarkable

908
01:20:21,860 --> 01:20:25,140
predictive power. And when you could get to the point where, because this is my dream,

909
01:20:25,700 --> 01:20:31,700
I want to know, can I eat that right now or not for my parameters? So this is my pipe dream is,

910
01:20:32,500 --> 01:20:37,700
I want to be able to say, go into the algorithm and tell it your desired average glucose,

911
01:20:37,700 --> 01:20:43,220
your desired variability. So I want an average glucose that's below a hundred milligrams per

912
01:20:43,300 --> 01:20:48,420
deciliter or below 110 milligrams per deciliter. I want a standard deviation that doesn't exceed

913
01:20:48,420 --> 01:20:54,500
15 milligrams per deciliter. And now you tell me what I can eat. Spend the first month watching

914
01:20:54,500 --> 01:21:01,460
me eat, learning how my body responds to every different food and go from there. I mean,

915
01:21:01,460 --> 01:21:04,580
directionally speaking, how long would it take to get us there?

916
01:21:04,580 --> 01:21:09,460
We're getting there. I mean, we're chipping away at that. So the gut microbiome is a big

917
01:21:09,460 --> 01:21:15,380
part of the story too. And I know you're such a proponent of this and I call myself a gut skeptic

918
01:21:16,020 --> 01:21:24,740
because, well, why would I say that? I certainly don't disregard the importance of that. I think

919
01:21:25,780 --> 01:21:31,540
I'm waiting to see a great example of how I can use it outside of like the really clear clinical

920
01:21:31,540 --> 01:21:37,460
ones. Like certainly knowing how to change the gut microbiome in the context of C. diff colitis

921
01:21:37,460 --> 01:21:44,580
is profound. It seems very likely that something about the gut changes in patients with diabetes

922
01:21:44,580 --> 01:21:50,020
who undergo gastric bypass, that seems to really suggest, but it could be as high as the duodenum.

923
01:21:50,020 --> 01:21:54,740
And the most compelling evidence I've seen is that it's actually the change. It's the duodenal bypass

924
01:21:55,380 --> 01:21:59,220
that specifically gives them this incredible remission out of the gate,

925
01:21:59,940 --> 01:22:05,940
more so than the lower GI tract. But I think most of my skepticism comes from the fact that it's

926
01:22:05,940 --> 01:22:11,300
not clear to me what to do with all those data, which may be exactly your point. That when I see

927
01:22:11,940 --> 01:22:16,980
patients constantly show up to me with their gut sequence and they say, well, look at this

928
01:22:16,980 --> 01:22:20,580
pathology state here. And I say, well, first of all, I don't know that that's a pathology state.

929
01:22:20,580 --> 01:22:26,100
And if it is a pathology state, is taking a probiotic the answer? I don't have any evidence

930
01:22:26,100 --> 01:22:33,780
that that's the case either. So is it more a readout state or is it a malleable state that

931
01:22:33,860 --> 01:22:39,140
we directly interfere with? Right. So those are all important questions. I think the real

932
01:22:39,700 --> 01:22:46,020
insight here is that up until when Aaron Siegel and his group at Weizmann Institute in Israel,

933
01:22:46,020 --> 01:22:50,900
up until they did what now has to be seen as a classic study. This was the cell metabolism

934
01:22:50,900 --> 01:22:57,460
paper from about a year ago? Well, there was a paper in cell 2015, which was really the seminal

935
01:22:57,460 --> 01:23:02,900
work. And now there's been several more and it's been replicated by many others. They took now

936
01:23:02,900 --> 01:23:09,060
thousands of people healthy like yourself and they went ahead and got microbiome, but they also

937
01:23:09,060 --> 01:23:16,020
got exact same amount of food at the exact same time. They also got all their labs and every

938
01:23:16,020 --> 01:23:21,140
piece of data they could get on these people. And they found that you could predict if they had a

939
01:23:21,140 --> 01:23:26,020
bagel, which ones are going to have and what level of glucose spikes they're going to see.

940
01:23:26,020 --> 01:23:33,460
And they found that so many spikes, even very significant spikes, 160, 180, 200 in healthy

941
01:23:33,460 --> 01:23:39,060
people with no sign of diabetes. And how durable do you think the knowledge is from the sequence?

942
01:23:39,060 --> 01:23:47,460
So for example, like if you sequenced that patient Monday morning at 9 AM, how much do we know that

943
01:23:47,460 --> 01:23:52,580
Friday at 5 PM, the data are still relevant, assuming you could even, because you can't get

944
01:23:52,580 --> 01:23:58,020
the data in real time. Right. So they didn't do any DNA sequencing. Of course that wouldn't change.

945
01:23:58,020 --> 01:24:03,220
So we don't know the genomic side of this, but we do know the microbiome, unless you do something

946
01:24:03,220 --> 01:24:08,420
significant. Oh no, sorry. I didn't mean their DNA. I meant the DNA of the bacteria. Okay, good.

947
01:24:09,060 --> 01:24:14,900
The DNA of the microbiome is pretty darn stable everywhere you look at it, unless you

948
01:24:14,900 --> 01:24:19,460
change it, radical change in your diet, like change fiber content, or you take antibiotics,

949
01:24:19,460 --> 01:24:25,940
but it's very stable from day to day. I see. So you would say that, look Peter, only if the patient

950
01:24:25,940 --> 01:24:31,540
does a course of antibiotics do we need to recheck them. Yeah. Or make a radical dietary change. But

951
01:24:31,540 --> 01:24:38,740
if a person's in quasi steady state, you could sequence them every quarter and basically update

952
01:24:38,740 --> 01:24:44,660
your pretest probabilities of what the distribution is. That's right. And another tier of complexity,

953
01:24:44,660 --> 01:24:50,660
because it's good that you're a skeptic on this, but early on these various companies that would

954
01:24:50,660 --> 01:24:56,100
do microbiome assessment, they just said how much you have of this bacteria or that bacteria. It was

955
01:24:56,100 --> 01:25:01,460
like a density of bacteria. Turns out that you touched on it. If you sequence the bacteria,

956
01:25:01,460 --> 01:25:07,540
the changes in that bacteria species sequence is just as important as the density of the type of

957
01:25:07,540 --> 01:25:13,060
bacteria. So this is not easy. It's expensive to do it right. And we've already seen a big fraud

958
01:25:13,060 --> 01:25:18,100
on this front quite recently, right? Yeah. You buy them. They basically are anos of this space.

959
01:25:18,100 --> 01:25:22,740
It's yeah. Well, they, they were billing people illegitimately and they were only reporting on

960
01:25:22,740 --> 01:25:27,460
density of bacteria. I don't know that they're reporting object fraud. It was just bad practices.

961
01:25:27,460 --> 01:25:32,660
Yeah. I don't think they were doing things wrong with respect to the microbiome density,

962
01:25:32,660 --> 01:25:38,020
which is very rudimentary. They didn't do sequencing. They did basically bacterial density.

963
01:25:38,340 --> 01:25:42,100
I mean, I found them to be the most useless company in the history of civilization because

964
01:25:42,660 --> 01:25:50,340
back in 2012, 2011, I was, I mean, at least acting like I was on the forefront of this,

965
01:25:50,340 --> 01:25:55,460
trying to understand it and ordering these sequences on myself and all my patients. And

966
01:25:56,180 --> 01:25:59,540
I don't understand how this company stayed in business. I mean, they, they didn't, but

967
01:25:59,540 --> 01:26:03,940
they couldn't run a sequence to save their lives. Well, yeah, I think the biggest thing is they were

968
01:26:03,940 --> 01:26:08,980
fraudulently billing people, double billing, triple billing, you know, all that sort of thing.

969
01:26:08,980 --> 01:26:13,460
That's what got them into, you know, basically collapse mode. I don't know enough about their

970
01:26:13,460 --> 01:26:18,020
sequencing. I mean, I always found Larry Smarr's stuff to be the most interesting because Larry's

971
01:26:18,020 --> 01:26:22,580
doing it at a level that you couldn't do commercially. Yeah. So shotgun sequencing,

972
01:26:22,580 --> 01:26:27,380
where you do true metagenomics, you know, there are only certain labs like the one I mentioned

973
01:26:27,380 --> 01:26:34,740
in Wiseman here in San Diego, the Knight lab does metagenomics. That is K-N-I-G-H-T,

974
01:26:34,740 --> 01:26:40,580
Robert Knight. So these are the centers that are doing it right, that are sequencing each species

975
01:26:40,580 --> 01:26:47,300
of every organism that's found. And we now know that though that sequence is equally as important

976
01:26:47,300 --> 01:26:52,500
as the type of bacteria. So that's the sort of data. Now, the other thing you're bringing up

977
01:26:52,500 --> 01:26:58,740
that's really important is we have no idea how to manipulate the gut microbiome. The only thing

978
01:26:58,740 --> 01:27:04,260
we know is a fecal transplant in certain people with, you know, pseudomembranous, colitis, C. difficile.

979
01:27:04,260 --> 01:27:10,900
Outside of that, we don't have, we have crapsules that are being made that are being tested.

980
01:27:10,900 --> 01:27:15,380
So I think you and I definitely see probably more closely on this than I would have guessed

981
01:27:15,380 --> 01:27:20,820
initially, because we agree that at this point, it's an output of data, not an input to manipulate

982
01:27:20,820 --> 01:27:25,220
necessarily. So I probably need to go back and look at the Wiseman paper again, because I don't

983
01:27:25,220 --> 01:27:31,140
think I've looked at it in over a year. And my view was, which is probably incorrect by the way,

984
01:27:31,780 --> 01:27:38,740
that CGM and dietary logging would have been sufficient. So what I really want to do is go

985
01:27:38,740 --> 01:27:44,180
back and look at that paper and see what the gut biome added above those things, which I'm guessing

986
01:27:44,180 --> 01:27:49,300
there is something there. Yeah, no, there is. And we need more. I mean, basically right now,

987
01:27:49,300 --> 01:27:53,860
you could predict if you had all the data and the right algorithms, you could predict

988
01:27:53,860 --> 01:27:59,700
which foods you'll spike from. And then this was taken to another level by the group in London,

989
01:27:59,700 --> 01:28:04,980
King's College led by Tim Spector. He brought in all these twins from all over the country,

990
01:28:04,980 --> 01:28:11,220
identical twins. So they had their gut microbiome. And they also put in a line to a vein to get blood

991
01:28:11,220 --> 01:28:15,860
samples for triglycerides. And they saw the same thing. Which by the way, you could get out of a

992
01:28:15,860 --> 01:28:22,580
sensor. Ultimately. Yeah. We don't have one yet. No. And you know why I mentioned lactate earlier,

993
01:28:22,580 --> 01:28:28,420
if you have real time lactate, you are estimating with really great precision mitochondrial

994
01:28:28,420 --> 01:28:35,060
oxidation. Now you understand fuel partitioning. You see, to me, if you asked me a year ago,

995
01:28:35,060 --> 01:28:38,900
how would you want to best estimate fuel partitioning? I'd say, oh, it's tough because you got to have

996
01:28:38,900 --> 01:28:44,820
somebody basically walk around with a respirator or something that can measure oxygen consumption

997
01:28:44,900 --> 01:28:50,020
and CO2 production. But I think lactate is telling you that. I think if you really know how to

998
01:28:50,020 --> 01:28:58,020
calibrate lactate, you can estimate fat oxidation versus glycolysis and or glycolysis through to

999
01:28:58,020 --> 01:29:02,820
lactate. And so all of a sudden you now get into this. So the reason that right now my glucose is

1000
01:29:02,820 --> 01:29:09,620
94, but if I ate a bagel, it would go to like 104 is because I'm so glycogen depleted because I

1001
01:29:09,620 --> 01:29:14,660
haven't eaten in 20 hours and I've worked out very hard or at least for a long enough duration.

1002
01:29:15,220 --> 01:29:20,340
Conversely, it's not uncommon after dinner. Let's say you have dinner, you have a glucose spike up

1003
01:29:20,340 --> 01:29:27,380
to 120 and then it comes back down to 90, 94. You eat that same bagel, you'll go higher. Well,

1004
01:29:27,380 --> 01:29:34,260
a very important input into predicting that is knowing glycogen stores and insulin sensitivity

1005
01:29:34,260 --> 01:29:40,260
of the muscle and all these other things. So what I need to better get is how many different

1006
01:29:40,260 --> 01:29:47,060
phenotypes, macro phenotypes of gut biome are there that really matter? Right. The bigger picture

1007
01:29:47,060 --> 01:29:52,020
though, I agree with all your point and it'd be nice to see a lactate sensor that's tested at

1008
01:29:52,020 --> 01:29:57,700
scale and is accurate. It took a while to get that for glucose and we're at the earliest stages on

1009
01:29:57,700 --> 01:30:03,060
the lactate. But there's still a lot of naysayers here and I understand their perspective and that

1010
01:30:03,060 --> 01:30:08,180
is so what? So what if your glucose goes to 180 or your lactate goes to this or your triglycerides

1011
01:30:08,180 --> 01:30:14,660
go to that? The point is, do we know that changing that, that keeping everything nice level?

1012
01:30:15,460 --> 01:30:20,900
You don't think the diabetes literature has made it clear enough that normalizing glucose and insulin

1013
01:30:20,900 --> 01:30:28,100
is beneficial? Not enough. No. The only way you can get at this, inferentially, yes. But you know

1014
01:30:28,180 --> 01:30:35,380
what? Oh, you're saying I can tell the story that a glucose of 120 is better than 180 because I have

1015
01:30:35,380 --> 01:30:41,220
clinical trial data to demonstrate that all day long and I can even tell you that how you achieve

1016
01:30:41,220 --> 01:30:49,460
that matters. But you're saying I don't have the data to tell you that 100 is better than 110.

1017
01:30:49,460 --> 01:30:56,180
No, another way to put it is I don't have the data to show that if you wear a sensor for x number

1018
01:30:56,180 --> 01:31:02,500
of time, 90 days in your case or forever or a week, that you're learning about avoidance of

1019
01:31:02,500 --> 01:31:07,860
glucose spikes changes your prognosis. We don't know that. And the same thing for triglycerides,

1020
01:31:07,860 --> 01:31:13,940
which by the way, they don't correlate and we're learning that each person's individual response

1021
01:31:13,940 --> 01:31:20,260
throughout their day is so incredibly unique. And we're learning some of the factors. I don't even

1022
01:31:20,260 --> 01:31:24,500
know what we know, all the factors that influence that. You've mentioned, of course, glycogen stores

1023
01:31:24,500 --> 01:31:30,100
and physical activity and the microbiome. And cortisol in my experience plays a staggering

1024
01:31:30,100 --> 01:31:35,140
role. No question, stress. I mean, just if you get an intermittent intervening cold,

1025
01:31:35,140 --> 01:31:41,940
no less stress in your family, your life, whatever experience. So yeah, this is a really

1026
01:31:41,940 --> 01:31:46,580
interesting area where learning about ourselves is like know thyself sort of thing. We're in the

1027
01:31:46,580 --> 01:31:52,180
early stages and I see the skeptics. I understand their perspective. I think that we have to prove

1028
01:31:52,180 --> 01:31:59,220
it. I lean where you are, which is why not have this information? I've learned a lot about

1029
01:31:59,220 --> 01:32:04,180
myself, no less the feel from it. But I think we have to admit that we got a ways to go.

1030
01:32:05,060 --> 01:32:10,340
What do you think is a significant blind spot in medicine today? At the macro level and at any

1031
01:32:10,340 --> 01:32:17,620
level? No, I think the biggest blind spot is how poor we are in diagnosis, no less in treatment.

1032
01:32:17,780 --> 01:32:23,220
I mean, I think that when you really look hard at the data, what's amazing Peter is you see all

1033
01:32:23,220 --> 01:32:29,460
these clinical trials that declare a triumph and they're helping three out of a hundred people.

1034
01:32:29,460 --> 01:32:35,940
I mean, a great example of statins, primary prevention of statins, if not the number one,

1035
01:32:35,940 --> 01:32:42,260
close to number one class of drugs that are used today. And you see that out of a thousand people

1036
01:32:42,340 --> 01:32:49,540
for primary prevention, 988 derive no benefit for five years of taking a statin and 12 out of a

1037
01:32:49,540 --> 01:32:54,900
thousand get benefit. So whether you look at the diagnosis where, you know, if a doctor...

1038
01:32:54,900 --> 01:32:59,620
That's another topic I know that we may disagree on. My view on that has always been that

1039
01:33:00,500 --> 01:33:05,060
because the time course of atherosclerosis is so long, you know, it's a disease that begins in

1040
01:33:05,060 --> 01:33:05,780
infancy. Sure.

1041
01:33:05,860 --> 01:33:11,860
We certainly know from, you know, the starry stuff of the, you know, the seventies that, you know,

1042
01:33:11,860 --> 01:33:17,300
basically by the time you're 18 years old, you've most people have a stage three lesion at that

1043
01:33:17,300 --> 01:33:22,580
point. That the challenge with studying primary prevention is you could never study it long enough

1044
01:33:22,580 --> 01:33:27,860
to really see where those curves start to diverge. Well, no, they diverge, but the question is,

1045
01:33:27,860 --> 01:33:31,860
are they going to keep diverging? And, you know, most of the benefits starts to kick in

1046
01:33:31,860 --> 01:33:36,580
right around 18 months. And yes, they're still slightly diverging, you know, after five years,

1047
01:33:36,580 --> 01:33:41,380
but we don't have any data beyond that. What I guess I should restate that is that we don't

1048
01:33:41,380 --> 01:33:47,460
have any proof that more, you know, you can suggest that instead of 18 out of a thousand

1049
01:33:48,260 --> 01:33:53,460
people benefit that it goes to 36. Exactly. But what about the 970? They don't derive it.

1050
01:33:53,460 --> 01:33:59,540
No, no, no, that's fair. I think the point is, do you believe the Mendelian randomizations?

1051
01:33:59,540 --> 01:34:05,220
Or do you think that the Mendelian randomizations have artifacts in them, which any Mendelian

1052
01:34:05,220 --> 01:34:10,340
randomization will have an artifact if that which changes the variable of interest also changes

1053
01:34:10,340 --> 01:34:13,620
something else that you don't know? That's always, that's, that's the blind spot of the Mendelian

1054
01:34:13,620 --> 01:34:17,860
randomization. No, they're, they're a neat way to get a readout, but they're not perfect. You know,

1055
01:34:17,860 --> 01:34:23,460
I think that whether you look at diagnosis where, you know, if you take people who have died under

1056
01:34:23,540 --> 01:34:31,300
a doctor's care and you say, why did that person die? What was the cause of death? 40% of doctors

1057
01:34:31,300 --> 01:34:40,340
say, I absolutely know the answer, are wrong. 40%. That's how many it autopsy or show a different

1058
01:34:40,340 --> 01:34:46,100
reason for the death as what was pre-mortem. Wow. That's, that's a, that's a, that's a big gap.

1059
01:34:46,100 --> 01:34:51,700
Yeah. If you ask doctors to make a diagnosis, if they don't think about it in the first five minutes,

1060
01:34:52,500 --> 01:34:57,540
five minutes, 95% accurate. But if they don't, it doesn't come to mind in the first five minutes,

1061
01:34:57,540 --> 01:35:04,660
it drops down to 24% accuracy. Basically what we have is type one system, type one thinking system,

1062
01:35:04,660 --> 01:35:11,460
one thing I should say, Kahneman's work. And we just are reflexive. We don't reflectively go over

1063
01:35:11,460 --> 01:35:17,060
anything. We don't have time. We don't simulate all the data we can because it's so much. And so

1064
01:35:17,140 --> 01:35:24,500
we have to basically the whole, the big hole is acknowledge that we can do far better and it can

1065
01:35:24,500 --> 01:35:31,300
all be through human support. We need help. Do you think there's any area where I think you've made

1066
01:35:31,300 --> 01:35:38,100
a compelling case that machine plus human should be better than human? Eventually it will be. I

1067
01:35:38,100 --> 01:35:42,900
mean, I think even in radiology today it should be. Oh yeah. Yeah. Hopefully critical care would

1068
01:35:42,900 --> 01:35:47,620
be an amazing place where machine plus human should be better than human. Do you think that

1069
01:35:47,620 --> 01:35:51,700
there are extremes on either way? Do you think there are places where humans will always be

1070
01:35:51,700 --> 01:35:59,140
better than machines plus humans? Yes. And that's in being human, which is the bond. Just like our

1071
01:35:59,140 --> 01:36:06,580
conversation, this deep conversation, it's really illustrates the human connection. We don't have

1072
01:36:06,580 --> 01:36:12,180
those kinds of conversations in seven minutes or 10 minutes where the patient can't. And you can't

1073
01:36:12,260 --> 01:36:17,780
get to know a person's life history. That's never going to get really digitized story, their story.

1074
01:36:17,780 --> 01:36:23,460
We interrupt patients within 18 seconds from listening. We don't listen. So the point being

1075
01:36:23,460 --> 01:36:28,580
is that machines, that's the, what we don't want machines. They don't have context. They're not

1076
01:36:28,580 --> 01:36:36,340
going to be able to truly understand all the nonverbal communication and the real issues of a person

1077
01:36:36,340 --> 01:36:42,340
that are deep. And that's where the humanity, we need to bring it back. I mean, the essence of

1078
01:36:42,340 --> 01:36:49,300
medicine is that and it's been lost. It's become this big business and it's. Do you worry about the,

1079
01:36:50,180 --> 01:36:54,500
I don't know, well, it's not really a brain drain, but do you worry that the war for talent has

1080
01:36:54,500 --> 01:37:00,580
shifted? And back when you went into medicine, it's probably safe to assume you were one of the best

1081
01:37:00,580 --> 01:37:05,220
students in your high school, the best student in college. Like it would have been skimming at the

1082
01:37:05,300 --> 01:37:10,740
very, very top of the pyramid of students. Is that still the case today? Or has, I mean,

1083
01:37:10,740 --> 01:37:15,860
you've already alluded to a number of things that I've seen. Luckily I don't experience them just on

1084
01:37:15,860 --> 01:37:21,700
the nature of my practice because it's private. But I mean, these stories you tell, I know so well,

1085
01:37:21,700 --> 01:37:25,860
the doctor that gets seven minutes to see a patient and in that seven minutes, six minutes

1086
01:37:25,940 --> 01:37:36,340
is typing into an EMR. The moral distress and the absolute erosion of a belief in what you're doing

1087
01:37:36,340 --> 01:37:42,740
is a huge cause of burnout amongst physicians. And I don't understand like if you're the top

1088
01:37:42,740 --> 01:37:47,380
student in college and you're interested in life sciences, why you'd pick medicine today, unless

1089
01:37:47,380 --> 01:37:52,660
you had a profound confidence that you could carve a path distinct from what most are probably going

1090
01:37:52,740 --> 01:37:58,500
to do. I mean, you have to be an optimist, I think, to pursue medicine today. Do you worry

1091
01:37:58,500 --> 01:38:04,580
that it's going to be hard to recruit the most talented kids out of college into medical school?

1092
01:38:04,580 --> 01:38:11,300
Well, I'm hoping it won't be, but I do hear constantly friends who are doctors who tell

1093
01:38:11,300 --> 01:38:19,540
their kids, don't go into medicine. Yes, I hear that. That's really bad because here is the ultimate

1094
01:38:19,540 --> 01:38:25,300
profession for sense of really helping people. And then you have the people who are in it saying

1095
01:38:25,300 --> 01:38:30,740
it's horrible. And as you know, Peter, the physician burnout, no less all clinician burnout,

1096
01:38:30,740 --> 01:38:38,260
is at peak. And why is that? It's because it become data clerks and they're squeezed for the time.

1097
01:38:38,260 --> 01:38:44,900
They can't care for people when you don't have time. So this is, of course, going back to that

1098
01:38:44,900 --> 01:38:52,020
main thesis of gift of time, we can get that. But I think we have to restore medicine the way

1099
01:38:52,020 --> 01:38:58,740
it was in order to attract the talent that you're referring to. And I think it's doable. It's not

1100
01:38:58,740 --> 01:39:03,220
going to be easy. It's going to require a lot of activism, which we haven't had that much of in

1101
01:39:03,220 --> 01:39:09,860
medicine. Yeah, this is something doctors are quite ill-equipped to do, it seems. We're seeing

1102
01:39:09,860 --> 01:39:15,300
the light on activism. The gun control, NRA really brought it out when they said,

1103
01:39:15,300 --> 01:39:20,420
stay in your lane. This was in the AMA said a physician should be asking a patient if they

1104
01:39:20,420 --> 01:39:25,860
own a gun. It was America College of Physicians. They published it in the Annals Internal Medicine

1105
01:39:25,860 --> 01:39:32,180
last fall. And then NRA said, these doctors should stay in their lane. And then you had all the

1106
01:39:32,180 --> 01:39:38,020
doctors came out, one of them Judy Melanik saying, this is my fucking lane. And it went everywhere.

1107
01:39:38,900 --> 01:39:42,340
The idea being if doctors are going to be killed potentially by patients,

1108
01:39:42,340 --> 01:39:46,900
it's not an unreasonable question to say. That and caring for all the gunshot victims.

1109
01:39:48,340 --> 01:39:52,180
And the suicides. And the suicide. Which is probably the greatest cause of gun

1110
01:39:52,180 --> 01:39:57,620
killing. Yeah, you've got both suicides and homicides and mass killings and AR-15s and all

1111
01:39:57,620 --> 01:40:03,380
this stuff. So the fact that this was taboo, that doctors weren't allowed to talk to patients and

1112
01:40:03,380 --> 01:40:07,620
they weren't allowed to do research. I mean, there was no research in this area.

1113
01:40:07,620 --> 01:40:14,260
So this was a void that now has brought out a lot of the activists and social media. And

1114
01:40:14,260 --> 01:40:21,460
it's this new era of young physicians, a lot of them women. And we're seeing activism like never

1115
01:40:21,460 --> 01:40:26,100
before. I wrote about it in the New Yorker recently about this and now should doctors organize and

1116
01:40:26,100 --> 01:40:31,540
this what we're seeing, it is happening. And the hope is that we're going to see an organization

1117
01:40:31,540 --> 01:40:38,900
take hold where all doctors can join as well as ultimately patient advocates and others. And

1118
01:40:38,900 --> 01:40:44,740
that we will turn this around because this is the biggest concern I have, Peter, is that we're

1119
01:40:44,740 --> 01:40:50,820
going to see AI kick in more and more over the years. But what will it do if the administrators,

1120
01:40:51,380 --> 01:40:58,100
who are the overlords, who are overrepresented as compared to the people taking care of patients,

1121
01:40:58,820 --> 01:41:04,740
if they keep the squeeze on, we're just going to see things get worse. So we have to override that.

1122
01:41:04,740 --> 01:41:08,980
And the only way we're going to do that and turn inward and get the humanity back in medicine

1123
01:41:09,620 --> 01:41:16,660
is to have doctors organize. And the gravitas of a million doctors in America, all being part of one

1124
01:41:16,660 --> 01:41:22,100
entity, it could be enormous. You know, and this, this gets to another problem within medicine that

1125
01:41:22,100 --> 01:41:28,100
I alluded to earlier, which is the patients aren't footing the bill based on the system directly

1126
01:41:28,660 --> 01:41:32,980
and therefore demand in a demand-based system. So if it's not a demand-based system, it doesn't

1127
01:41:32,980 --> 01:41:38,660
matter. Like in other words, in the NHS or the Canadian system, the patient is not footing the

1128
01:41:38,660 --> 01:41:43,460
bill, but they're also not driving the demand. The demand is budget set. But in the US where you have

1129
01:41:43,460 --> 01:41:48,580
this paradox, it also means the patient's voice doesn't matter. And that's the irony of it, right?

1130
01:41:48,580 --> 01:41:55,060
So it's the opposite of the DC license plate, you know, which is taxation without representation.

1131
01:41:56,260 --> 01:42:03,140
It's like no taxation and no representation. And that's why I don't, I worry that patients can't

1132
01:42:03,140 --> 01:42:09,620
be the ones to drive this change, which they should be able to, like the patients should demand

1133
01:42:09,620 --> 01:42:15,060
this change, but because they're not the ones writing the checks to feed that $3.8 trillion

1134
01:42:15,300 --> 01:42:20,260
machine directly, they're only doing it indirectly. In other words, they don't get to control it,

1135
01:42:20,260 --> 01:42:26,100
right? They're paying their taxes and their employers are withholding it, but it's not the

1136
01:42:26,100 --> 01:42:31,780
same as saying, here's my dollar, go and do this thing with it. Yeah, no, that's why if you get the

1137
01:42:31,780 --> 01:42:38,180
doctors to come together to start this and the sole purpose is the patient-doctor relationship.

1138
01:42:38,180 --> 01:42:44,260
It's not about better reimbursement or all the other trade guild activities, but rather it's

1139
01:42:44,260 --> 01:42:49,860
about we want to fix this relationship and bring the humanity back in medicine. Then we start to

1140
01:42:49,860 --> 01:42:55,060
see, you know, the ability for that patient interest to be recognized. Because now all you

1141
01:42:55,060 --> 01:43:00,020
have is you got a lot of patient advocacy groups, but they're, you know, they're just like the doctor

1142
01:43:00,020 --> 01:43:06,900
organizations, they're all balkanized. We need one entity to stand strong and I hope we'll get there.

1143
01:43:06,900 --> 01:43:10,260
Well, that's a good point, right? It should be made up of physicians and patients. Yes.

1144
01:43:10,260 --> 01:43:12,420
Actually, it really shouldn't be, they shouldn't be separate.

1145
01:43:12,420 --> 01:43:16,660
No, no, I think, you know, it started with the physicians because, you know, there's just a

1146
01:43:16,660 --> 01:43:22,420
million of them that we can identify and get them together as many as possible. Then you start adding

1147
01:43:22,420 --> 01:43:26,580
on. Are there really a million physicians in the United States? Yeah, you know, the fact checking

1148
01:43:26,580 --> 01:43:31,220
of the New Yorker is amazing when they, I've never experienced that before, but they tracked down

1149
01:43:31,220 --> 01:43:36,900
everything and they got to the numbers that I never could get to. So not all of them are practicing.

1150
01:43:36,900 --> 01:43:42,660
There's about 900, almost 900,000 are actually practicing some respect, but there are a million

1151
01:43:42,660 --> 01:43:49,540
docs in this country. Have you done the math about how many doctors we would need under this new regime

1152
01:43:49,540 --> 01:43:56,740
of empathic, intelligent, artificial, this symbiotic relationship? In other words, because

1153
01:43:57,540 --> 01:44:02,740
there's a bunch of moving pieces, right? It's radiologists still exist, but now they're there

1154
01:44:02,740 --> 01:44:08,580
to talk more with patients and interpret the diagnosis as opposed to make the diagnosis,

1155
01:44:08,580 --> 01:44:14,100
right? And you're still going to need the internist, but now they have more time with the patient and

1156
01:44:14,100 --> 01:44:18,740
they don't have to worry about the diagnosis as much as they have to worry about the treatment.

1157
01:44:19,300 --> 01:44:24,740
Directionally, Eric, do we need the same number of physicians? Are we going to need more physicians

1158
01:44:24,740 --> 01:44:29,860
or are we going to need less physicians in 30 years on a per patient basis? Yeah, you know,

1159
01:44:29,940 --> 01:44:36,180
when I did the UK review, we got into that and we had economists and all sorts of brilliant

1160
01:44:36,180 --> 01:44:42,260
people modeling on that. And I think what we'll see is even though everything now would suggest

1161
01:44:42,260 --> 01:44:46,580
we need a lot more doctors because of the aging population and all the comorbidities and the

1162
01:44:46,580 --> 01:44:50,740
complexity. I mean, if you just look at like, for example, how do you care for a patient with

1163
01:44:50,740 --> 01:44:57,460
cancer today? It's gotten very complex. So you would project we're going to need a steep growth

1164
01:44:57,460 --> 01:45:02,180
curve, but what we're going to see, I think is a big blunting of that because we are going to be

1165
01:45:02,820 --> 01:45:08,500
the machine story is not just about doctors relying more on machines, getting support.

1166
01:45:08,500 --> 01:45:14,260
It's also about consumers, patients. And so when you get the outsourcing and the offloading,

1167
01:45:14,260 --> 01:45:18,980
you start to see a pretty big, and then when you get rid of the hospital story and just have

1168
01:45:18,980 --> 01:45:25,300
surveillance centers, remote monitoring, you start to see a very less need for expansion. So I don't

1169
01:45:25,300 --> 01:45:31,220
see what we're going to be a decline, but just a difference in the curves as they go forward over

1170
01:45:31,220 --> 01:45:36,340
the next few decades. And there will also be this, I don't know, for lack of a better word,

1171
01:45:36,340 --> 01:45:41,940
kind of a growing pain as people transition. I mean, most of the radiologists I know today,

1172
01:45:42,820 --> 01:45:46,100
you know, for example, have very technical backgrounds. I mean, any of the MRI

1173
01:45:46,820 --> 01:45:51,300
folks I know, they usually have a great background in physics and things like that. So

1174
01:45:51,300 --> 01:45:54,580
all of a sudden there's going to be a different selection criteria. For example,

1175
01:45:54,580 --> 01:45:59,540
you may want to choose radiology independent of how technical your background is in physics

1176
01:45:59,540 --> 01:46:05,620
or mathematics. And so it's like it takes a generation to make these switches. Do you see

1177
01:46:05,620 --> 01:46:10,980
any other types of changes in how people will select into different specialties?

1178
01:46:11,700 --> 01:46:18,020
Well, that's hard to know. I think we have theorized, or John and I from Penn radiology,

1179
01:46:18,020 --> 01:46:22,980
that there might be a new specialty that would just be radiology and pathology combined,

1180
01:46:22,980 --> 01:46:28,980
at least the pathologists who work with slides because it's very similar interaction with the

1181
01:46:28,980 --> 01:46:33,540
computer. And they're often very much, as you know, integrated. So that might be a whole new

1182
01:46:33,540 --> 01:46:39,380
specialty over time. But you know, overall, one thing we don't want to forget here is that

1183
01:46:39,380 --> 01:46:45,620
the empowerment of patients to do doctorless diagnoses of most common conditions, whether

1184
01:46:45,620 --> 01:46:52,100
that's an ear infection of a child or a urinary tract infection, a skin rash or skin lesion,

1185
01:46:52,100 --> 01:46:57,780
and on and on. The routine things are not going to have doctor in the loop. Only if, you know,

1186
01:46:57,780 --> 01:47:02,180
treatment is needed, perhaps. And that's only in the US, not in a lot of other countries. So

1187
01:47:02,740 --> 01:47:07,380
that is going to change also specialties. Because today you look at pediatrics, you know, it's a

1188
01:47:07,380 --> 01:47:14,020
wonderful specialty, but a lot of that could be decompressed if you give parents more autonomy

1189
01:47:14,020 --> 01:47:20,180
for their kids. So we're going to see lots of changes based on the patient side or the parent

1190
01:47:20,180 --> 01:47:25,700
side of things, which I think has not been adequately appreciated. If you could conduct

1191
01:47:25,700 --> 01:47:32,900
any experiment and there were no limits on the resources you had. So you could, and it could be

1192
01:47:32,900 --> 01:47:40,340
an experiment within the real recesses of basic science. It could be a real translational

1193
01:47:40,340 --> 01:47:45,700
experiment that takes something from the cutting edge of the bench to the bedside, or it could be

1194
01:47:45,700 --> 01:47:53,380
the largest clinical trial ever done to test a question that vexes you without any new introduction

1195
01:47:53,380 --> 01:47:57,780
of a new technology, but just simply asking a question like the Vioxx one, for example, you know,

1196
01:47:57,780 --> 01:48:03,620
those take as much time as you want, but I'd love to know what would be a dream experiment for you

1197
01:48:03,620 --> 01:48:10,260
if this is the one shot on goal where you've got billions of dollars and no holds barred.

1198
01:48:10,260 --> 01:48:16,580
Yeah, no, it's easy. Yeah, for me, I mean, I mean, it's a dream. You know, I wrote about this with

1199
01:48:16,580 --> 01:48:22,260
Kai-Fu Lee and Nature Biotech last month, and it was called It Takes a Planet. And basically,

1200
01:48:22,260 --> 01:48:29,060
it would be to the experiment would be to develop a planetary digital infrastructure with all the

1201
01:48:29,060 --> 01:48:36,180
data of each individual and continually being assessed and assimilated process inputs,

1202
01:48:36,180 --> 01:48:41,140
federated AI. So the data never leaves the country, whether it's China or the US or wherever.

1203
01:48:41,140 --> 01:48:47,620
So it's not a privacy security. But the point being is, when a new person comes in, and we want to

1204
01:48:47,620 --> 01:48:54,020
prevent a condition or better treat it, and we have billions of other people to draw from. And we have

1205
01:48:54,020 --> 01:49:00,500
these digital twins, if you will, because today we learn from clinical trials. And it's really

1206
01:49:00,500 --> 01:49:05,220
farcical in many respects, because those clinical trials are contrived. And the benefit is three per

1207
01:49:05,300 --> 01:49:09,460
hundred or something like that. What about the other people? Well, not only that, it's often three

1208
01:49:09,460 --> 01:49:14,100
per hundred because of the heterogeneity of the population. Exactly. It might be 30 per hundred

1209
01:49:14,100 --> 01:49:18,900
if you knew who to apply it to. Yeah. So my experiment would be just for that you have

1210
01:49:18,900 --> 01:49:26,180
pinpoint precision, because I know Peter's twins, all of them around the world, and what treatment

1211
01:49:26,180 --> 01:49:31,540
they got and what outcomes they got, and how I could prevent their issue that they otherwise had.

1212
01:49:31,540 --> 01:49:35,700
And so it would be to develop the ultimate learning health system. And the twin you're

1213
01:49:35,700 --> 01:49:43,380
defining is obviously not just genetic, but it's every layer. So it's my gut twin, my epigenetic

1214
01:49:43,380 --> 01:49:51,620
twin, or my approximate genetic twin, my phenotype twin, my metabolic twin. Right. And that's, I think,

1215
01:49:51,620 --> 01:49:57,860
where we can go in the future. And it involves many different types of AI. And I think we'll get

1216
01:49:57,940 --> 01:50:02,100
there someday. And it's an experiment. Is that a 50 year? I mean, what realistically is...

1217
01:50:02,740 --> 01:50:08,980
I think it could be done in 20 if we really were going after it, because it's doable. The question

1218
01:50:08,980 --> 01:50:13,780
is... That strikes me as bigger than any one country though, right? No, no, but if you get

1219
01:50:13,780 --> 01:50:19,300
US and China to start it because of the diverse population and the largest, the US being third in

1220
01:50:19,300 --> 01:50:25,060
population in the world, and you get that going and the rest of them join on, pretty quickly you

1221
01:50:25,060 --> 01:50:30,340
will have twins. Do you have a sense of how much that could cost per person? It just depends on the

1222
01:50:30,340 --> 01:50:36,100
layers of data and the analytics. I mean, it's not going to be trivial, but less than you would think.

1223
01:50:36,100 --> 01:50:44,260
This is mostly in silico work. It's not... It's happening anyway. This data all sits in places

1224
01:50:44,260 --> 01:50:51,860
that's all fragmented. What's the price of a full exome sequence today? Both, a full genome sequence

1225
01:50:51,860 --> 01:50:56,500
today. So not a 23 and me, but where they're doing the full sequence. Yeah, well, exome 400,

1226
01:50:56,500 --> 01:51:03,780
a full genome 8, 900. So they're sub a thousand. They're going to keep coming down. And at scale,

1227
01:51:03,780 --> 01:51:09,940
perhaps that's going to happen even faster. But you start having all these things done at scale.

1228
01:51:11,300 --> 01:51:17,140
Right now, we don't have enough... The reason why a genome isn't valuable is because we don't have

1229
01:51:17,140 --> 01:51:22,020
a billion people with a sequence and a phenotype. Once we start to get into those big numbers,

1230
01:51:22,660 --> 01:51:26,580
then we start to... Do you believe... So you don't think it's that the genome is not

1231
01:51:26,580 --> 01:51:30,900
deterministic enough. You think it's too small an N so far? That's a bigger problem?

1232
01:51:30,900 --> 01:51:35,940
Big part of it, yes. I mean, a genome will never be fully deterministic. I mean, that's not possible,

1233
01:51:35,940 --> 01:51:42,180
probabilistic, yes. But the probabilistic side of it is hampered because of inadequate numbers.

1234
01:51:42,180 --> 01:51:47,380
Yeah, because I got to tell you right now, I find every time my patient sends me their genome,

1235
01:51:47,380 --> 01:51:51,380
I just roll my eyes and say like... No, there's a limited amount of value.

1236
01:51:51,380 --> 01:51:53,380
Yeah. Well, you know what I usually say to them? I say, look,

1237
01:51:54,260 --> 01:51:59,860
anything in here that matters, we already know. If you're a 40 year old and unless you were adopted,

1238
01:51:59,860 --> 01:52:04,980
sometimes you figure out this person has Lynch syndrome or something that you had to know.

1239
01:52:04,980 --> 01:52:09,460
So there are some numbers on that. If you look at the Danville, Pennsylvania,

1240
01:52:09,460 --> 01:52:15,620
Geisinger Health System, where they've done over 100,000 people, they have exomes, not full

1241
01:52:15,620 --> 01:52:21,700
genomes, but decoding elements and they have 5% that they find something quite important,

1242
01:52:21,700 --> 01:52:27,780
so-called pathogenic. So like Lynch syndrome or BRCA or sudden death, arrhythmia.

1243
01:52:27,780 --> 01:52:32,340
Which again, my point is if you're doing your job as a doctor, you should have figured that

1244
01:52:32,340 --> 01:52:36,740
out in the family history and gone looking for it, but not with... In other words, you should have

1245
01:52:36,740 --> 01:52:40,020
gone to the genome to be confirming what you suspected, hopefully.

1246
01:52:40,020 --> 01:52:45,620
Yeah, but a lot of that's missed. Like BRCA is a perfect example. What about BRCA men carriers?

1247
01:52:46,580 --> 01:52:47,460
You just don't know.

1248
01:52:47,460 --> 01:52:51,940
And you're taking a good family history, don't you? I guess it depends on how much the patients

1249
01:52:51,940 --> 01:52:57,060
know about their family too, but you're right. No, those are good examples. But when I look at the...

1250
01:52:57,060 --> 01:53:00,820
Like how many times do I look at Prometheus and it spits out, oh, you're at a higher risk

1251
01:53:00,820 --> 01:53:05,460
for atherosclerosis and a higher risk for diabetes. And I'm like, this is such nonsense,

1252
01:53:05,460 --> 01:53:09,540
right? Like if you actually understand how to evaluate lipids and you're wearing a CGM,

1253
01:53:09,540 --> 01:53:11,460
you certainly don't need this thing to tell somebody.

1254
01:53:11,460 --> 01:53:17,460
No, Prometheus, I mean, I think there's gross deficiencies of outputs because again, going back

1255
01:53:17,460 --> 01:53:22,500
to we don't have one central repository of data.

1256
01:53:22,500 --> 01:53:23,300
Do you know what the denominator is?

1257
01:53:23,300 --> 01:53:23,940
Oh gosh.

1258
01:53:23,940 --> 01:53:25,060
How small is it?

1259
01:53:25,060 --> 01:53:31,140
In terms of how many millions of people have had sequence, it's small still. It's in the 10...

1260
01:53:32,100 --> 01:53:37,540
Well, it's in less than 10 million for sure of whole genome sequence.

1261
01:53:37,540 --> 01:53:42,420
And then of course, how many do we have accurate phenotyping on? Because if the phenotype is

1262
01:53:42,420 --> 01:53:46,180
not that accurate, then it dilutes the quality of what you're trying to do, right?

1263
01:53:46,180 --> 01:53:52,100
That's essential because what phenotype we have is fixed at the moment the genome was assessed.

1264
01:53:52,100 --> 01:53:52,500
Exactly.

1265
01:53:52,500 --> 01:53:55,460
We don't know what the phenotypes change. So all the studies that we have...

1266
01:53:55,460 --> 01:53:57,860
So this has to be living, breathing and longitudinal.

1267
01:53:57,860 --> 01:54:01,460
Exactly. And that's why I'm trying to see our way through...

1268
01:54:02,500 --> 01:54:06,820
The all of us study of the million people that we're onto right now is the beginning of something

1269
01:54:06,820 --> 01:54:11,860
like that where all the layers of data for long-term follow-up, it's still tiny, 100,000.

1270
01:54:11,860 --> 01:54:16,820
I mean, a million people is tiny, but it's a start. But if we could get the leading

1271
01:54:16,820 --> 01:54:23,780
countries in the world to get behind this, this is something that should override concerns about

1272
01:54:23,780 --> 01:54:29,940
competition in countries. This is about for mankind, humankind. Then we might be able to

1273
01:54:30,500 --> 01:54:36,820
really develop something that would promote health of all human beings. That would be far-reaching.

1274
01:54:36,820 --> 01:54:40,740
And you know what? I actually think this is going to happen someday. I know it sounds far-fetched.

1275
01:54:40,740 --> 01:54:42,900
I know you think... Looking at me like I'm a little coop.

1276
01:54:42,900 --> 01:54:46,980
No. I mean, look, a lot of things seem far-fetched in the moment. I think it's

1277
01:54:47,540 --> 01:54:53,780
truthfully, I think it's technologically more capable. It's more possible to me technologically

1278
01:54:53,780 --> 01:54:56,980
than it is politically. Okay. Well, that's good to hear. I like that.

1279
01:54:56,980 --> 01:55:01,540
Because I don't... I think the biggest challenge, I'm doing the back of the envelope math.

1280
01:55:02,580 --> 01:55:06,260
I think it's a couple trillion dollars to do this in the United States alone.

1281
01:55:07,220 --> 01:55:11,940
I don't know. Depends on how long... I mean, this is over forever.

1282
01:55:12,260 --> 01:55:19,620
Exactly. Which means it's a moonshot. And I don't feel like our political environment

1283
01:55:19,620 --> 01:55:22,980
is capable of moonshots anymore. We're just trying to live day to day here now.

1284
01:55:22,980 --> 01:55:29,540
Yeah. So long gone are those days when you could make a bold, we're going to spend the equivalent

1285
01:55:29,540 --> 01:55:35,460
of a couple trillion dollars over 20 years. Long after I'm gone, meaning me, meaning the

1286
01:55:35,460 --> 01:55:40,820
politician who's going to be the torchbearer of this to make this a reality. I don't know.

1287
01:55:40,820 --> 01:55:44,340
Maybe I'm just a jaded skeptical guy when it comes to our political system.

1288
01:55:44,340 --> 01:55:49,940
It's actually healthy to be looking at it that way. And also, I want to also frame it, Peter,

1289
01:55:49,940 --> 01:55:55,060
as it is an experiment because you still have to, once you develop it at some scale, you still have

1290
01:55:55,060 --> 01:55:59,940
to prove that it's helping people. Right. So you'll need to use a biased and an unbiased subset of

1291
01:55:59,940 --> 01:56:06,100
these tools. Yeah. So it's theory, it's intriguing, it's doable, and it's going to get progressively

1292
01:56:06,100 --> 01:56:12,020
better, you know, what we can put in as input, but it's still a question mark. Will it improve?

1293
01:56:12,020 --> 01:56:17,460
I just think that our complete reliance on clinical trials is misled.

1294
01:56:17,460 --> 01:56:22,100
I completely agree. I think the heterogeneity problem and the exposure, the time under the

1295
01:56:22,100 --> 01:56:28,020
curve, the exposure problem make clinical trials very difficult to extrapolate from. I mean, here's

1296
01:56:28,020 --> 01:56:35,060
one of my favorite pet peeve examples is people are so quick to dismiss Zetia as a useful drug,

1297
01:56:35,060 --> 01:56:39,380
but in reality, it's never once to my knowledge been targeted towards patients that are

1298
01:56:39,380 --> 01:56:44,820
hyperabsorbers of sterols. Right. And yet, you know, so Zetia gets sort of diluted in clinical

1299
01:56:44,820 --> 01:56:51,380
trials because you're giving it to patients that have normal and abnormal absorption of sterols.

1300
01:56:51,380 --> 01:56:56,420
And so on balance, it doesn't look like a very interesting drug. It seems to work okay with a

1301
01:56:56,420 --> 01:57:01,380
statin, but I'm convinced there are patients out there taking statins who should be taking Zetia

1302
01:57:01,380 --> 01:57:06,100
because if you can phenotype this, you can really see people who don't make that much cholesterol,

1303
01:57:06,100 --> 01:57:09,460
but they absorb it like crazy. It's amazing that never would.

1304
01:57:09,460 --> 01:57:12,900
Who's going to do that clinical trial, right? No one's going to do that clinical trial.

1305
01:57:12,900 --> 01:57:18,740
The lack of interest of the people that manufacture the drug because of dilution of the market. I mean,

1306
01:57:18,740 --> 01:57:21,300
it's really unfortunate, but you're absolutely right about that.

1307
01:57:22,020 --> 01:57:26,740
Well, Eric, this has been a really interesting discussion and I'm glad it's crazy that it took

1308
01:57:26,740 --> 01:57:32,420
a decade for us to, I mean, I'm amazed you knew my name. I'm flattered, but I certainly known

1309
01:57:32,420 --> 01:57:37,940
about you for, from the day I got to San Diego and I'm just glad the podcast and your book really

1310
01:57:37,940 --> 01:57:42,420
became a good excuse to sit down. So thank you. Thank you for your work most importantly, but also

1311
01:57:42,420 --> 01:57:46,420
thank you for making the time today. I know you've talked about this a lot and I'm sure you didn't

1312
01:57:46,420 --> 01:57:49,540
necessarily feel like talking about the book. Oh, and some of these stories over and over again,

1313
01:57:49,540 --> 01:57:52,100
but I know that people listening to this are going to appreciate it.

1314
01:57:52,100 --> 01:57:57,380
Well, thanks Peter. We talked about things I actually haven't really gotten into in the past,

1315
01:57:58,100 --> 01:58:04,660
but I also really enjoyed great intellectual thinking with you. It's fun. I hope we'll have

1316
01:58:04,660 --> 01:58:06,980
a chance to get together a lot more in the years ahead.

1317
01:58:06,980 --> 01:58:10,980
Oh, we certainly will. We're almost neighbors, so it has to happen. Thanks, Eric.

1318
01:58:10,980 --> 01:58:11,300
Thank you.

1319
01:58:12,820 --> 01:58:16,900
Thank you for listening to this week's episode of The Drive. If you're interested in diving deeper

1320
01:58:16,900 --> 01:58:21,060
into any topics we discuss, we've created a membership program that allows us to bring

1321
01:58:21,060 --> 01:58:26,180
you more in-depth, exclusive content without relying on paid ads. It's our goal to ensure

1322
01:58:26,180 --> 01:58:30,980
members get back much more than the price of this subscription. Now to that end, membership

1323
01:58:30,980 --> 01:58:36,340
benefits include a bunch of things. One, totally kick ass comprehensive podcast show notes that

1324
01:58:36,340 --> 01:58:41,780
detail every topic, paper, person, thing we discuss on each episode. The word on the street is,

1325
01:58:41,780 --> 01:58:47,060
nobody's show notes rival these. Monthly AMA episodes or Ask Me Anything episodes,

1326
01:58:47,060 --> 01:58:52,100
hearing these episodes completely. Access to our private podcast feed that allows you to

1327
01:58:52,100 --> 01:58:57,060
hear everything without having to listen to spiel's like this. The Qualies, which are a

1328
01:58:57,060 --> 01:59:01,620
super short podcast, typically less than five minutes that we release every Tuesday through

1329
01:59:01,620 --> 01:59:06,100
Friday, highlighting the best questions, topics, and tactics discussed on previous episodes of

1330
01:59:06,100 --> 01:59:10,980
The Drive. This is a great way to catch up on previous episodes without having to go back and

1331
01:59:10,980 --> 01:59:16,740
necessarily listen to everyone. Steep discounts on products that I believe in, but for which I'm not

1332
01:59:16,740 --> 01:59:21,460
getting paid to endorse and a whole bunch of other benefits that we continue to trickle in

1333
01:59:21,460 --> 01:59:25,620
as time goes on. If you want to learn more and access these member only benefits, you can head

1334
01:59:25,620 --> 01:59:32,980
over to PeterAttiaMD.com forward slash subscribe. You can find me on Twitter, Instagram, and Facebook,

1335
01:59:32,980 --> 01:59:39,540
all with the ID PeterAttiaMD. You can also leave us a review on Apple Podcast or whatever podcast

1336
01:59:39,540 --> 01:59:45,060
player you listen on. This podcast is for general informational purposes only. It does not constitute

1337
01:59:45,060 --> 01:59:49,700
the practice of medicine, nursing, or other professional healthcare services, including

1338
01:59:49,700 --> 01:59:55,860
the giving of medical advice. No doctor patient relationship is formed. The use of this information

1339
01:59:55,860 --> 02:00:01,780
and the materials linked to this podcast is at the user's own risk. The content on this podcast is

1340
02:00:01,780 --> 02:00:07,940
not intended to be a substitute for professional medical advice, diagnosis, or treatment. Users

1341
02:00:07,940 --> 02:00:14,260
should not disregard or delay in obtaining medical advice from any medical condition they have, and

1342
02:00:14,340 --> 02:00:19,860
they should seek the assistance of their healthcare professionals for any such conditions. Finally,

1343
02:00:19,860 --> 02:00:25,380
I take conflicts of interest very seriously. For all of my disclosures and the companies I invest in

1344
02:00:25,380 --> 02:00:32,980
or advise, please visit PeterAttiaMD.com forward slash about where I keep an up-to-date and active

1345
02:00:32,980 --> 02:00:44,020
list of such companies.

