1
00:00:00,000 --> 00:00:15,600
Hey everyone. Welcome to the Drive podcast. I'm your host, Peter Attia. This podcast,

2
00:00:15,600 --> 00:00:19,380
my website, and my weekly newsletter all focus on the goal of translating the science of

3
00:00:19,380 --> 00:00:24,360
longevity into something accessible for everyone. Our goal is to provide the best content in

4
00:00:24,360 --> 00:00:28,440
health and wellness, full stop, and we've assembled a great team of analysts to make

5
00:00:28,440 --> 00:00:32,720
this happen. If you enjoy this podcast, we've created a membership program that brings you

6
00:00:32,720 --> 00:00:36,920
far more in-depth content if you want to take your knowledge of this space to the next level.

7
00:00:36,920 --> 00:00:41,560
At the end of this episode, I'll explain what those benefits are, or if you want to learn more now,

8
00:00:41,560 --> 00:00:48,080
head over to PeterAttiaMD.com forward slash subscribe. Now, without further delay, here's

9
00:00:48,080 --> 00:00:56,080
today's episode. My guest this week is Vinay Prasad. Vinay is a practicing hematologist and

10
00:00:56,760 --> 00:01:02,480
associate professor of medicine at UC San Francisco, where he focuses on not just the

11
00:01:02,480 --> 00:01:08,060
treatment of patients, but also health policy, clinical trials, and decision-making. He's what

12
00:01:08,060 --> 00:01:13,580
some might call a meta researcher. He studies the quality of medical evidence and lately his focus

13
00:01:13,580 --> 00:01:19,640
most of his energy on oncology, of course. He's the author of over 250 academic articles,

14
00:01:19,640 --> 00:01:23,760
along with two books, Ending Medical Reversal, which was published about five years ago and

15
00:01:24,400 --> 00:01:29,800
published this year, the book Malignant, which we spend a lot of time discussing. He also hosts

16
00:01:29,800 --> 00:01:35,280
the oncology podcast, which is called Plenary Session. I recommend you check it up. He runs a

17
00:01:35,280 --> 00:01:42,120
YouTube channel along with his various activities on social media. He's primarily active on Twitter

18
00:01:42,320 --> 00:01:53,680
at VPrasad, that's P-R-A-S-A-D-M-D-M-P-H, where he writes some really great tutorials related to

19
00:01:53,680 --> 00:01:59,400
clinical trials, critical thinking, decision-making, et cetera. In this episode, we talk a little bit

20
00:01:59,400 --> 00:02:04,320
about his beginnings, how he got into medicine, and really how things that he saw during his

21
00:02:04,320 --> 00:02:10,640
medical training kind of woke him up to some of the issues in clinical medicine. Probably the first

22
00:02:10,680 --> 00:02:15,600
thing that he observed with some of the limitations in cardiology and how there was a disconnect

23
00:02:15,600 --> 00:02:21,400
between clinical practice and research, but really that kind of took off once he chose the

24
00:02:21,400 --> 00:02:25,680
profession of oncology, which is a field that is really rife with some of these inconsistencies.

25
00:02:25,680 --> 00:02:29,480
Now, this is a podcast that sort of builds on a lot of the stuff that we discussed in a previous

26
00:02:29,480 --> 00:02:35,120
podcast with Azra Raza, but we go a little bit deeper into some of the structural failures.

27
00:02:35,120 --> 00:02:39,720
This one goes by quick, and yet somehow at the end of it, I looked and realized we'd been talking

28
00:02:39,720 --> 00:02:46,160
for two hours. I could have spoken with Vinay for another two hours. We close this by doing

29
00:02:46,160 --> 00:02:53,080
kind of a deep dive on what he describes as his six hallmarks of cancer policy. And I think this

30
00:02:53,080 --> 00:02:59,600
is a really great discussion. I was just constantly impressed by the way that Vinay was able to kind

31
00:02:59,600 --> 00:03:04,160
of articulate things in ways that I even made the comment at one point, if you gave me two hours to

32
00:03:04,160 --> 00:03:09,080
explain what you just explained in five minutes, I wouldn't have been able to. So I hope you enjoy

33
00:03:09,080 --> 00:03:19,080
this. And without further delay, please enjoy my discussion with Vinay. Hey Vinay, thank you so

34
00:03:19,080 --> 00:03:24,400
much for joining me. Where are you physically today? I'm physically located in the Bay Area,

35
00:03:24,400 --> 00:03:29,560
in one of the distant suburbs, just a temporary place I'm staying here, but hope to be settled

36
00:03:29,560 --> 00:03:35,560
in soon. I started at UC San Francisco pretty recently. Yeah. I noticed that and I was going

37
00:03:35,560 --> 00:03:40,400
to ask you about that. That's a soon to be your new permanent gig, huh? Yeah. I mean, I started

38
00:03:40,400 --> 00:03:44,840
the job, but I do not yet have a permanent place to stay. So I'm just kind of hanging out for now,

39
00:03:44,840 --> 00:03:50,440
but I'm going to work on that. And right now, you know, we're in the midst of forest fires and COVID.

40
00:03:50,440 --> 00:03:55,920
And so it wasn't a terrific time to move in retrospect. UCSF has a dear place in my heart.

41
00:03:55,920 --> 00:04:01,000
When I was in medical school, which was at Stanford, we still spent a lot of time at UCSF.

42
00:04:01,000 --> 00:04:05,480
We had the option to spend time electively and Stanford was not a great place to get a lot of

43
00:04:05,480 --> 00:04:11,440
trauma experience for a budding wannabe surgeon. But of course San Francisco general was. And then

44
00:04:11,440 --> 00:04:18,440
many years later when I returned, my wife was an ICU nurse at UCSF before later moving over to run

45
00:04:18,440 --> 00:04:24,920
the Coumadin clinic, which was run by just a solo NP and one hematologist. So up on Parnassus,

46
00:04:24,920 --> 00:04:29,800
they're at some of the most beautiful views of the entire city. So one thing I remember her saying

47
00:04:29,800 --> 00:04:35,640
was there was this gym up on the Parnassus campus where she would go and work out at like 5 30 before

48
00:04:36,200 --> 00:04:41,640
clinic starts. And you sort of get to kind of just watch the city as the sun was coming up.

49
00:04:42,280 --> 00:04:46,920
Oh, it's gorgeous. That sounds terrific. I'm actually based in San Francisco general hospital

50
00:04:46,920 --> 00:04:52,600
for my clinic. So that's where I do my clinical time. Fantastic. So you and I were scheduled to

51
00:04:52,600 --> 00:04:58,840
speak, I think initially, God probably around the time of the COVID outbreak. And then obviously,

52
00:04:58,920 --> 00:05:04,040
everything kind of got derailed a little bit. So I appreciate your patience. There's a lot to talk

53
00:05:04,040 --> 00:05:08,040
about here. And I almost don't know where to begin. But I do think it probably helps the listener to

54
00:05:08,040 --> 00:05:13,720
understand your background a little bit. Because it's a lot of times people who come to medicine

55
00:05:13,720 --> 00:05:19,240
through the not so obvious routes that maybe bring in a little bit of a different perspective. So

56
00:05:19,800 --> 00:05:24,760
I know you weren't a pre med student. It's not like you grew up thinking I can't wait to be a

57
00:05:24,760 --> 00:05:29,320
doctor. If I recall from reading something, you were actually like a philosophy major in college.

58
00:05:29,320 --> 00:05:33,880
Is that right? Yeah, I guess I kind of might have done a little bit of both. I genuinely felt

59
00:05:33,880 --> 00:05:40,120
undecided at the time. I graduated high school and I thought I'm good in science. I like science,

60
00:05:40,120 --> 00:05:44,280
maybe I'll major in science. And so when I started college at Michigan State University,

61
00:05:44,280 --> 00:05:49,160
I think my original major was in the sciences. Early on in my second year, I took a philosophy

62
00:05:49,160 --> 00:05:54,520
class and it really kind of struck a chord. The professor was very kind and reached out to me.

63
00:05:54,600 --> 00:05:58,680
And very quickly, I added that on as a major. And so I ended up doing a little bit of both.

64
00:05:58,680 --> 00:06:02,360
I can't say when exactly I started thinking about medical school, but I remember feeling

65
00:06:02,360 --> 00:06:06,760
really sort of uncertain if that was the right path for me. I certainly wasn't somebody who

66
00:06:06,760 --> 00:06:10,360
in high school always knew they wanted to be a doctor or something, anything like that.

67
00:06:10,360 --> 00:06:13,080
It came to me sort of on the back end of college, really.

68
00:06:13,080 --> 00:06:14,760
You went to University of Chicago, is that correct?

69
00:06:14,760 --> 00:06:16,360
That's right for medical school. Yeah.

70
00:06:16,360 --> 00:06:19,960
What was that like? I mean, I know that different medical schools had different environments.

71
00:06:19,960 --> 00:06:23,480
Stanford, for what it's worth, was a very relaxed medical school.

72
00:06:23,480 --> 00:06:28,360
My guess is University of Chicago being one of the top 10 schools in the country was not relaxed.

73
00:06:28,360 --> 00:06:29,880
It didn't strike me as relaxed.

74
00:06:30,520 --> 00:06:33,400
It's so funny you say that when I was a medical student at University of Chicago,

75
00:06:33,400 --> 00:06:38,360
I once visited a friend who was doing his doctorate work at Stanford and I toured the hospital and the

76
00:06:38,360 --> 00:06:42,760
windows were open and the smell of Jasmine kind of wafted in. And I was like, wow, this is a place of

77
00:06:42,760 --> 00:06:49,000
healing. And it was really markedly different than University of Chicago, which is a really gritty

78
00:06:49,000 --> 00:06:54,120
city hospital feel. A lot of the faculty had trained on the East Coast and I think it really

79
00:06:54,120 --> 00:06:59,240
had that East Coast mentality. It was an intense place. I remember yelling in the operating room

80
00:06:59,240 --> 00:07:05,000
was common, throwing things was common, people getting chewed out was common. So it had all that

81
00:07:05,000 --> 00:07:10,040
sort of East Coast feel, which these days might be a bygone era, but I kind of caught the tail of it,

82
00:07:10,040 --> 00:07:13,880
at least maybe second generation. But I caught the tail of, I think, that sort of tough East

83
00:07:13,880 --> 00:07:16,760
Coast mentality, which was President Chicago. Yeah.

84
00:07:16,840 --> 00:07:21,080
There's a book that I've spoken about before on the podcast called Forgive and Remember. I don't

85
00:07:21,080 --> 00:07:27,320
know if you ever read it. It was written by a sociologist from Pennsylvania University,

86
00:07:27,320 --> 00:07:34,360
Charles Bosque. And in the book, he spends 18 months with a group of surgical residents to

87
00:07:34,360 --> 00:07:40,920
understand the culture of surgical training. In the book, he never mentions to my knowledge,

88
00:07:40,920 --> 00:07:45,960
I don't think he ever mentions where it was, but for some reason, either I spoke with Bosque and

89
00:07:45,960 --> 00:07:51,160
asked him or somehow inferred, but I believe it actually was the University of Chicago where it

90
00:07:51,160 --> 00:07:55,800
was. And you have to imagine you take the environment you saw and go back a couple of

91
00:07:55,800 --> 00:08:01,080
decades. This was sort of late 70s, early 80s, and you want to talk about toxicity, but you're

92
00:08:01,080 --> 00:08:07,880
absolutely right. There's a real East Coast, West Coast divide in medical education. And I think,

93
00:08:07,880 --> 00:08:14,200
put it this way, when I applied to my residency on the East Coast at Hopkins, there was a real view

94
00:08:14,200 --> 00:08:20,280
that no one from Stanford could go there and do general surgery. Because the last guy, I think,

95
00:08:20,280 --> 00:08:24,440
who had gone and done general surgery, who had come from Stanford had committed suicide.

96
00:08:24,440 --> 00:08:26,040
I see. They thought you were too soft.

97
00:08:26,040 --> 00:08:30,440
Yeah. The view was, it was just a little too soft to come from there. And of course,

98
00:08:30,440 --> 00:08:36,440
that's such a silly thing to say that this person's suicide had anything to do with that. I mean,

99
00:08:36,440 --> 00:08:40,440
the distinction between one versus the other, but it was really viewed as no, no, no, no. Like,

100
00:08:40,440 --> 00:08:45,480
if you went to Chicago, you could go to Hopkins if you went to Penn or Michigan or something you

101
00:08:45,480 --> 00:08:49,480
couldn't. But anyway, that was my view, having friends that went to medical school in Chicago

102
00:08:49,480 --> 00:08:55,240
was I was academically just a tough school in a tough environment. So how did you like medical

103
00:08:55,240 --> 00:09:00,440
school? Well, I guess I probably would be honest with you. I mean, I think I was in the fraction

104
00:09:00,440 --> 00:09:05,640
of people that didn't really care for it a lot of the time. To be more specific, when you start in

105
00:09:05,640 --> 00:09:10,760
medical school, especially in the years in which I trained, you had two just full years of classroom,

106
00:09:10,760 --> 00:09:15,720
almost 40 hours a week of just memorize this kid, memorize this, memorize this, memorize this,

107
00:09:15,720 --> 00:09:19,320
you didn't really get into sort of the decision making of medicine. You didn't get much exposure

108
00:09:19,320 --> 00:09:23,720
to patients. You didn't get that side of medicine. What is actually medicine? Those first two years

109
00:09:23,720 --> 00:09:29,320
are, I found it really demoralizing. I mean, I wasn't somebody who was used to memorizing lots

110
00:09:29,320 --> 00:09:33,800
of things in sort of a disconnected way. I was somebody who liked to think about things and think

111
00:09:33,800 --> 00:09:38,600
about them rigorously. And so I really felt, I would say, very frustrated in the first two years.

112
00:09:38,600 --> 00:09:43,480
Step one, studying was a very anxious time in my life and I didn't quite care for it. And it was

113
00:09:43,480 --> 00:09:48,600
only when third year started and I was there on the wards and I was on internal medicine first as

114
00:09:48,600 --> 00:09:54,440
my clerkship. And I had some really great and influential practitioners of medicine who would

115
00:09:54,440 --> 00:09:59,160
kind of teach me how they think about cases. That was when I started to feel like, okay, that's the

116
00:09:59,160 --> 00:10:03,320
first moment that this felt like the right choice for me. So it wasn't until my third year. So I was

117
00:10:03,320 --> 00:10:07,000
pretty frustrated in the beginning. And then from third year to fourth year, fourth year is really

118
00:10:07,000 --> 00:10:11,240
sort of an expensive vacation, but there were ups and downs in that process too. Remind me where you

119
00:10:11,240 --> 00:10:15,400
did your residency. Northwestern University, I stayed in the city. You stayed in Chicago.

120
00:10:15,400 --> 00:10:20,280
My folks at that time were living in Northwest Indiana, not too far. And so my undergrad,

121
00:10:20,280 --> 00:10:26,440
medical school, residency, it was all pretty close to where my family was. Okay. So when is it like,

122
00:10:26,440 --> 00:10:32,600
if we look at the work that you are now basically defining your career by, right? I mean, and we're

123
00:10:32,680 --> 00:10:36,280
going to get into this in some detail. Where do you think those seeds were sown?

124
00:10:37,000 --> 00:10:41,960
I guess I'd say I was somebody who probably said what I needed to say to get into medical school

125
00:10:41,960 --> 00:10:46,120
and get into residency. But the truth is in my heart, I was probably somebody who was thinking

126
00:10:46,120 --> 00:10:50,920
about medical school as a route to private practice and that I saw myself as a practitioner.

127
00:10:50,920 --> 00:10:55,080
I didn't know exactly what field or what specialty, but I thought I would primarily be taking care of

128
00:10:55,080 --> 00:11:00,440
patients in a private practice setting. I think that was true even when I graduated medical school

129
00:11:00,440 --> 00:11:04,600
and even sort of the beginning of the first year, my internship, but only kind of changed for me in

130
00:11:04,600 --> 00:11:11,080
the middle of residency. And what made a change for me was I had consistently been put in clinical

131
00:11:11,080 --> 00:11:16,680
situations where what I saw we were doing and then what I would read about in the evening,

132
00:11:16,680 --> 00:11:20,680
they was a disconnect. They were things we were doing that were not supported by strong evidence.

133
00:11:20,680 --> 00:11:23,800
There were some things that appeared to run counter to the best evidence, but it didn't make

134
00:11:23,800 --> 00:11:28,840
a lot of sense to me. And so I started just on the margins of that problem, doing a few studies to

135
00:11:28,840 --> 00:11:33,640
kind of make sense of what were these things that had become a part of medical culture that

136
00:11:33,640 --> 00:11:37,800
ultimately proved not to work. We ended up calling that medical reversal. We wrote a few papers about

137
00:11:37,800 --> 00:11:42,120
it, but that was really kind of how I got into it. I got into it from the point of view of a clinician

138
00:11:42,120 --> 00:11:47,640
who was struggling to make sense of what was going on around me. And that was really how I fell into

139
00:11:47,640 --> 00:11:52,600
research. And I didn't know that research would become my career, but the funny thing about life

140
00:11:52,600 --> 00:11:57,480
is you do something long enough and it starts to define you. And so after maybe 15 or 20 papers in

141
00:11:57,560 --> 00:12:02,040
this space, people started to say, this is a guy who's doing health policy research in that space.

142
00:12:02,040 --> 00:12:05,240
And eventually it kind of just keeps yourself so busy. You're just doing the next thing,

143
00:12:05,240 --> 00:12:08,680
doing the next thing. And eventually it kind of starts to turn into a career.

144
00:12:09,320 --> 00:12:16,200
So I think in your book, you write a story about a woman who had a stent placed and had a bad

145
00:12:16,200 --> 00:12:24,440
outcome and maybe tell folks a little bit about a, what a stent is, b, what her situation was

146
00:12:24,440 --> 00:12:31,720
specifically, and perhaps C, and we can probably both elaborate on part C, which is what in the

147
00:12:31,720 --> 00:12:37,240
hell was going on with stents? So I guess I would say, I appreciate you looking through my book and

148
00:12:37,240 --> 00:12:41,400
reading it. I guess I would say that we tried to use composite patience. We try not to base it on

149
00:12:41,400 --> 00:12:46,760
any one particular person, but the situation you're talking about, somebody who gets a stent placed for

150
00:12:46,760 --> 00:12:51,240
an indication that is kind of questionable, who suffers a complication. Oh, there are many people

151
00:12:51,240 --> 00:12:56,840
that come to my mind about that. I have very clear images in my mind of people in that situation.

152
00:12:56,840 --> 00:13:00,200
And I guess I would say, I guess there's two parts to this. So the first part is like,

153
00:13:00,200 --> 00:13:06,440
what is a stent? So a stent is a little flexible metallic tube that expands. And it's often placed

154
00:13:06,440 --> 00:13:11,000
in a coronary artery when there is a blockage. And for people who come in with an heart attack,

155
00:13:11,000 --> 00:13:15,720
an ST elevation, myocardial infarction, a total blockage of the artery, a stent is a

156
00:13:15,720 --> 00:13:20,760
transformative, life-saving intervention. It's one of those amazing medical miracles that we

157
00:13:20,760 --> 00:13:25,000
proudly celebrate in medicine. But it's also something like so many medical technologies that

158
00:13:25,000 --> 00:13:31,000
can be used more broadly. It works really well in a critical situation. Maybe it works well for

159
00:13:31,000 --> 00:13:35,480
somebody who just has a little bit of narrowing of the arteries and maybe just a touch of angina.

160
00:13:35,480 --> 00:13:39,400
That's that kind of reproducible chest pain that comes on when you shovel your driveway in the

161
00:13:39,400 --> 00:13:44,440
winter or go for a long walk. So people would extrapolate from the critical situation to less

162
00:13:44,440 --> 00:13:48,680
severe situations in medicine. And of course, there's a lot more less severe situations than

163
00:13:48,680 --> 00:13:52,920
there are severe situations. So it becomes a big driver of market share. So it turns out stents

164
00:13:52,920 --> 00:13:58,840
became very popular for chronic stable angina. And I knew in the years in which I was a resident

165
00:13:58,840 --> 00:14:04,440
that we had just had a large mega randomized control trial called courage that asked whether

166
00:14:04,440 --> 00:14:09,560
or not stents lower the rate of heart attack or improve longevity. And the answer was for people

167
00:14:09,560 --> 00:14:14,040
with chronic stable angina, not that acute heart attack, but this sort of chest pain that comes on

168
00:14:14,040 --> 00:14:18,600
when you shovel your driveway, there was no improvement in survival and no reduction in

169
00:14:18,600 --> 00:14:23,640
subsequent myocardial infarction or heart attack. Yet survey after survey of patients showed that

170
00:14:23,640 --> 00:14:27,720
when they were consented to the procedure, when they had it done, they believed it was being done

171
00:14:27,720 --> 00:14:32,360
with that purpose in mind. So there was a disconnect between what patients felt it was for and what

172
00:14:32,360 --> 00:14:37,560
doctors knew it could do. This disconnect always played a role in the lives of residents. We're

173
00:14:37,560 --> 00:14:41,720
not the people who place the stent. We're the people who manage the patient in the days afterwards.

174
00:14:41,720 --> 00:14:45,400
And every so often you place a stent, something bad happens. There can be a cardiac arrest on

175
00:14:45,400 --> 00:14:49,800
the table. There can be an occlusion of the stent, a thrombus that forms within this foreign body

176
00:14:49,800 --> 00:14:53,880
that's being placed in the artery. And we witnessed several of these sort of complications. These are

177
00:14:53,880 --> 00:14:58,600
known complications. All things in medicine have some complications. But the question that kind of

178
00:14:58,600 --> 00:15:04,280
plagued me was, it's okay to accept the risk of a complication if the procedure has a net benefit.

179
00:15:04,280 --> 00:15:09,400
But if the procedure is questionable as it's being done and the benefit is questionable,

180
00:15:09,480 --> 00:15:15,240
you're just taking risk without any upside. So cases like that, that we talk about in the book,

181
00:15:15,240 --> 00:15:20,120
were a powerful, I think, motivator for me personally, to kind of look deeper into this

182
00:15:20,120 --> 00:15:25,080
issue, to really understand why we do what we do. And that led to a lot of work in this space.

183
00:15:25,800 --> 00:15:31,160
And I think part of it is, this gets to how we deal with policy, especially on the procedural

184
00:15:31,160 --> 00:15:36,040
side, which I think we'll get to is, because I remember seeing this very tangentially as a

185
00:15:36,040 --> 00:15:42,760
resident as well, you couldn't ignore the conflict of interest that existed there, which was when

186
00:15:43,400 --> 00:15:49,000
interventional cardiologists were being compensated for the number of stents being placed. And I say

187
00:15:49,000 --> 00:15:53,160
that, to be clear, not being critical of them, but acknowledging that if I were in their situation,

188
00:15:53,160 --> 00:15:57,720
I'm not sure how I would self-police. That's the real issue. It's not that they're necessarily

189
00:15:57,720 --> 00:16:01,480
bad people, but they're people. They're just good people. They're just normal people. Yeah,

190
00:16:01,480 --> 00:16:06,840
they're good people. And we all suffer from our own cognitive biases. So what was the next step on

191
00:16:06,840 --> 00:16:11,240
that rabbit hole? I just want to kind of build on what you're saying, which I think is really astute,

192
00:16:11,240 --> 00:16:14,840
which is that these are just people, interventional cardiologists are just people like we're all

193
00:16:14,840 --> 00:16:20,360
people, and they suffer from the same sort of psychological trappings that we all suffer from.

194
00:16:20,360 --> 00:16:25,080
And in this case, I think there's two parts to the equation that make stents so seductive.

195
00:16:25,080 --> 00:16:30,680
So part one is, you place this stent and the patient comes back in your office nine times out

196
00:16:30,680 --> 00:16:34,920
of 10, and they're happy that you did it. They believe you have saved their life or extended

197
00:16:34,920 --> 00:16:39,000
their life. They may even believe they feel better. And in fact, we could talk about orbita

198
00:16:39,000 --> 00:16:42,600
and whether or not that's a real effect or a placebo effect, we can come to that. But they

199
00:16:42,600 --> 00:16:46,360
believe they feel better. They believe you saved their life. So you do this procedure, the patient

200
00:16:46,360 --> 00:16:50,200
comes to your office and say, thank you so much, doctor. You saved my life. And you know what? It

201
00:16:50,200 --> 00:16:54,680
doesn't hurt so much when I shovel the driveway anymore. Thank you. So you pair that incredibly

202
00:16:54,680 --> 00:16:59,320
powerful feeling of gratitude. You pair that with one other thing, which is you get a little bit of

203
00:16:59,320 --> 00:17:04,760
money and you get more money the more you do. So when you combine that powerful psychological

204
00:17:04,760 --> 00:17:09,720
stimulus of gratitude with a little bit of financial remuneration, I think that's the

205
00:17:09,720 --> 00:17:15,000
methamphetamine of being a doctor. That's a highly addictive substance that whatever we do in medicine,

206
00:17:15,000 --> 00:17:20,280
particularly procedures, because that's what pays, we become addicted to that. And then a couple

207
00:17:20,280 --> 00:17:25,000
years later, some investigators say, you know what? The patient didn't feel better actually when we

208
00:17:25,080 --> 00:17:29,160
randomized them to stenting or we made them wear headphones and we told them we put a stent,

209
00:17:29,160 --> 00:17:33,160
but we didn't put the stent. We kind of deceived them into thinking they had a stent. They actually

210
00:17:33,160 --> 00:17:37,960
both had the same exercise tolerance improvement on a treadmill test. So that's the orbit of study.

211
00:17:37,960 --> 00:17:41,400
So when somebody comes at you and they tell you that, you know, actually it's just a placebo effect.

212
00:17:41,400 --> 00:17:47,960
It doesn't actually improve symptoms. It's psychologically unacceptable. How can that be?

213
00:17:47,960 --> 00:17:51,640
It doesn't fit with my experience and it doesn't fit with the way I've been rewarded. And I think

214
00:17:51,640 --> 00:17:56,520
that's in part why many of these medical practices that have evidence, I think that goes

215
00:17:56,520 --> 00:18:00,600
the other way, are very difficult to dispel. We have become addicted to doing them.

216
00:18:01,400 --> 00:18:06,600
So you're chugging along through your residency, which means you're basically getting to rotate

217
00:18:06,600 --> 00:18:12,440
through all the different subspecialties within medicine. Obviously, you're taking care of the

218
00:18:12,440 --> 00:18:17,640
critical cardiac patients. Presumably at some point you're taking care of GI patients,

219
00:18:17,640 --> 00:18:23,960
oncology patients. What is this journey like for you now that you've got bit by this bug of,

220
00:18:24,600 --> 00:18:30,120
hey, wait a minute. If the stent thing is a little bit off the rails, is there anything else in

221
00:18:30,120 --> 00:18:34,920
medicine that's similarly off the rails? No, it's like being a kid in a Kanzi store, Peter.

222
00:18:34,920 --> 00:18:39,240
You're onto something. I mean, it's kind of a privilege really, now that I look back on it,

223
00:18:39,240 --> 00:18:43,400
as a student, I got to spend a month with neurosurgeons watching what they do. Then the next

224
00:18:43,480 --> 00:18:46,920
month I got to spend it with a breast surgeon. The next month I spend it with a radonk

225
00:18:46,920 --> 00:18:52,040
specialist. As a resident, it's a GI doctor for two weeks. Then it's a hematologist,

226
00:18:52,040 --> 00:18:56,600
and then it's an allergist. I mean, it's really a privilege. I get to be a fly on the wall of so

227
00:18:56,600 --> 00:19:01,320
many different situations and see so much of medical practice. And the moment you start to

228
00:19:01,320 --> 00:19:07,240
recognize some of the classic, I think, research pitfalls, the evidence pitfalls, you do as you

229
00:19:07,240 --> 00:19:11,880
exactly say, you start to see it everywhere. You start to see it in how decisions are made in

230
00:19:11,880 --> 00:19:16,840
one clinic, decisions are made in another clinic. There's a theme that emerges. So one theme might

231
00:19:16,840 --> 00:19:24,040
be for mechanical interventions done to alleviate a subjective symptom, whether it's angina or pain

232
00:19:24,040 --> 00:19:29,160
or dyspnea, which is how you catch your breath or back pain or any sort of discomfort. If you do a

233
00:19:29,160 --> 00:19:33,080
mechanical intervention for that, a number of studies show that it is no better than a sham

234
00:19:33,080 --> 00:19:36,680
intervention. And that's different than if you compare it to a medical pill where the person

235
00:19:36,680 --> 00:19:40,680
doesn't get that sort of psychological benefit that you're doing something for them. So you start

236
00:19:40,760 --> 00:19:45,400
to see this theme emerge when you go shadow many places. Explain to folks what a sham intervention

237
00:19:45,400 --> 00:19:51,880
is because I think we don't do them much anymore. But there was a day when actual sham surgical

238
00:19:51,880 --> 00:19:56,040
procedures were done. Yeah, well, you might know a little bit more about that surgical history than

239
00:19:56,040 --> 00:20:00,440
I do. But I guess I would say, when I think about a sham intervention, I say, let's just start with

240
00:20:00,440 --> 00:20:05,400
arthroscopic knee surgery. So you know, a lot of people have pain and discomfort and degenerative

241
00:20:05,400 --> 00:20:10,920
osteoarthritis of the knee, and they get a orthopedist to go in with a scope and actually

242
00:20:10,920 --> 00:20:15,000
debreed some of the cartilage, clean up the joint, make it look a little nicer. So hopefully they

243
00:20:15,000 --> 00:20:19,160
have less pain and discomfort. And lo and behold, if you have that done, people feel better

244
00:20:19,160 --> 00:20:24,120
afterwards than they did before. And if you compare that to physical therapy or maybe taking

245
00:20:24,120 --> 00:20:28,680
ibuprofen, it might even work better. But when you compare it against a sham intervention,

246
00:20:28,680 --> 00:20:33,080
that's where the orthopod goes in with the scope, they fiddle with it a little bit, but they don't

247
00:20:33,080 --> 00:20:37,400
actually do anything inside. And they take out the scope and they tell you they did something,

248
00:20:37,400 --> 00:20:41,880
there is no difference in outcomes, they both feel better. And what that shows you, it's not

249
00:20:41,880 --> 00:20:48,600
the debreedment, per se, it's the psychological stimulus of having that done. And this is true for

250
00:20:48,600 --> 00:20:54,680
injecting polyacrylamide cement into osteoporotic fractures of the vertebra. It's true for a couple

251
00:20:54,680 --> 00:20:59,560
of shoulder procedures. It's true for, I believe, stenting chronic stable angina. That's what the

252
00:20:59,560 --> 00:21:04,200
orbiter trial shows. That's they did it or they made the person believe they did it. And this is

253
00:21:04,200 --> 00:21:08,840
called a sham intervention. And it's a really useful, I think, method to separate what is the

254
00:21:08,840 --> 00:21:14,040
benefit from doing that final step that you think matters versus from all the other things we do in

255
00:21:14,040 --> 00:21:18,760
medicine, which is reassuring the patient, telling them I'm going to fix it, telling them that I fixed

256
00:21:18,760 --> 00:21:23,160
it and telling them that they should feel better. What's the added benefit of actually doing the

257
00:21:23,160 --> 00:21:27,560
thing? In orbita, did they actually cannulate the femoral artery? Yeah, they did. Yeah, they

258
00:21:27,560 --> 00:21:31,800
cannulated the artery. And I believe they performed a diagnostic angiogram. They have all

259
00:21:31,800 --> 00:21:35,720
the pictures of it. And they made the patient wear headphones. And they didn't inform them whether or

260
00:21:35,720 --> 00:21:40,440
not they had the stent placed. And then the primary outcome that they're looking at is the modified

261
00:21:40,440 --> 00:21:44,840
Bruce protocol exercise treadmill. So they put people on treadmills and see how long they can go.

262
00:21:44,840 --> 00:21:49,720
And we knew from prior stenting trials that when you stent someone and you tell them you stent them,

263
00:21:49,720 --> 00:21:55,320
they're going for another minute, two minutes. And in orbita study, they're going for a difference of

264
00:21:55,320 --> 00:22:00,280
16 seconds. And it's not statistically significant. And it's not clinically meaningful. So they really

265
00:22:00,280 --> 00:22:05,400
do call into question that the benefit of that procedure on this sort of standardized endpoint

266
00:22:05,400 --> 00:22:10,520
of subjective symptoms is really sort of what the patient believes it to be. And so that's what a

267
00:22:10,520 --> 00:22:16,280
sham study is. And so to your point, which is you see the theme emerge across many spaces in medicine

268
00:22:16,280 --> 00:22:20,920
when you have the privilege of getting to shadow in many spaces. And now, five years into my faculty

269
00:22:20,920 --> 00:22:25,320
career, I don't have that privilege too often in my own bubble. I'm in my own clinic. I'm in

270
00:22:25,320 --> 00:22:30,840
oncology and hematology. But in the last year, befriended an orthopedic surgeon who I really

271
00:22:30,840 --> 00:22:35,320
like, I have a great deal of respect for her. She's terrific. And she let me shadow on a couple of

272
00:22:35,320 --> 00:22:39,480
surgeries that I hadn't seen. So I get to feel like a medical student again, maybe someday to

273
00:22:39,480 --> 00:22:43,320
lead to a project that we're working on. But to answer your question, I mean, I think you're right,

274
00:22:43,320 --> 00:22:47,800
which is that you do get a sense of sort of the broad lay of the land in medicine when you are a

275
00:22:47,800 --> 00:22:52,840
trainee and you can see things with fresh eyes and in many places. So how did you choose oncology

276
00:22:52,840 --> 00:22:57,560
as your fellowship? I guess the first jump in becoming an oncologist is you decide to go into

277
00:22:57,560 --> 00:23:02,680
internal medicine. I went into internal medicine because I'd had so many positive experiences in

278
00:23:02,680 --> 00:23:06,520
internal medicine. I guess the other options for listeners who may not know are you could do general

279
00:23:06,520 --> 00:23:12,200
surgery residency, a few subspecialties of surgery you can go into right off the bat like urology or

280
00:23:12,200 --> 00:23:15,800
your nose and throat or neurosurgery. You could also be a radiation oncologist. You can be an OB

281
00:23:15,800 --> 00:23:20,120
guy. You can be a pediatrician. For me, general internal medicine was sort of a very broad

282
00:23:20,120 --> 00:23:25,240
category. You have sent the most kids each year into internal medicine. We had really good mentors

283
00:23:25,240 --> 00:23:28,920
in internal medicine. So I knew I wanted to be an internist of some sort. I wanted to kind of

284
00:23:28,920 --> 00:23:33,640
have a broad look at the body and think of things very broadly. I thought for a while I might be a

285
00:23:33,640 --> 00:23:37,960
cardiologist, not just because that's a stereotype, but because I actually was a little bit interested

286
00:23:37,960 --> 00:23:42,760
in cardiology. I thought I might be an intensivist, a critical care doctor along the way. And finally,

287
00:23:42,760 --> 00:23:46,840
I had some really positive experiences with a couple of oncologists at Northwestern,

288
00:23:47,400 --> 00:23:52,040
one of which is Dr. Munshi, who's still there on faculty, but really terrific experiences with

289
00:23:52,040 --> 00:23:56,360
sort of consummate doctors, people who balanced. They knew a little bit about the basic science.

290
00:23:56,360 --> 00:23:59,800
They really knew about clinical trials. They knew about evidence and they were really good

291
00:23:59,800 --> 00:24:04,200
with patients in great bedside manner. And they made decisions I felt were substantive and

292
00:24:04,200 --> 00:24:09,320
important. And so because of that, it's so funny how so much of life is shaped by just the people

293
00:24:09,320 --> 00:24:13,800
you meet. I decided to go into oncology. And so I made that decision early on in my internal

294
00:24:13,800 --> 00:24:21,720
medicine training. Talk to me about your first days as a medical oncology fellow. Now you're

295
00:24:21,720 --> 00:24:26,920
basically also going through comparable stuff. Presumably you're doing rotations on GI oncology,

296
00:24:27,480 --> 00:24:32,200
going through the liquid cancers. I mean, you're running the gamut again, correct?

297
00:24:32,200 --> 00:24:36,520
Yeah, you have a great sense of it. Yeah. The first time I left the Midwest, I went to Washington,

298
00:24:36,520 --> 00:24:42,520
DC and the National Cancer Institute. They have a very unique and fascinating program and you get

299
00:24:42,520 --> 00:24:46,680
to see a lot of stuff. And it's just as you say, one month, you're with a couple of GI oncologists,

300
00:24:46,680 --> 00:24:50,840
you spend a few days at the NCI, a couple days at Georgetown, and maybe you go to Washington

301
00:24:50,840 --> 00:24:55,240
Hospital Center, one of the other flagship hospitals in the city. One month, you may go

302
00:24:55,240 --> 00:25:00,040
to Hopkins and do leukemia. Another month, you may be at the NCI on their clinics, which are really

303
00:25:00,040 --> 00:25:05,160
highly specialized, often rare disease, every patient on a clinical trial or protocol, just a

304
00:25:05,160 --> 00:25:10,440
different experience. And so you get a huge, I think, exposure in oncology. You get exposure

305
00:25:10,440 --> 00:25:15,560
to so many different diseases, many of which before that time, you know very little of. I think the

306
00:25:15,560 --> 00:25:20,120
sad reality of internal medicine training is it trains you a lot in things like cardiology and

307
00:25:20,120 --> 00:25:23,800
pulmonary disease. But oncology is one of the things that you don't get into that much until

308
00:25:23,800 --> 00:25:27,800
you commit to being an oncologist. And so the learning curve is steep. You got to learn a lot

309
00:25:27,800 --> 00:25:31,320
of drugs. There are more new drugs every day, which I think we're going to talk about. And you got to

310
00:25:31,320 --> 00:25:36,520
learn a lot of new diseases and a lot of genetics and a lot of things you didn't know before. And so

311
00:25:36,520 --> 00:25:40,680
the learning curve is steep. But to be honest, I probably think that's one of my favorite years of

312
00:25:40,680 --> 00:25:45,400
training. My first year as an oncology fellow, I had a really great cohort of people training with.

313
00:25:45,400 --> 00:25:50,440
We always went for the drink on Friday evenings to have some camaraderie. And we had a great

314
00:25:50,440 --> 00:25:55,160
exposure to faculty and we were learning 20 new things every day. I mean, I thought it was really

315
00:25:55,160 --> 00:26:00,360
sort of a terrific and transformational year for me. It's also a special place. I was at NCI in

316
00:26:00,360 --> 00:26:05,800
medical school and then for two years after and lived just outside of Bethesda in Silver Spring.

317
00:26:05,800 --> 00:26:10,360
And I still think of it as some of the fondest memories, which is a very special place. I know

318
00:26:10,360 --> 00:26:15,400
we're going to come back and talk about NIH. And I think the NIH in particular, how it funds

319
00:26:15,400 --> 00:26:20,840
research is I certainly have some issues with it and maybe we can get into that. But there's really,

320
00:26:20,840 --> 00:26:26,120
it's not hyperbolic to say there's no place like it on Earth. And I still remember that first day

321
00:26:26,200 --> 00:26:31,720
I stepped foot on that campus as a third year medical student, just thinking, how can this place

322
00:26:31,720 --> 00:26:37,560
exist? It is so marvelous. Yeah, it's a marvelous set of buildings there in Bethesda, this huge

323
00:26:37,560 --> 00:26:41,800
campus, a lot of greenery, so many different buildings. And the building that predominantly

324
00:26:41,800 --> 00:26:45,480
where clinical operations have is Building 10, the sort of centerpiece building, this massive

325
00:26:45,480 --> 00:26:49,640
federal building that's just been constantly expanded over the years. So much of history

326
00:26:49,640 --> 00:26:54,600
occurred in that building. People pioneered the cure for Hodgkin's Lymphoma, at least the chemotherapeutic

327
00:26:54,600 --> 00:26:59,960
cure for Hodgkin's Lymphoma. People did some fundamental work on chemotherapy and breast cancer.

328
00:26:59,960 --> 00:27:04,920
So many great laboratory scientists come from that place. And it's a place that certainly gives you a

329
00:27:04,920 --> 00:27:09,640
feeling of awe and reverence when you're actually physically there. And I think many of us really

330
00:27:09,640 --> 00:27:13,240
are appreciative of the time we spent training there. I think it's a great experience for anyone

331
00:27:13,240 --> 00:27:17,880
who listens who's a trainee, or they're thinking about going into medicine. If you can spend a

332
00:27:17,880 --> 00:27:22,600
year there, spend a summer there or do a fellowship there, you'll be richer for it. Now coming back to

333
00:27:22,600 --> 00:27:29,080
oncology, I mean, I think, again, maybe I'm biased because I know enough about cancer relative to other

334
00:27:29,080 --> 00:27:34,040
disciplines of medicine. But one of the things that strikes me as challenging about doing a fellowship

335
00:27:34,040 --> 00:27:40,840
in medical oncology as you did is that there's not a lot that's consistent or similar between

336
00:27:41,480 --> 00:27:46,520
acute lymphocytic leukemia or lymphoblastic leukemia and breast cancer. And even though

337
00:27:46,520 --> 00:27:51,240
they're both quote unquote cancer, for all intents and purposes, they're totally different diseases.

338
00:27:51,800 --> 00:27:57,320
And now multiply that by all the cancers you just rattled off, right? You've got these patients with

339
00:27:57,320 --> 00:28:01,240
lymphomas, and some of them are Hodgkins, and some of them are not Hodgkins. And then you've got the

340
00:28:01,240 --> 00:28:05,720
leukemias, and then you've got the pancreatic cancer, and you've got the colon cancer, and the

341
00:28:05,720 --> 00:28:12,760
breast cancer, and the head and neck cancers. That is a lot of different diseases. How did you

342
00:28:12,760 --> 00:28:19,000
sort of navigate your way through that as a trainee? And then how do you decide as you're going

343
00:28:19,000 --> 00:28:24,840
through that, what does this mean for me in my career? I mean, I agree with your observation

344
00:28:24,840 --> 00:28:31,640
that cancer is a category term. It's not a single monolith. It's so many things. And even within

345
00:28:31,640 --> 00:28:36,360
cancers, I mean, even within something as small as non small cell lung cancer, which itself is

346
00:28:36,360 --> 00:28:41,720
a category of lung cancer. Now we have EGFR, mutation driven non small cell lung cancer,

347
00:28:41,720 --> 00:28:45,080
we have Alkary range, non small cell lung, RET, and then ROS1. We have all these molecular

348
00:28:45,080 --> 00:28:50,280
categories, we have RAS, this undrugable target, we have superimposed with that sort of the role

349
00:28:50,280 --> 00:28:55,960
of immunotherapy. I mean, it's a lot. And I guess the only way to kind of do it is just to do it

350
00:28:55,960 --> 00:29:01,160
piecemeal a little bit at a time, just reading as you go. Every night you read an article or two,

351
00:29:01,160 --> 00:29:05,960
you read up to date, you start there, eventually start to dig into the references. I don't pretend

352
00:29:05,960 --> 00:29:10,200
to know everything about every cancer even to this date. I don't think, and I once heard somebody say,

353
00:29:10,760 --> 00:29:15,880
1963 was the last time a scientist died who knew everything. I mean, it's just impossible to know

354
00:29:15,880 --> 00:29:20,600
it all. You can't know all the basic science, you can't know all the clinical medicine. I pick a

355
00:29:20,600 --> 00:29:25,240
certain spot. And I think that spot is the spot of a clinician. I mean, my primary sort of interest

356
00:29:25,240 --> 00:29:31,080
and still to this day, what I do with about half my time, hour wise is see patients and think about

357
00:29:31,080 --> 00:29:35,000
patient care. So I start with that vantage. And then I go outward from there. And so what trials

358
00:29:35,000 --> 00:29:39,000
do I need to know? What clinical evidence do I need to know? What heuristics do I need to know

359
00:29:39,000 --> 00:29:43,800
to guide patient care? How can I work on my bedside manner? And then beyond that, what policy

360
00:29:44,440 --> 00:29:48,680
determines these things and what basic science is relevant to that. But yeah, I don't know everything

361
00:29:48,680 --> 00:29:53,320
about cancer basic science. I don't pretend to. That's beyond what I can know. That's what most

362
00:29:53,320 --> 00:29:57,720
people do in oncology. They start with the patient, they work their way outward and they try to learn

363
00:29:57,720 --> 00:30:02,440
as much as possible. And you're always going to be learning new things even four or five, six years

364
00:30:02,440 --> 00:30:07,640
into practice as I am now. You've brought up bedside manner indirectly twice. So I want to touch on

365
00:30:07,640 --> 00:30:11,800
that. It obviously means a lot to you. You mentioned it in the first setting with respect to

366
00:30:11,800 --> 00:30:16,440
a mentor you had. And again, you referenced it now in a way that I think is quite interesting,

367
00:30:16,440 --> 00:30:21,080
which is it's an area of something you would think about improving upon being better. And

368
00:30:21,080 --> 00:30:25,720
do you get the sense a lot of doctors feel that way? And how do you specifically think about

369
00:30:25,720 --> 00:30:32,520
improving that? Because anybody who's done what you do understands two things that are simultaneously

370
00:30:32,520 --> 00:30:40,440
true, yet cut at odds. The first is the practice of medicine can be exhausting. And at times it can

371
00:30:40,440 --> 00:30:45,720
sort of suck the life out of the practitioner, be it the doctor, the nurse, the therapist, etc.

372
00:30:46,280 --> 00:30:52,600
But the flip side of that is, again, you referenced this earlier, it's an unmistakable privilege.

373
00:30:52,600 --> 00:30:59,560
And that anybody gets to be that intimate with another human being at their most vulnerable time.

374
00:30:59,560 --> 00:31:04,360
That's something that can't really be forgotten. And therein lies this tension, at least for me,

375
00:31:04,360 --> 00:31:09,880
around what bedside manner means. Yeah, I mean, I guess I can't profess to be the expert in bedside

376
00:31:09,880 --> 00:31:14,760
manner, but it is something that I take seriously. And I'm constantly trying to do better. I'll start

377
00:31:14,760 --> 00:31:18,280
by saying one thing, I think there is a misconception, I think, among many people outside of

378
00:31:18,280 --> 00:31:23,640
oncology that oncology is often doom and gloom and tough situations. And I guess I want to say

379
00:31:23,640 --> 00:31:29,080
that that is true. There is a fair bit of end of life in oncology. We do have to have those hard

380
00:31:29,160 --> 00:31:33,640
conversations. But it's not exclusively true. That's not all we do. We also have a lot of people

381
00:31:33,640 --> 00:31:37,640
who are concerned about things that frankly, are not going to be the thing that shortens their life,

382
00:31:37,640 --> 00:31:41,720
it's not going to be the end of the world. We also have many patients in whom we cure and we have to

383
00:31:41,720 --> 00:31:45,800
follow for long term side effects. So it's really a range of medical experiences, some of which I

384
00:31:45,800 --> 00:31:50,840
think is that stuff stuff that people focus on. I'll just tell you one anecdote, sort of what put

385
00:31:50,840 --> 00:31:54,760
it all in perspective for me. When I was at the NCI, I worked with a really senior oncologist who

386
00:31:54,840 --> 00:31:59,480
had been practicing for 30 years. And we had seen a patient on several visits, and I followed this

387
00:31:59,480 --> 00:32:03,160
patient for many months with this senior oncologist. And the senior oncologist had followed this patient

388
00:32:03,160 --> 00:32:08,920
for I think over a decade. We finally reached an impasse, we reached a situation where there were

389
00:32:08,920 --> 00:32:13,640
really no further therapeutic options, the tumors were growing uncontrolled, it was clearly going

390
00:32:13,640 --> 00:32:17,800
to take this man's life. And we had sort of radiographic and laboratory evidence that that was

391
00:32:17,800 --> 00:32:22,120
the case. We were going to go into that room, and this senior oncologist was going to have to tell

392
00:32:22,120 --> 00:32:26,840
this gentleman, somebody he had known for over maybe nearly a decade at that point, that there

393
00:32:26,840 --> 00:32:31,480
was nothing more he could do for him and that he was going to pass away. To me as a trainee, I

394
00:32:31,480 --> 00:32:35,960
thought that I have no idea how this guy's going to do this because this is a heartbreaking

395
00:32:35,960 --> 00:32:40,200
conversation. My heart is broken. And I've only known this guy for a few weeks. And I'm not in

396
00:32:40,200 --> 00:32:47,160
his shoes. And so I go into the room and he does what would be the stuff of legend. I mean, he's

397
00:32:47,160 --> 00:32:51,960
compassionate, he's caring, he's hearing the patient, he's seeing the patient, he's keeping

398
00:32:51,960 --> 00:32:55,560
a little bit of distance, he's giving the information he needs to give, but he's also giving

399
00:32:55,560 --> 00:32:59,480
sort of the emotional support he needs to give as well. It's very hard for me to even describe how

400
00:32:59,480 --> 00:33:04,920
he did it. It felt like magic to me as a trainee. And afterwards, this patient thanks him for the

401
00:33:04,920 --> 00:33:09,880
news, and is clearly upset about it, but then hugs him and says, I just want to thank you for taking

402
00:33:09,880 --> 00:33:14,200
care of me for the last 10 years. I couldn't have done this without you. We come out of that room,

403
00:33:14,200 --> 00:33:19,000
and we're just all sort of in that shocked feeling of what we had born witness to, which is

404
00:33:19,000 --> 00:33:23,640
really sort of that rare privilege moment of being a doctor. And I remember telling this attending

405
00:33:23,640 --> 00:33:27,480
physician, I've seen a bunch of people do that. I don't think I've ever seen anyone do it as

406
00:33:27,480 --> 00:33:32,440
gracefully as you have done. That was really well done. How do you think about that? What's going

407
00:33:32,440 --> 00:33:36,680
through your mind? How do you approach these situations? And he looked at me and he said,

408
00:33:36,680 --> 00:33:41,160
I can't say that I'm good at it after doing it for 30 years, but I can say I try to be better

409
00:33:41,160 --> 00:33:46,520
at it each year. And I realized that that was his secret, of course, is that he had never let

410
00:33:46,520 --> 00:33:51,720
himself become complacent. He never let himself feel like he did a good job. He always aspired

411
00:33:51,720 --> 00:33:56,920
to be a little bit better at delivering that news a little bit more in the moment than the year

412
00:33:56,920 --> 00:34:00,840
before. And that was why he was so good at it is that he didn't take it for granted. He knew how

413
00:34:00,840 --> 00:34:05,960
important it was, and he worked on it. And I guess that's how the moment he told me that, obviously,

414
00:34:05,960 --> 00:34:10,120
it was sort of the moment that that was how I was going to think about it forever, because he was

415
00:34:10,120 --> 00:34:14,440
right. And it's so easy to think as an oncologist that your decision is just prescribing the right

416
00:34:14,440 --> 00:34:20,520
chemotherapy drug. But so much of oncology is being the person you need to be for this person

417
00:34:20,520 --> 00:34:24,680
who needs you to be there for them in that moment. And I think that's in part what makes the feel so

418
00:34:24,680 --> 00:34:30,440
rich and so interesting. First of all, that's a absolutely beautiful story. Secondly, I find it

419
00:34:30,440 --> 00:34:37,160
interesting that it's hard to actually articulate the nuances of what he said while you're still

420
00:34:37,160 --> 00:34:42,680
able to capture the gestalt of it. And that actually echoes an experience I had also at the

421
00:34:42,680 --> 00:34:52,600
NCI with my mentor, who I remember a very similar situation. This was a very young patient, about

422
00:34:52,600 --> 00:34:58,920
27 years old, metastatic melanoma. He had progressed through at the time the best available

423
00:34:58,920 --> 00:35:03,880
immunotherapy. So a couple of things that stood out to me. One is when we were rounding on patients,

424
00:35:03,880 --> 00:35:09,480
we were never permitted to say this patient failed such and such. As you know, in oncology,

425
00:35:09,480 --> 00:35:15,400
that's a very common parlance. Mr. So-and-so is a 27 year old male. He has failed interleukin-2.

426
00:35:15,400 --> 00:35:22,840
He failed GP100 vaccine. Now, it was never that. It was the therapy failed the patient. The patient

427
00:35:22,840 --> 00:35:28,760
didn't fail the therapy. So we were very clear in our words. There was no ambiguity about how we

428
00:35:28,840 --> 00:35:36,040
spoke about it. But when it became clear, just as you describe radiographically, that there was now

429
00:35:36,040 --> 00:35:41,800
no option remaining for this patient and the tumors in his lungs and liver were exploding.

430
00:35:42,440 --> 00:35:47,720
Steve Rosenberg was my mentor. He said something to me that was, it's really easy at this point

431
00:35:47,720 --> 00:35:52,840
to think that because we have failed, we should be ashamed and we should run away from this patient.

432
00:35:52,840 --> 00:35:58,360
But he said patients who are dying need us more than patients who are living. Which again,

433
00:35:58,360 --> 00:36:05,320
sounds kind of vague, but it's that ethos that gets carried into the discussion. And if you think

434
00:36:05,320 --> 00:36:10,920
of it that way, I think that's what actually contributed to the interaction. So your story

435
00:36:10,920 --> 00:36:15,720
is really a beautiful example of that. We're going to end up talking quite a bit about cancer here,

436
00:36:15,720 --> 00:36:21,240
not only because it's your career, but it's also potentially maybe outside of cardiology,

437
00:36:21,880 --> 00:36:28,600
one of the places where we see the best of intentions gone awry with respect to how to

438
00:36:28,600 --> 00:36:32,600
help people. God, there's a part of me that almost wants to jump right into your hallmarks,

439
00:36:32,600 --> 00:36:38,600
because you close your book with, and by the way, I'm in the process of sort of trying to write a

440
00:36:38,600 --> 00:36:45,400
book so I can appreciate the comedic relief in how the appendix or slash end of your book is,

441
00:36:45,400 --> 00:36:49,720
oh, by the way, as I finish this book, I think I finally figured out a succinct way to explain this

442
00:36:49,800 --> 00:36:56,600
now, which I loved. Yeah, yeah, so true. But maybe we'll save that for a moment. And because I think

443
00:36:56,600 --> 00:37:00,680
that's a nice way to synthesize it, unless you just want to start with that. But do you want to kind

444
00:37:00,680 --> 00:37:06,760
of get into a little bit of the how you came to appreciate what was not working and how that sort

445
00:37:06,760 --> 00:37:13,160
of led you to formulate your journey through this? Yeah, I guess I'd say I'll talk a little bit about

446
00:37:13,160 --> 00:37:17,000
what are the sort of structural problems in oncology, but I guess I'd say one of the sort of

447
00:37:17,000 --> 00:37:22,840
moments in my life that it all came into crystal clear focus was there was a year at the National

448
00:37:22,840 --> 00:37:29,160
Oncology Conference that there was one super transformational drug. And it was the drug that

449
00:37:29,160 --> 00:37:33,800
made the big stage and it was the drug everyone was celebrating. And it was a drug that prior to

450
00:37:33,800 --> 00:37:38,120
having this drug, the median survival with this condition was something on the order of six months.

451
00:37:38,120 --> 00:37:42,040
And now that we have this drug, unfortunately, nobody was cured with this drug, but the median

452
00:37:42,040 --> 00:37:46,840
survival was extended to 11 months. And so it's something like five month improvement in average

453
00:37:46,840 --> 00:37:51,160
median survival that a patient would experience. And it was getting standing ovations and people

454
00:37:51,160 --> 00:37:55,400
were incredibly enthusiastic about it. And I just remember I think at the time the conference was

455
00:37:55,400 --> 00:37:58,920
going on in Chicago and one of my high school friends was visiting, and he asked me he's like,

456
00:37:58,920 --> 00:38:03,000
you know, what's the big talk at that conference? And I told him about this drug, survival six

457
00:38:03,000 --> 00:38:07,000
months, and now it's gone to 11 months. And he said six months, 11 months. And he said, Oh,

458
00:38:07,000 --> 00:38:11,320
he said, what are you talking about? He said, do better than that. That's not good. I mean, 11

459
00:38:11,320 --> 00:38:15,880
months, not good enough, man. He just said it. And it was just his gut reaction. He didn't go to

460
00:38:15,880 --> 00:38:19,480
medical school. And he didn't even complete college. But he's a terrific person. And but

461
00:38:19,480 --> 00:38:24,120
he was just giving his sort of just raw feeling about that that wasn't good enough. You all need

462
00:38:24,120 --> 00:38:29,400
to aspire for more. And led to a lot of investigations where we and others have looked at what's the

463
00:38:29,400 --> 00:38:34,760
average benefit of a new cancer drug come into market. If you look at 71 consecutively approved

464
00:38:34,760 --> 00:38:39,640
drugs for the solid cancers, as Foho and colleagues did in a famous paper a few years ago,

465
00:38:39,640 --> 00:38:46,200
the average improvement was 2.1 months. So that's the average of 71 drugs. Two months can mean a

466
00:38:46,200 --> 00:38:50,360
lot to somebody. I mean, those are those could be two months where you do a lot of important things.

467
00:38:50,360 --> 00:38:56,440
But two months also feels like, boy, can't we do better than that. And two months should also come

468
00:38:56,440 --> 00:39:02,280
with another asterisk, which is what is the cost of these medications, they now routinely run 100,000

469
00:39:02,280 --> 00:39:07,000
to $200,000 per year of treatment. And you got to take it for maybe eight, nine, 10 months to get

470
00:39:07,000 --> 00:39:12,120
the two month benefits. So you're spending nearly $100,000 of money, just on the drug itself,

471
00:39:12,120 --> 00:39:17,480
potentially all these other costs that come. And so many of us, myself included, started to feel

472
00:39:17,480 --> 00:39:22,360
like, why are we spending so much money on these drugs that appear to offer less than what we would

473
00:39:22,360 --> 00:39:27,320
want and hope for, for our patients? Why are there so many of them coming? Why aren't there fewer,

474
00:39:27,320 --> 00:39:31,960
but better drugs coming? What are the sort of structural problems in this space that create a

475
00:39:32,040 --> 00:39:37,480
glut of often me too drugs? And by that, I mean, there's a Coca-Cola, and there's a Pepsi-Cola,

476
00:39:37,480 --> 00:39:41,080
and we're getting lots of Pepsi-colas. We're getting a lot of me too drugs. We're not getting

477
00:39:41,080 --> 00:39:46,680
as many drugs that are as novel, that really are transformative. And so that was kind of one of the

478
00:39:46,680 --> 00:39:52,680
core questions that started me on this path. And Malignant is sort of a book that summarizes

479
00:39:52,680 --> 00:39:58,440
sort of all the work we've done in cancer and cancer drug space and drug policy space. But it

480
00:39:58,520 --> 00:40:01,960
wasn't, of course, the goal when we started doing this work, we just kind of wanted to understand a

481
00:40:01,960 --> 00:40:07,160
bunch of things. And after a while, we understood a few things in a way. And we realized that I

482
00:40:07,160 --> 00:40:11,080
realized that there's a story that could be told across all these different domains, and that it

483
00:40:11,080 --> 00:40:14,360
makes more sense when you tell the whole story. And so that's why I decided to write that book.

484
00:40:15,240 --> 00:40:20,040
So let's start with the idea of reversal. You alluded to it earlier, let's go back and revisit

485
00:40:20,040 --> 00:40:25,880
that. So that was sort of inspired by work I did as a resident, I had seen those situations that we

486
00:40:25,880 --> 00:40:30,120
talked about situations where people were getting things done that appear to run counter to the best

487
00:40:30,120 --> 00:40:34,280
available evidence. And with a colleague of mine, Adam Sifu, who's a professor at the University of

488
00:40:34,280 --> 00:40:39,000
Chicago, he was a former teacher of mine, later became a mentor of mine and later became a friend,

489
00:40:39,000 --> 00:40:43,960
which is sort of how the progression of those events often is in life. We started to ask a bunch

490
00:40:43,960 --> 00:40:49,240
of questions about how many medical practices are adopted based on low levels of evidence,

491
00:40:49,240 --> 00:40:54,520
what drive their adoption, and what happens when years later, people come along and do really

492
00:40:54,520 --> 00:40:59,000
carefully done rigorous studies and find that some of them do not work as intended. And we call

493
00:40:59,000 --> 00:41:03,320
those practices, practices that weren't just replaced by something better, but practices that

494
00:41:03,320 --> 00:41:08,040
truly were reversed. We found that not doing it was better or whatever you did before was better.

495
00:41:08,040 --> 00:41:12,200
We called those medical reversals and we started to kind of make lists of them, keep track of them,

496
00:41:12,200 --> 00:41:16,600
and try to understand how often they occur, why they occur, what the downsides of having so many

497
00:41:16,600 --> 00:41:21,160
reversals are. That led to the sort of the first book that I wrote with Adam Sifu, which is called

498
00:41:21,160 --> 00:41:25,400
Ending Medical Reversal, which is really about all the flip-flops that happen in the doctor's office.

499
00:41:26,040 --> 00:41:29,720
What are some of the examples that people might bring to mind when you think about that?

500
00:41:29,720 --> 00:41:33,960
I mean, I guess a lot of them are actually from our talk and shop or sort of things that doctors

501
00:41:33,960 --> 00:41:39,240
do, but I'll give you a few examples. So one is hormone therapy. So there was this very provocative

502
00:41:39,240 --> 00:41:43,960
idea that was put out in the 1990s and supported by a couple of observational studies from the

503
00:41:43,960 --> 00:41:48,840
Harvard investigators. And that was the idea that women who typically have low rates of

504
00:41:48,840 --> 00:41:52,760
cardiovascular events, when they go through menopause, they have a higher rate of cardiovascular

505
00:41:52,760 --> 00:41:57,560
events. Maybe estrogen was protecting them, lowering the rate of cardiovascular events,

506
00:41:57,560 --> 00:42:01,560
and maybe if we supplement them with estrogen after menopause, they'll have lower rates of

507
00:42:01,560 --> 00:42:06,520
cardiovascular events. A retrospective observational study from Harvard found that nurses who happened

508
00:42:06,520 --> 00:42:11,640
to take hormone therapy, replacement therapy, did in fact have lower rates of cardiovascular disease.

509
00:42:11,640 --> 00:42:16,280
And this led to sort of widespread promotion. There's a company called Wyeth that was really,

510
00:42:16,280 --> 00:42:21,080
I think, instrumental in driving prescriptions of hormone replacement therapy. There was a lot of

511
00:42:21,080 --> 00:42:25,560
basic science evidence that corroborated that estrogen has sort of favorable effects on vascular

512
00:42:25,560 --> 00:42:31,960
endothelium, etc., etc. It quickly became sort of a widely used common medication accruing dollar

513
00:42:31,960 --> 00:42:37,880
amounts in the billions of dollars. And then lo and behold, in 2001, 2002, a randomized controlled

514
00:42:37,880 --> 00:42:42,680
trial called Women's Health Initiative came out, but randomly assigned postmenopausal women to

515
00:42:42,680 --> 00:42:47,720
estrogen supplementation or not. And it found, in fact, it was halted for an increase in

516
00:42:47,720 --> 00:42:51,480
thromboembolic and cardiovascular events. It actually did the opposite of what investigators

517
00:42:51,480 --> 00:42:56,840
had thought. That was sort of a seminal moment, I think, for many people, that maybe things that

518
00:42:56,840 --> 00:43:02,040
are widely done don't work as intended. I'll give you just another example. After somebody has a

519
00:43:02,040 --> 00:43:06,600
heart attack, if you put them on an EKG machine and watch it, you'll see there are a bunch of

520
00:43:06,600 --> 00:43:13,160
aberrant beats, premature ventricular contractions, PVCs. A number of really well done studies have

521
00:43:13,160 --> 00:43:18,600
shown that the more PVCs a patient had, the more likely they are to have sudden cardiac death.

522
00:43:18,600 --> 00:43:23,240
And there was a reason why that might happen, that these aberrant electrical activity of the heart

523
00:43:23,240 --> 00:43:27,160
could actually precipitate a reentrant circuit and precipitate actually cardiac arrest.

524
00:43:27,720 --> 00:43:32,680
A number of drugs were made that could suppress PVCs. They suppressed PVCs rather potently,

525
00:43:32,680 --> 00:43:36,920
so you could have somebody take the drug and you can watch those PVCs just drop out of the EKG

526
00:43:36,920 --> 00:43:43,000
tracing. And finally, somebody came along and said, look, we know PVCs are bad. We know the

527
00:43:43,000 --> 00:43:48,280
drug suppresses PVCs, but we don't know for sure the drug lowers the risk of dying. Let's test that.

528
00:43:48,280 --> 00:43:52,200
Let's do a randomized trial. And they did that randomized trial. It was called CAST. And

529
00:43:52,200 --> 00:43:56,280
cardiologists were really reluctant to randomize patients because they thought it was unethical

530
00:43:56,280 --> 00:44:01,160
not to give the drug. And finally, through persistence, they did do the randomized study

531
00:44:01,160 --> 00:44:04,600
and actually found that it increased the risk of dying. And so those studies have led to the

532
00:44:04,600 --> 00:44:09,640
abandonment, primarily the class 1C agents, flec and the like. And I think what the takeaway

533
00:44:09,640 --> 00:44:15,320
message there is that, wow, a lot of smart, well-intentioned people who really have plausible

534
00:44:15,320 --> 00:44:19,960
pathophysiology, who have a compelling retrospective observational story, they can be wrong

535
00:44:20,600 --> 00:44:25,640
and has this happened elsewhere. And so we started investigating it. Now we have lists of

536
00:44:25,640 --> 00:44:30,760
hundreds of items. They span everything from ways in which we screen patients, ways in which we test

537
00:44:30,760 --> 00:44:35,000
patients, drugs we give patients, procedures we do on patients, surgeries we do on patients.

538
00:44:35,000 --> 00:44:39,640
It really spans the gamut. There have been these medical reversals across broad domains of medicine.

539
00:44:39,640 --> 00:44:44,040
They are quite common. I'd like to come back to one of those because I do think the

540
00:44:44,040 --> 00:44:50,520
WHI is arguably the worst study that's ever been done, which then brings up a broader question,

541
00:44:50,520 --> 00:44:57,000
which is how do we know if the patient in front of us is represented by the patient in this study?

542
00:44:57,000 --> 00:45:01,720
But I don't want to take us off the oncology path, although the WHI's biggest headline,

543
00:45:01,720 --> 00:45:06,280
of course, was the increase in the risk of breast cancer, which of course has since been

544
00:45:06,280 --> 00:45:09,640
abandoned, which actually brings up another point, which is the difference between relative

545
00:45:09,640 --> 00:45:12,840
risk and absolute risk. So I think there's a lot of interesting stuff to talk about.

546
00:45:12,840 --> 00:45:19,080
So let's get back to oncology and go back to what you were just outlining as I think the broad

547
00:45:19,080 --> 00:45:26,920
problem statement here, which is we've got a disease that I think it's safe to say we haven't

548
00:45:26,920 --> 00:45:32,120
really had much success against, despite a lot of propaganda. I sort of explain it to my patients

549
00:45:32,120 --> 00:45:37,080
this way. I don't know if you'd agree, but I say, look, there are three broad pillars of disease,

550
00:45:37,080 --> 00:45:41,080
chronic disease that are going to kill us. So you've got sort of this foundation of metabolic

551
00:45:41,080 --> 00:45:46,360
disease. So that's everything from hyperinsulinemia to insulin resistance to fatty liver disease to

552
00:45:46,360 --> 00:45:53,800
type 2 diabetes. That creates the foundation upon which three other disease processes get a lot

553
00:45:53,800 --> 00:45:59,400
worse, cardiovascular and cerebrovascular disease, cancer and neurodegenerative disease.

554
00:45:59,400 --> 00:46:03,640
There's really only one of those three pillars we've had some success against, and that's

555
00:46:03,640 --> 00:46:10,120
cardiovascular disease. Your probability of surviving a heart attack in 2020 is infinitely

556
00:46:10,120 --> 00:46:17,320
better than 1960. I mean, between advanced cardiac life support, better drugs to lower

557
00:46:17,320 --> 00:46:23,160
cholesterol, lower blood pressure, as you pointed out earlier, stents that actually do their job

558
00:46:23,160 --> 00:46:28,200
in patients who are having an MI or immediately post. And also, I think we have a greater path

559
00:46:28,200 --> 00:46:37,320
ophysiologic understanding of it. But if you have metastatic breast cancer in 2020 versus 1970 or

560
00:46:37,320 --> 00:46:45,720
1960, if you have Alzheimer's disease in 2020 versus 1970 or 1960, you're not a hell of a lot better

561
00:46:45,720 --> 00:46:51,240
off. And I think that's a hard thing for people to understand, especially when you consider

562
00:46:51,240 --> 00:46:57,080
the resources that have been put at it. Have you got a ballpark of how much has been invested in

563
00:46:57,080 --> 00:47:01,800
cancer research in the last 50 years, directionally since the war on cancer was declared?

564
00:47:02,360 --> 00:47:06,360
The total will easily be in the hundreds of billions of dollars. So I guess I would say,

565
00:47:06,360 --> 00:47:12,840
I mean, I think your assessment, although people may not like it, I think it is not inaccurate.

566
00:47:12,840 --> 00:47:18,200
It is accurate assessment is that we've poured in hundreds of billions of dollars. That's probably

567
00:47:18,200 --> 00:47:24,680
combined with public purses and private purses. And the returns on that, it's easy to sort of fixate

568
00:47:24,680 --> 00:47:30,280
on the few sort of examples where we've made massive progress. But we can't forget the denominator,

569
00:47:30,280 --> 00:47:34,680
which is the average person walking in clinic who might not have chronic myeloid leukemia or one of

570
00:47:34,680 --> 00:47:38,840
the rare conditions that we've made sort of transformative leaps in the average person,

571
00:47:38,840 --> 00:47:44,040
I think is still facing a very grave prognosis and that the progress is, as my friend said,

572
00:47:44,040 --> 00:47:47,720
that's not good enough, you got to do better. It's a great story about your friend,

573
00:47:47,720 --> 00:47:52,920
because sometimes you just need an outsider to sort of call your BS, which is we celebrate this

574
00:47:52,920 --> 00:47:56,760
drug. I remember when I was in residence, I don't remember what drug it was, but I literally remember

575
00:47:56,760 --> 00:48:04,120
going to ASCO or something and someone presented a drug that increased median survival by 0.7

576
00:48:04,120 --> 00:48:12,840
months, 0.7 months, right? 20 days. And I remember doing the math. It was at a cost of $38,000 for

577
00:48:12,840 --> 00:48:19,400
the extra 20 days. And I mean, I think people listening to this might be understandable to

578
00:48:19,400 --> 00:48:25,240
question, well, who are we to say what $38,000 is worth? How do you think about that question,

579
00:48:25,240 --> 00:48:31,960
which is the societal cost versus the individual cost? So all things equal, let's assume there was

580
00:48:31,960 --> 00:48:38,520
no toxicity of the drug or let's assume that the pain and discomfort of the drug wasn't a deciding

581
00:48:38,520 --> 00:48:45,400
factor. Who is to decide the cost of a life? I guess before I dive into that answer, let me

582
00:48:45,400 --> 00:48:49,960
give a little bit more background that I think will even make it more sort of relevant for the

583
00:48:49,960 --> 00:48:54,680
listener to really get a sense of why I'm going to answer the way I answer, which is that 0.7

584
00:48:54,680 --> 00:49:00,600
months, that's not in everybody. And what do I mean by that? So the trials that we use to identify

585
00:49:00,600 --> 00:49:06,360
these numerical amounts that the drugs improve survival are not average Americans off the street

586
00:49:06,360 --> 00:49:11,640
with the cancer. They're really carefully curated populations. They're often 10 years younger than

587
00:49:11,640 --> 00:49:16,120
average cancer patients. They don't have the same range of comorbidities. They don't have as much

588
00:49:16,120 --> 00:49:19,960
diabetes. They're not as overweight. They don't have cardiovascular disease. They don't have renal

589
00:49:19,960 --> 00:49:25,240
dysfunction. They're younger, healthier. One of my colleagues describes clinical trial patients as

590
00:49:25,240 --> 00:49:29,880
somebody who could run a marathon who also happens to have cancer. They're really fit individuals.

591
00:49:29,880 --> 00:49:36,120
And in that person, you give them a drug that may make some of them have diarrhea eight times a

592
00:49:36,200 --> 00:49:41,080
day or make their hands and feet ache or make them lose their hair or have bone marrow suppression

593
00:49:41,080 --> 00:49:45,480
or all of these things. But because they're so fit, they can tolerate that drug and they can take that

594
00:49:45,480 --> 00:49:50,440
dose and they can handle those side effects. And in that person, even though they push the dose in

595
00:49:50,440 --> 00:49:55,560
that person to handle all that side effects, the benefit is still 0.7 months. So now imagine what

596
00:49:55,560 --> 00:50:00,040
happens when you give it to an older person who's frail, who can't handle the full dose because they

597
00:50:00,040 --> 00:50:04,760
need a dose reduction because that side effect is massively more severe in that person. What's the

598
00:50:04,760 --> 00:50:08,920
benefit in that person? And I think a number of empirical studies have looked at these cancer

599
00:50:08,920 --> 00:50:14,760
drugs and find that the benefits in the average American are maybe even absent. I mean, I'll give

600
00:50:14,760 --> 00:50:19,960
you one example. There's a drug seraphanib in the trial that led to its approval. This is for patients

601
00:50:19,960 --> 00:50:24,040
with liver cancer that can't be treated by surgery. This is liver cancer where the horse has left out

602
00:50:24,040 --> 00:50:28,680
of the barn and it cannot be cured with surgical resection or transplant. If you randomize them to

603
00:50:28,680 --> 00:50:33,800
seraphanib or placebo, sugar pale, 11 months median survival with seraphanib and about eight months

604
00:50:33,880 --> 00:50:37,720
with placebo, a difference of about three months. And this got a standing ovation at the national

605
00:50:37,720 --> 00:50:42,760
meeting. People really celebrated this drug. A couple of researchers looked at Medicare data

606
00:50:42,760 --> 00:50:48,120
sets, your Medicare, which is Americans over the age of 65. And they found people who took seraphanib

607
00:50:48,120 --> 00:50:52,760
for this disease. And they found the median survival of somebody who took seraphanib in

608
00:50:52,760 --> 00:50:58,440
Medicare was around four months. So in other words, in the real world, somebody taking the drug

609
00:50:58,440 --> 00:51:03,880
that improves survival lives 50% as long as the person taking sugar pill in the trial,

610
00:51:03,880 --> 00:51:08,120
which just shows you that the Grand Canyon of difference between real world patients and

611
00:51:08,120 --> 00:51:12,680
clinical trial patients. And in the real world, they compared those people taking seraphanib

612
00:51:12,680 --> 00:51:18,600
who live four months to similar people who didn't take seraphanib. And they also live four months.

613
00:51:18,600 --> 00:51:23,080
So I think that some of the benefits of these drugs do evaporate when you give them in broad

614
00:51:23,080 --> 00:51:27,560
populations. First of all, I'm really glad you brought that up. I interviewed Azra Raza a little

615
00:51:27,560 --> 00:51:31,800
while ago and she made the same point. I just don't think that can be overstated. So I want to

616
00:51:31,800 --> 00:51:39,880
make sure that people really understand what you're getting at. When we look at name plate clinical

617
00:51:39,880 --> 00:51:48,200
trials, they aren't just best case scenario by a little bit. They are best case scenario by a log

618
00:51:48,200 --> 00:51:53,880
order based on patient selection. And let's be clear, if you're in the business of trying to get

619
00:51:53,880 --> 00:52:01,320
your drug approved, it is in your best interest to spoon feed and handpick the absolute healthiest

620
00:52:01,320 --> 00:52:08,280
people on the planet in whom to test your drug. So again, the system is set up to make this happen.

621
00:52:08,280 --> 00:52:15,480
This isn't some grand conspiracy. It's common sense. You've spent a billion dollars generating

622
00:52:15,480 --> 00:52:22,040
this drug. The final hurdle for you to get this drug to market is a very large phase three trial.

623
00:52:22,040 --> 00:52:26,840
You're not about to blow it by screwing up the patient's selection. It's no different

624
00:52:26,840 --> 00:52:33,560
than being a trial lawyer who spends a year preparing for a case only to pick the wrong

625
00:52:33,560 --> 00:52:39,720
jury. You've got to do your job and pick the right patients. Your point is, hey, and Azra made the

626
00:52:39,720 --> 00:52:46,600
same point. Look, the likelihood that the person sitting in your clinic in front of you is half as

627
00:52:46,600 --> 00:52:51,480
healthy as the patient that clinical trial is very low. And therefore, they're not going to be as

628
00:52:51,560 --> 00:52:58,680
resilient, which means A, they probably can't tolerate the drug as well. And B, they don't

629
00:52:58,680 --> 00:53:06,760
have the physiologic reserve such that the delta between them and the untreated patient is likely

630
00:53:06,760 --> 00:53:12,200
to be far compressed. That's right. And I think you make another terrific point, which is that

631
00:53:12,920 --> 00:53:16,440
we can't blame the tiger for being the tiger. The pharmaceutical company is doing what's rational.

632
00:53:16,520 --> 00:53:21,800
They're tasked with running a trial to test their own product. If you win, you're going to earn,

633
00:53:22,360 --> 00:53:28,520
on average, $12 billion in the next 14 years. If you get a P of 0.049, you get 12 billion.

634
00:53:28,520 --> 00:53:33,640
If you get a P of 0.051, you get minus a few hundred million dollars, your outlay on the drug.

635
00:53:33,640 --> 00:53:39,160
And if you have an incentive system like that, I mean, you should not blame the industry for,

636
00:53:39,160 --> 00:53:44,360
I think, all of the things we see in clinical trial design, which are sort of okay, acceptable

637
00:53:44,360 --> 00:53:49,320
ways to put a thumb on the scale. One of which is you carefully curate your patient population.

638
00:53:49,320 --> 00:53:53,960
Another is you test your new drug against, well, maybe not the drug doctors are actually using,

639
00:53:53,960 --> 00:53:59,720
maybe you test it against the oldest, weakest drug in the space. Maybe for patients who take the old

640
00:53:59,720 --> 00:54:05,000
drug, when they have progressive disease, they don't get access to the best new drugs. They get

641
00:54:05,000 --> 00:54:09,000
substandard care, which often happens in these registration studies. There are a number of ways

642
00:54:09,000 --> 00:54:13,480
that I think are within the realm of what people accept that allow for gaming of the trial. And it

643
00:54:13,560 --> 00:54:17,880
would be irrational not to take advantage of that because the amount of money at stake is vast. And

644
00:54:17,880 --> 00:54:22,680
the incentive to put a thumb on the scale when you can is great. But I think it's worth restating,

645
00:54:22,680 --> 00:54:26,760
as you say, that these are drugs that we're really talking about the best case scenario and the best

646
00:54:26,760 --> 00:54:30,680
case scenario is often less than desired. Should we go to the next part, which is the cost? How do

647
00:54:30,680 --> 00:54:35,720
you decide who pays? Yeah. So I'll think back to probably the first time I remember thinking about

648
00:54:35,720 --> 00:54:41,960
this was when Gleevac was approved. Now Gleevac was, God, I'm trying to think I was probably in

649
00:54:41,960 --> 00:54:47,480
residency when Gleevac was approved. Yeah, 2001. Yeah. This was a big deal because I remember in

650
00:54:47,480 --> 00:54:51,800
medical school, I had read Judah Folkman's book. Judah Folkman has since passed, but he was sort

651
00:54:51,800 --> 00:54:59,000
of a luminary oncologist and he was basically one of the first people to propose this idea that if

652
00:54:59,000 --> 00:55:05,320
you cut off the blood supply to a tumor, you could stop a tumor. And again, to put this in the broader

653
00:55:05,320 --> 00:55:10,600
context, this was a pretty remarkable idea because up until that point in time, you had this idea

654
00:55:10,600 --> 00:55:16,120
that you could cut a tumor out if it was localized. You could give a bunch of chemicals that targeted

655
00:55:16,120 --> 00:55:21,000
its ability to replicate. That was sort of the gist of chemotherapy, or you could radiate the

656
00:55:21,000 --> 00:55:25,720
crap out of it, which also basically destroyed its ability to replicate. And some people like

657
00:55:25,720 --> 00:55:31,320
Steve Rosenberg and Jim Allison were working on these immunotherapy ideas, but Judah comes along

658
00:55:31,320 --> 00:55:37,160
and says, look, there's another really obvious idea here, which is any cancer cell that leaves its

659
00:55:37,160 --> 00:55:41,320
site of origin and goes to take up residence somewhere else, better figure out a way to get

660
00:55:41,320 --> 00:55:47,160
blood. And so we have these growth factors for blood, VEGF being one of them. And so this whole

661
00:55:47,160 --> 00:55:52,280
new thing of anti-VEGF compounds comes along. And so Gleevac becomes the first drug approved for this.

662
00:55:52,280 --> 00:55:56,120
And if my memory serves me correctly, it was for colon cancer was the first indication.

663
00:55:56,120 --> 00:56:00,200
You probably think of a Bevacizumab and Avastin. Oh, that's right. I'm sorry. Sorry. Sorry. Sorry.

664
00:56:00,200 --> 00:56:03,240
You're absolutely right. I'm thinking of Avastin. Of course, Gleevac is for

665
00:56:03,400 --> 00:56:08,440
GIS, Dremel tumors. And then, yeah, I'm actually thinking of Avastin. Yes. Okay. Same time period

666
00:56:08,440 --> 00:56:13,000
though. Avastin was about 2000, 2001, maybe 2002. And was it colon cancer for Avastin?

667
00:56:13,000 --> 00:56:14,520
Yeah, it was the first one. Yeah.

668
00:56:14,520 --> 00:56:20,840
And I sort of remember directionally it being about a hundred thousand bucks for a year.

669
00:56:21,400 --> 00:56:25,880
And do you remember what the improvement in survival was with all the noted caveats that

670
00:56:25,880 --> 00:56:29,160
we just gave? Was it like eight versus 12 months or something to that effect?

671
00:56:29,640 --> 00:56:33,640
I think it was less than that. I think the original paper that led to approval was the

672
00:56:33,640 --> 00:56:37,800
Hurwitz paper. And I think it was on the order of a couple of months. But I guess I would say that

673
00:56:37,800 --> 00:56:42,360
if you look at Avastin across all the different cancers, I mean, you're, you look at where it

674
00:56:42,360 --> 00:56:45,640
works and where it doesn't work, where it works, we're talking about a month and a half, two months.

675
00:56:45,640 --> 00:56:50,280
I mean, that's about the average. The point of my very unfortunate long-winded story was

676
00:56:51,160 --> 00:56:57,320
certain countries, and I think the UK was on the list, just said, no, we're not paying for this

677
00:56:57,320 --> 00:57:03,880
drug because it doesn't reach our threshold, which I think at the time was about 100,000

678
00:57:03,880 --> 00:57:09,640
pounds per quality adjusted life year per quality. And the United States, of course,

679
00:57:09,640 --> 00:57:13,640
took a different position, which was private insurance companies will pay for this, which

680
00:57:13,640 --> 00:57:18,120
basically means the government and or employer will pay for this. And I remember that being the

681
00:57:18,120 --> 00:57:23,480
very first time that I thought about this and thought, huh, what is the implication of this?

682
00:57:23,480 --> 00:57:28,360
How does this work? So how do you think about that? And again, this is almost asking you to

683
00:57:28,360 --> 00:57:33,240
put on a policy hat on half of your head with a physician hat on the other half of your head,

684
00:57:33,240 --> 00:57:36,920
right? I think you're right about one thing, which is that there are different hats. And when you're

685
00:57:36,920 --> 00:57:40,120
in the room, what you do might not be the same thing as what you advise a government to do.

686
00:57:40,120 --> 00:57:43,320
And maybe that's okay. That's the right way these things should happen. When you're in the room,

687
00:57:43,320 --> 00:57:46,680
you do everything you can that adds anything that the patient really wants and that the

688
00:57:46,680 --> 00:57:50,840
toxicity worth it to the patient. When you were in the policy hat, you ask what's best for everybody.

689
00:57:50,840 --> 00:57:55,320
And I guess what I would say here is one thing that fits in this discussion is that every dollar

690
00:57:55,320 --> 00:58:00,040
spent on Avastin is a dollar not spent on a lot of healthcare interventions that may have better

691
00:58:00,040 --> 00:58:04,840
bang for your buck. May I take all that money that we're spending on Avastin for 100 people,

692
00:58:04,840 --> 00:58:09,160
and we might get 100,000 people to take their lisinopril or their hydrochlorothiazide, their

693
00:58:09,160 --> 00:58:13,800
blood pressure pills more religiously, we might get them to do something else. And the cumulative

694
00:58:13,800 --> 00:58:18,440
number of life years added to the world from those people doing whatever that other thing is,

695
00:58:18,440 --> 00:58:23,480
that may exceed the Avastin benefit by an order of magnitude. When societies pay for something,

696
00:58:23,480 --> 00:58:27,160
it is different than when you and I pay for something. If I take my own money and buy

697
00:58:27,160 --> 00:58:30,680
anything, it's really nobody else's business. And if you do whatever you want with your money.

698
00:58:30,680 --> 00:58:35,080
But if we all make a commitment to healthcare, what we're saying is, we're all going to pool all this

699
00:58:35,080 --> 00:58:39,960
money. And we're going to pull this money so that we pay for this thing that we think is a different

700
00:58:39,960 --> 00:58:44,040
commodity. It's something that's a human right, something that we all deserve to have. The only

701
00:58:44,120 --> 00:58:48,760
rational way I think a society to spend that money is to do what benefits the most people,

702
00:58:48,760 --> 00:58:53,800
what brings the most good in the world. That might mean sometimes societies make, as the UK does,

703
00:58:53,800 --> 00:58:59,800
tough decisions. They decide instead of 20 people getting access to some new cancer drug that

704
00:58:59,800 --> 00:59:04,040
doesn't cure the disease but may extend survival by a couple months, let's take all that money and

705
00:59:04,040 --> 00:59:09,720
let's give pregnant women access to X, Y, or Z that might improve the longevity of their children

706
00:59:09,720 --> 00:59:15,080
and another generation of kids in the UK. And so I think that all nations struggle with this

707
00:59:15,080 --> 00:59:21,000
question, which is how do we ration limited resources? In other countries, they are upfront

708
00:59:21,000 --> 00:59:26,360
and open about the discussion. They use things like cost effectiveness to ration products.

709
00:59:26,360 --> 00:59:31,080
In the US, we do ration. We just ration in a way you don't see because some people don't get

710
00:59:31,080 --> 00:59:35,640
anything. They don't get beneficial medicines and they don't get really marginal medicines either.

711
00:59:35,720 --> 00:59:40,200
They just don't have access at all. They're cut out of the system. And so we ration by

712
00:59:40,200 --> 00:59:44,360
discriminating against people. That's a different type of rationing. It's cruel and irrational,

713
00:59:44,360 --> 00:59:48,760
but it is a rationing nonetheless. I think that this is a thorny problem and no one person has the

714
00:59:48,760 --> 00:59:53,800
answer. And I guess I also think that people should be free to spend their own money as they so choose.

715
00:59:53,800 --> 00:59:58,440
I think the reality is if you had to spend your own money versus deciding whether or not to leave

716
00:59:58,440 --> 01:00:03,240
that money for your children or for a loved one, I think a lot of people wouldn't buy these drugs.

717
01:00:03,560 --> 01:00:06,520
There might be some ultra wealthy people, but I think some of us might make different choices

718
01:00:06,520 --> 01:00:10,840
with our own money. If you have to spend society's money, I think the obligation to really know that

719
01:00:10,840 --> 01:00:14,920
they work, that they actually are delivering benefit to people, I think that obligation is

720
01:00:14,920 --> 01:00:19,320
a bit stronger. When you go back and think about your undergrad as a philosophy major,

721
01:00:19,320 --> 01:00:23,720
this is probably one of those places where you probably have an insight that someone like me

722
01:00:23,720 --> 01:00:28,440
doesn't have. How do you think about this through the lens of what philosophers would have said?

723
01:00:29,240 --> 01:00:33,480
Yeah, I mean, I guess I would say, I don't know if that's true, that I have an insight that you

724
01:00:33,480 --> 01:00:37,400
don't have. I think you have a lot of good insights. I guess I would say, obviously, what

725
01:00:37,400 --> 01:00:41,800
I'm speaking about is a type of ethic, which is probably called utilitarian ethics, which is this

726
01:00:41,800 --> 01:00:47,320
idea that ethical principles, when they are in conflict, they prioritize the greatest good for

727
01:00:47,320 --> 01:00:52,840
the greatest number over a lesser good for a lesser number. And when it comes to deciding

728
01:00:52,840 --> 01:00:58,200
whether or not to cover really costly cancer drugs that have small benefits and idealized settings

729
01:00:58,200 --> 01:01:02,920
and have question mark benefits in real world settings, versus paying for other things that

730
01:01:02,920 --> 01:01:07,160
have more convincing evidence that they improve outcomes for a lot more people, I think a

731
01:01:07,160 --> 01:01:11,800
utilitarian ethic would tend to side with the latter that you got to pay for what benefits more

732
01:01:11,800 --> 01:01:19,000
people. I mean, one of the constant criticisms of the UK system is that cancer outcomes aren't as

733
01:01:19,000 --> 01:01:24,920
good in the UK as they are in the US. I think that fact alone has been overstated to some degree.

734
01:01:24,920 --> 01:01:28,520
But that's not really the question because the question is their life expectancy is better.

735
01:01:28,520 --> 01:01:31,560
They're spending less on healthcare, they got better life expectancy, their medical care may

736
01:01:31,560 --> 01:01:35,480
be better. I think if you're looking at somebody, if you're looking at the person just getting the

737
01:01:35,480 --> 01:01:39,080
Avastin, I think the decision is different than if you're looking at everybody. There are different

738
01:01:39,080 --> 01:01:43,320
conceptions of ethics. And I think some people might put a value on caring for people at the

739
01:01:43,320 --> 01:01:48,280
end of life, irrespective of I think societal implications for others. And I guess I don't,

740
01:01:48,280 --> 01:01:52,360
sort of a deontologic perspective, and I guess I don't discount that at all. I mean, I think

741
01:01:52,440 --> 01:01:56,920
when people are question mark on Avastin for colon cancer, we're not saying don't treat a person,

742
01:01:56,920 --> 01:02:00,360
there's still lots of treatments you would give. You give the same treatment except leave out the

743
01:02:00,360 --> 01:02:04,600
Avastin and the difference is probably very slight. Maybe there's no difference at all,

744
01:02:04,600 --> 01:02:07,720
is what somebody like me might think if I haven't reviewed all the evidence.

745
01:02:08,600 --> 01:02:12,760
One of the things that I just think, I'll tell you a funny story and I'll come back to the thing.

746
01:02:12,760 --> 01:02:16,120
And I may have even mentioned this during the podcast before, but I've got a friend who used

747
01:02:16,120 --> 01:02:20,680
to live in Saudi Arabia. He's an American, lived in DC. So during the summer, he would always come

748
01:02:20,680 --> 01:02:25,720
back to DC. So he would sort of spend June to September in DC before going back to Saudi

749
01:02:25,720 --> 01:02:32,920
Arabia. And I was there visiting him once and I asked him, I said, dude, what's it like when you

750
01:02:32,920 --> 01:02:38,440
come back to your apartment in September? Just tell me how hot it is. Is it like, does it hit 130

751
01:02:38,440 --> 01:02:43,080
degrees after you've left the apartment sitting there all summer? He goes, no, man, it's like 70

752
01:02:43,080 --> 01:02:47,720
degrees. I leave the AC on the whole summer. And I'm like, what do you mean you leave the AC on

753
01:02:47,720 --> 01:02:51,560
the whole summer? You have a timer set to come on an hour before you get there. He goes, no, no,

754
01:02:51,560 --> 01:02:57,320
no, no. When I leave, the AC is on. When I come back, the AC is on. I can't wrap my head around

755
01:02:57,320 --> 01:03:04,600
this. I'm like, how criminal is that? How much money are you spending? He goes, it costs me $4

756
01:03:04,600 --> 01:03:09,960
a month. I'm like, how is that possible? He goes, oh, our energy is totally subsidized by the

757
01:03:09,960 --> 01:03:15,160
government here. We pay 19 cents a gallon for gasoline. So it's about $4 a month for me to,

758
01:03:15,160 --> 01:03:20,440
at the time, this was more than 10 years ago, it's about $4 a month for me to keep my apartment

759
01:03:20,440 --> 01:03:25,880
in Saudi Arabia and Riyadh at 70 degrees when it's 120 degrees outside. Now you have to understand

760
01:03:25,880 --> 01:03:33,400
something. This is a totally rational human being. This is a reasonable guy who would never do this

761
01:03:33,400 --> 01:03:42,200
in his apartment in Washington, DC. It just gave me a great example of how when we don't have skin

762
01:03:42,200 --> 01:03:49,880
in the game, we are incapable of making rational cost decisions. It's like, if I said to you,

763
01:03:49,880 --> 01:03:55,960
you can have any car you want, but you only have to pay 5% of the cost of the car, I think you're

764
01:03:55,960 --> 01:03:59,720
going to make a very different decision than if I say you can have any car you want, but you

765
01:03:59,720 --> 01:04:04,920
actually have to pay for the whole car. Now we can lend you the money and you can do X, Y, and Z.

766
01:04:04,920 --> 01:04:08,760
I guess the thing I've always struggled with, I don't know how to solve the problem of healthcare

767
01:04:08,760 --> 01:04:15,080
cost of which oncology is a huge driver, and we're going to get to that shortly, because I don't

768
01:04:15,080 --> 01:04:21,480
know how to reconcile these two points, which is on the one hand, you cannot make rational decisions

769
01:04:21,480 --> 01:04:26,920
if you don't have skin in the game. And if the physician and the patient don't have skin in the

770
01:04:26,920 --> 01:04:33,720
game, how is a rational decision being made? But on the other hand, with the costs of these things

771
01:04:33,720 --> 01:04:39,080
being so prohibitive, how can people have meaningful skin in the game? So how do we

772
01:04:39,080 --> 01:04:43,880
reconcile these two forces? Probably not going to have the perfect answer for that. I like what

773
01:04:43,880 --> 01:04:46,680
you're saying and I like your analogy and I want to kind of expand on it a little bit.

774
01:04:47,720 --> 01:04:52,520
So in your analogy, I think you're right that your friend is making the rational choice, which is

775
01:04:52,520 --> 01:04:56,440
it doesn't cost me much more, it's like a 50 cents, let us keep it cool all day. Who cares?

776
01:04:56,440 --> 01:05:00,520
Somebody else is paying for that. In your car analogy, yeah, if you only pay 5% of the price

777
01:05:00,520 --> 01:05:05,560
of car, yeah, I'm going to drive McLaren. I like your choice by the way. Good choice. McLaren,

778
01:05:05,560 --> 01:05:10,360
thank you for that. Big fan of Top Gear and Grand Tour. But I guess I would say, but to make it

779
01:05:10,360 --> 01:05:15,240
really more analogous to what we're thinking about, you pay 5% for the price of the car,

780
01:05:15,240 --> 01:05:20,040
but you never actually get the car and you don't get to drive the car. You just get told how good

781
01:05:20,040 --> 01:05:24,680
the car is. I'm telling you how good the car is. This is a great car. Don't you worry about how

782
01:05:24,680 --> 01:05:29,640
good the car is. You are not in a position to judge how good the car is. Having the car may change

783
01:05:29,640 --> 01:05:34,760
what you do. What do I mean by that? When it comes to cancer drugs, in a theoretical situation,

784
01:05:34,760 --> 01:05:39,640
I do think people will say, if it adds anything, if there's any upside and somebody else is paying

785
01:05:39,640 --> 01:05:44,200
most of the cost, I'm willing to accept that. I want to try it. Let's just try. That's the mantra

786
01:05:44,200 --> 01:05:49,640
of oncology. But many of these drugs, that idea that it only has an upside, that is sort of a

787
01:05:49,640 --> 01:05:54,040
construct that's been created through a system that is not giving you the accurate information.

788
01:05:54,040 --> 01:05:58,280
And in fact, some of these drugs, they may actually have a net downside. How might that be? One,

789
01:05:58,280 --> 01:06:02,440
they don't actually improve survival. That's one. They have no survival benefit. Two,

790
01:06:02,440 --> 01:06:08,200
they may have an opportunity cost that instead of being somebody who in their last few months of

791
01:06:08,200 --> 01:06:14,680
life is going to Tahiti or going to visit a friend, I'm somebody who is tethered to the infusion

792
01:06:14,680 --> 01:06:18,520
suite. And I got to keep coming back twice weekly for four weeks. I got to keep seeing this doctor.

793
01:06:18,520 --> 01:06:23,720
I got to spend 15% of my extra life you gave me, maybe 40% of that I'm spending in your lobby.

794
01:06:23,720 --> 01:06:28,760
You're taking it back from me. And so it makes people make choices about what they prioritize

795
01:06:28,760 --> 01:06:34,120
and how they view the end of life that may actually, I think, dissolve whatever ideal

796
01:06:34,120 --> 01:06:38,360
benefits that the drugs provide. I mean, your point is well taken that there is this sort of

797
01:06:38,360 --> 01:06:43,320
tension between who pays for something and choices people make. And I guess the way I try to get

798
01:06:43,320 --> 01:06:48,120
around that is, if we could just start an oncology with the choices that probably are not in your

799
01:06:48,120 --> 01:06:51,880
best interest, we probably would save a lot of money and people will be better off for it. And

800
01:06:51,880 --> 01:06:54,840
then we can go to the next level of choices where there really is that trade off that you

801
01:06:54,840 --> 01:07:00,440
articulate so well. What do you think are the no regret moves in oncology today?

802
01:07:02,120 --> 01:07:06,920
I guess I could do a tumor by tumor. I guess I mean, if you have localized cancer for the most

803
01:07:06,920 --> 01:07:12,200
part, no regret move is to cut it out surgery, you guys win a lot, no regret moves. Very little

804
01:07:12,200 --> 01:07:17,320
advanced in the last 50 years, right? I mean, where have we gotten better? Well, I think 50

805
01:07:17,320 --> 01:07:23,400
years ago, Whipple procedure carried a much higher mortality than today. I think 50 years ago,

806
01:07:23,400 --> 01:07:29,880
women were decimated by radical mastectomies that are, I didn't do one radical mastectomy in

807
01:07:29,880 --> 01:07:34,920
my residency. So basically for 20 years, that's a procedure that's never been done. And today,

808
01:07:34,920 --> 01:07:39,640
there's been a push to more and more and more localized surgeries with just as good an outcome.

809
01:07:39,640 --> 01:07:44,680
So that's been a huge advent. But yeah, for the most part, we figured that one out a long time ago.

810
01:07:44,680 --> 01:07:45,400
Exactly, right.

811
01:07:45,400 --> 01:07:46,920
Let's keep doing it. Okay, next.

812
01:07:47,480 --> 01:07:52,040
So next category. So let's get into I mean, I'm gonna make a nod to radiotherapy, which I think

813
01:07:52,040 --> 01:07:57,640
is really useful in many situations. Maybe the Michael Douglas situation had a cancer localized

814
01:07:57,640 --> 01:08:00,920
probably anything for a or better, you know, you get a great shot of a long lasting cure with

815
01:08:00,920 --> 01:08:04,760
chemo radiotherapy. Okay, so I think surgery radiotherapy are on the table. Now let's talk

816
01:08:04,760 --> 01:08:09,960
about drugs. You got Hodgkin's lymphoma. Well, good news. We've got a four drug combination

817
01:08:09,960 --> 01:08:14,760
that's curative. You've got testicle cancer and it's spread to your lung. Good news, we can cure

818
01:08:14,760 --> 01:08:21,240
95% plus of you, including those that are non-seminomatous. Now, obviously, it depends on

819
01:08:21,240 --> 01:08:25,560
the exact subtype. And sometimes you require a combination of both chemotherapy and surgical

820
01:08:25,560 --> 01:08:29,960
therapy. So for instance, like teratoma or something like that. CML chronic myeloid leukemia,

821
01:08:29,960 --> 01:08:34,360
this is the Gleevec story that I think is transformative. Some will argue is not cancer

822
01:08:34,360 --> 01:08:38,680
right away, the skeptic would say, is this a cancer, right? I mean, it's a very fair point,

823
01:08:38,680 --> 01:08:43,400
which is that what is CML? It is a blood based cancer. One of the things about blood based

824
01:08:43,400 --> 01:08:47,240
cancers is you tend to see that a lot sooner than you see other cancers because it's easy to find

825
01:08:47,240 --> 01:08:50,280
because it's in your blood, you can just draw the blood and there it is. And if you had that

826
01:08:50,280 --> 01:08:55,720
access to every organ in the same way, maybe you would be finding similar malignant lesions and

827
01:08:55,720 --> 01:09:00,360
other organs. That's a possibility. We also know it's genomically dumb. It's driven by BCR-able

828
01:09:00,360 --> 01:09:05,080
fusion, a genomic event, and not a whole heck of a lot else if that's the sole driver. And if you

829
01:09:05,080 --> 01:09:10,520
give a drug that inhibits that fusion event, you can turn median life expectancy from three to four

830
01:09:10,520 --> 01:09:15,480
years to nearly normal life expectancy in a Swedish dataset. They're living almost a full life,

831
01:09:15,480 --> 01:09:20,040
maybe a year shy of normal life expectancy that is a tremendous advance in medicine.

832
01:09:20,040 --> 01:09:24,360
So those are just a few no brainers, but there are a lot of no brainers I think that have to do with

833
01:09:24,360 --> 01:09:30,680
there's a benefit and the toxicity is low, or there's a benefit, a smaller benefit, but the

834
01:09:30,680 --> 01:09:34,360
toxicity is reversible. So you could try it and if it doesn't go well, you can stop it and you don't

835
01:09:34,360 --> 01:09:39,320
have them lost too much. I think those are kind of no brainers, but some things are much more dubious,

836
01:09:39,320 --> 01:09:46,200
which they are caustic, toxic treatments, long-term toxicity, toxicities that don't get better,

837
01:09:46,200 --> 01:09:50,440
don't go away, that at the best don't add that much. I mean, I think there are a lot of dubious

838
01:09:50,440 --> 01:09:55,320
choices that people find themselves in. And I think the other thing that's hard to express is that

839
01:09:55,880 --> 01:10:01,080
the person in the chair, the patient, is in a very difficult position. I mean, they didn't spend their

840
01:10:01,080 --> 01:10:05,160
lives studying cancer and studying all the data or maybe even studying how to think about cancer

841
01:10:05,160 --> 01:10:09,400
data and they're in a vulnerable position. Their life is on the line and that's not always a great

842
01:10:09,400 --> 01:10:14,520
position to make choices. And people often tend to make choices that may not be compatible with

843
01:10:14,520 --> 01:10:19,160
their best interest. Some people make choices that are driven by family members who believe those

844
01:10:19,160 --> 01:10:24,040
choices are in their best interest, but they might not be. There's a lot of social complexity here

845
01:10:24,040 --> 01:10:28,280
that makes it a thorny problem. But yeah, there are few no-brainer decisions. There are some real

846
01:10:28,280 --> 01:10:32,600
advances in the last 20 years and then there's a lot of fool's gold. A lot of things people say

847
01:10:32,600 --> 01:10:36,920
are game changers, miracles, revolutions that are no such thing, but really don't have those

848
01:10:36,920 --> 01:10:41,160
substantive benefits when you look at it critically. Give me an example of something you put in that

849
01:10:41,160 --> 01:10:45,720
latter category of something that's getting a ton of attention that you're not a big believer in. I

850
01:10:45,720 --> 01:10:51,720
mean, where do the cell-based therapies for immunotherapy fit in or something like checkpoint

851
01:10:51,720 --> 01:10:57,640
inhibitors like Ktruda, which I mean, boy, when those things work, they're game changers. And when

852
01:10:57,720 --> 01:11:02,760
they don't work, they don't work. Yeah. I mean, I guess I would say, I have a good example that

853
01:11:02,760 --> 01:11:06,840
comes to mind, which I will come to, but let me just first comment about those things. I mean,

854
01:11:06,840 --> 01:11:11,960
I guess I would say the cell-based therapies or things like CAR T or Tisagen, like Lusol or these

855
01:11:11,960 --> 01:11:16,440
sort of novel cell products, they still have a lot of promise and they have definitely shown sort of

856
01:11:17,080 --> 01:11:22,920
great responses in some people. But the real question will be, are they so beneficial in

857
01:11:22,920 --> 01:11:27,160
every condition? We're trying some of those cell-based therapies in multiple myeloma and a lot

858
01:11:27,160 --> 01:11:30,280
of people are having remissions, but those remissions are fleeting and it looks like everyone

859
01:11:30,280 --> 01:11:33,880
is having relapse. That's different than what we found with lymphoma and what we found with

860
01:11:33,880 --> 01:11:38,760
pediatric acute lymphoblastic leukemia, or ALL, which you talked about earlier. With Ktruda,

861
01:11:38,760 --> 01:11:43,080
we have a number of randomized control trials that show clear survival benefits in Ktruda in

862
01:11:43,080 --> 01:11:46,600
specific settings. We also have a bunch of settings that have been disappointing and we've

863
01:11:46,600 --> 01:11:51,000
had confirmatory randomized trials that don't show benefits. So Ktruda might be almost a case-by-case

864
01:11:51,000 --> 01:11:55,640
basis. What cancer, what patient? But let me give you a good example of a drug that I think is just

865
01:11:55,640 --> 01:12:00,200
a classic example of really a questionable decision. I'll talk about it in one cancer,

866
01:12:00,200 --> 01:12:05,560
pancreas cancer. So some people with pancreas cancer have a germline mutation in BRCA gene,

867
01:12:05,560 --> 01:12:09,880
or BRACA, which is a gene that confers susceptibility to cancers, one of which is

868
01:12:09,880 --> 01:12:14,440
pancreas, but often breast cancers where you most think about it. And if you have a BRACA germline

869
01:12:14,440 --> 01:12:19,160
mutation in pancreas cancer, they did a randomized trial where they gave people four months of the

870
01:12:19,160 --> 01:12:24,120
standard of care for fulfirinox therapy and they stopped it after a minimum of four months. You

871
01:12:24,120 --> 01:12:27,160
could have had a little bit more, but they stopped it after four months and randomized you to

872
01:12:27,160 --> 01:12:32,360
elaprib or sugar pill. And if you take elaprib, you have a progression-free survival benefit,

873
01:12:32,360 --> 01:12:36,760
which I can explain what it is in a second, versus sugar pill, but you don't have an overall

874
01:12:36,760 --> 01:12:43,080
survival benefit. You're not living any longer. And maybe it wasn't a good idea to stop fulfirinox.

875
01:12:43,080 --> 01:12:46,440
I think a lot of doctors would have continued that. So the control arm shouldn't have been

876
01:12:46,440 --> 01:12:51,160
sugar pill. It should have been continuing the drugs that were working. This is a trial called

877
01:12:51,160 --> 01:12:58,200
polo. This is a drug that has a cost of about $12,000 a month. And the reason the rhetoric

878
01:12:58,200 --> 01:13:03,960
and the reality are so separated is that the drugs we were using in pancreas cancer were old

879
01:13:03,960 --> 01:13:08,840
cytotoxic chemotherapy drugs. They were drugs that were not sexy, didn't sound so great.

880
01:13:08,840 --> 01:13:13,480
And this is a new targeted drug, quote unquote targeted drug. And it's based on your genomic

881
01:13:13,480 --> 01:13:18,120
signature, which is unique to just your tumor. I mean, it sounds like that should be terrific.

882
01:13:18,120 --> 01:13:22,840
The reality is the data suggests it might not be as good as if you continued that old fashioned

883
01:13:22,840 --> 01:13:27,960
therapy. So that's an example where I think that hype can easily outpace benefit. And then I guess

884
01:13:27,960 --> 01:13:31,640
I just wanted to just explain real quick for the listener, progression-free survival, which comes

885
01:13:31,640 --> 01:13:37,080
up just so often in cancer. It's a unique endpoint. It's really often interpreted in the lay press as

886
01:13:37,080 --> 01:13:42,120
the time it takes for cancer to get worse. That's not quite right. So progression-free survival is

887
01:13:42,120 --> 01:13:46,680
the time from when a patient enrolls on a study to one of four things really happening to them.

888
01:13:47,240 --> 01:13:51,400
One, they could die. The first thing they could be going along and then one day they could just

889
01:13:51,400 --> 01:13:56,440
die. So that's part of the endpoint. So if you die first, that's it. You have a PFS event.

890
01:13:56,440 --> 01:14:01,080
Fortunately for most trials, that's not the most common thing that counts as a PFS event. The second

891
01:14:01,080 --> 01:14:05,240
thing that could happen is you have a new lesion on your CAT scan. We're scanning you along and

892
01:14:05,240 --> 01:14:08,520
your lungs, we didn't find anything, but now there's a little ditzel there and I stick a

893
01:14:08,520 --> 01:14:12,520
needle in it and it's pancreas cancer. So you have progressed, you got a new lesion. That's

894
01:14:12,600 --> 01:14:17,800
progression. The third thing that could happen is your tumor that we measured at one centimeter,

895
01:14:17,800 --> 01:14:24,520
it got to 1.2 centimeters. It got 20% bigger. The moment it gets 20% bigger, 21% bigger,

896
01:14:24,520 --> 01:14:29,080
it's progression. When it's 19% bigger, oh, it's stable disease. That's what we call that. That's

897
01:14:29,080 --> 01:14:34,840
stable, but then 20%, 21%, that's progression. So it's an arbitrary cut point, very arbitrary.

898
01:14:34,840 --> 01:14:38,680
And that's why it doesn't always track with how people feel. The fourth thing that could happen

899
01:14:38,680 --> 01:14:43,320
is your tumor got smaller before it got bigger. And if your tumor got smaller before it gets

900
01:14:43,320 --> 01:14:48,760
bigger, it's 20% from the smallest it ever was. So those are the four things that count as

901
01:14:48,760 --> 01:14:55,560
progression. Often trials are driven by the latter two events. The tumors look 120% bigger.

902
01:14:55,560 --> 01:15:00,200
I like to also tell people that when you measure a tumor on a CAT scan, if you haven't done a lot

903
01:15:00,200 --> 01:15:03,000
of that, I'm sure you've done that, but if you haven't done a lot of that, people think it's

904
01:15:03,000 --> 01:15:07,240
like measuring your height. It's a lot more like measuring the width of a cloud between your fingers

905
01:15:07,320 --> 01:15:11,640
looking up at the sky. People have dispute about where the tumor ends and where the normal tissue

906
01:15:11,640 --> 01:15:16,040
begins. And that's been shown in many studies. The reason I say all this is let's go back to that

907
01:15:16,040 --> 01:15:20,840
example, that drug. This is a drug when tested against sugar pill in a setting where probably

908
01:15:20,840 --> 01:15:24,440
shouldn't have been tested against sugar pill, should have been tested against a real therapy we do.

909
01:15:24,440 --> 01:15:30,040
The only thing it could improve was the progression free survival, which is an arbitrary line in the

910
01:15:30,040 --> 01:15:34,680
sand for tumor growth that really doesn't measure people living longer or feeling better. That's why

911
01:15:34,680 --> 01:15:39,640
I think it's really a problematic drug. Yeah, I remember. God, how many years ago? Is that five

912
01:15:39,640 --> 01:15:43,640
years ago or four years ago when that came out? This trial actually is more recent than that. It

913
01:15:43,640 --> 01:15:47,320
was just in the last year, but they probably started it five years ago. The enthusiasm for

914
01:15:47,320 --> 01:15:52,280
it was there and they started down the path. I think I'd followed that drug maybe back at its

915
01:15:52,280 --> 01:15:56,360
phase two. And I don't know why. For some reason, I felt like that was a fool's errand, but it seems

916
01:15:56,360 --> 01:16:02,200
like that just concludes it. So there are a couple things I want to come back to. But before we do

917
01:16:02,200 --> 01:16:09,000
that, I want to go back and talk through your hallmarks because this strikes me as the culmination

918
01:16:09,000 --> 01:16:14,920
of all the work you put into this book and you do what everybody's doing when they're writing a book.

919
01:16:14,920 --> 01:16:19,000
You're probably putting the finishing touches on it and you have this epiphany and you realize,

920
01:16:19,000 --> 01:16:24,520
well, I could go and weave it into the narrative of the book or I could very concisely lay it out

921
01:16:24,520 --> 01:16:28,360
here. And I think you chose the latter, which is actually quite elegant. So walk us through

922
01:16:29,320 --> 01:16:33,800
the notion of these hallmarks, these six hallmarks and explain what they mean because they're not

923
01:16:33,800 --> 01:16:39,720
entirely obvious just from the brief description. Sure. Thanks for that. I guess I think one year

924
01:16:39,720 --> 01:16:43,720
I really write about how it happened, which was you write a book and people think you write the

925
01:16:43,720 --> 01:16:48,040
book, but of course there's so much back and forth and it goes on for years. And where you are

926
01:16:48,040 --> 01:16:51,400
emotionally and mentally when you finish the book and where you started are very different places.

927
01:16:51,400 --> 01:16:55,480
And often you grow a lot in the process of writing it. It changes the way you think about things. And

928
01:16:55,640 --> 01:16:59,720
so I start by in this chapter by saying, here's a way I should have said things all along. I could

929
01:16:59,720 --> 01:17:04,120
have said things better, but at least for my case, it's better late than never. I could still

930
01:17:04,120 --> 01:17:08,120
toss it in the book. And I think the other thing the listener should know is there is something

931
01:17:08,120 --> 01:17:13,080
called the hallmarks of cancer. These are six things proposed by Hannahann and Weinberg. There's

932
01:17:13,080 --> 01:17:17,480
six hallmarks of cancer like invasion, invasion, basement membrane, poor immune surveillance,

933
01:17:17,480 --> 01:17:22,120
angiogenesis, one of the ones you mentioned. These are sort of six biological hallmarks of cancer.

934
01:17:22,120 --> 01:17:26,440
And that is a highly influential paper and it's been really extremely well cited and has

935
01:17:26,440 --> 01:17:31,000
shaped a lot of people thinking. It might be one of the most cited papers in oncology,

936
01:17:31,000 --> 01:17:33,880
isn't it? I think it might be. I think you're right. It might be in fact the most cited

937
01:17:33,880 --> 01:17:39,160
paper in oncology. So anyway, so like all great things I try to steal from them. No, I try to

938
01:17:39,960 --> 01:17:43,480
imitate. When I finished this book, I realized that, well, you know, I'm not talking about

939
01:17:43,480 --> 01:17:47,640
biology. And in fact, I say many times, it's not a cancer biology book. This is a book about policy,

940
01:17:47,640 --> 01:17:52,760
our rules, our laws, our guidelines, and how policy can make things better. And I realized

941
01:17:52,760 --> 01:17:58,840
that maybe policy can be distilled like biology to six essential hallmarks, six ways in which

942
01:17:58,840 --> 01:18:02,840
we might make things better. And so I guess I'll take you through the six. I guess the first of

943
01:18:02,840 --> 01:18:08,680
the six was independence. What do I mean by this? So I guess one of the things in the book that I

944
01:18:08,680 --> 01:18:15,160
spend a lot of time talking about are conflicts of interest, ways in which the industry gets people

945
01:18:15,160 --> 01:18:19,480
who should be sort of competitors with the industry or should be stakeholders that push

946
01:18:19,480 --> 01:18:23,800
back on the industry, it gets them to buy into the industry's narrative in part because they fund

947
01:18:23,800 --> 01:18:28,040
those stakeholders. So talk about patient advocacy groups are often heavily funded by the industry,

948
01:18:28,040 --> 01:18:33,560
how FDA employees who you think should be sort of separated from the industry, their most common

949
01:18:33,560 --> 01:18:38,280
place of employment after the FDA is the industry. And so they kind of have a unique role as regulator,

950
01:18:38,280 --> 01:18:44,280
but also future employee. So I call independence is we need sort of some rules in the space to,

951
01:18:44,280 --> 01:18:49,320
I think, minimize conflict of interest and to allow entities to be free to advocate for their

952
01:18:49,320 --> 01:18:54,680
constituents. Probably the biggest offender is expert oncologists like me. In my case,

953
01:18:54,680 --> 01:18:59,320
unfortunately, I don't receive money from the pharmaceutical industry. However, I think that's

954
01:18:59,320 --> 01:19:04,520
not the case for the majority of expert oncologists, they do. And their views are often very supportive

955
01:19:04,520 --> 01:19:08,440
of these questionable drugs. They're almost at times as if they're cheerleaders. And I believe

956
01:19:08,440 --> 01:19:12,360
that some of that is driven by those sort of financial ties. So I guess the first one is

957
01:19:12,440 --> 01:19:16,920
independence. How does one do anything about that? Maybe we'll probe each one a little bit.

958
01:19:16,920 --> 01:19:17,080
Okay, let's probably.

959
01:19:17,080 --> 01:19:22,200
As opposed to just going through them. So completely get your point there. Look, I mean,

960
01:19:22,200 --> 01:19:26,200
when I was in residency, drug reps brought us lunches.

961
01:19:26,200 --> 01:19:26,840
Oh, yeah.

962
01:19:26,840 --> 01:19:33,000
Right. And gave us pens. Like, you know, something as silly as that, but that's a hook.

963
01:19:33,800 --> 01:19:34,920
Yeah, it's a little bit of a hook.

964
01:19:34,920 --> 01:19:41,080
It's a little bit of a hook. It's who has the coolest pen? Because these weren't just generic

965
01:19:41,080 --> 01:19:44,840
pens. These were like really cool pens. Like you wanted to be there at that lunch and get that

966
01:19:44,840 --> 01:19:49,480
really cool pen. It was like, I remember some of them to this day, like a pen that looked like a

967
01:19:49,480 --> 01:19:54,120
needle syringe. And when you clicked it, that color changed and all these other cool things. And

968
01:19:55,000 --> 01:19:59,320
look, all they wanted was five minutes of your time. And I remember by the end, residency,

969
01:19:59,320 --> 01:20:02,840
some of those, they were starting to take us, the senior residents out for dinners.

970
01:20:03,480 --> 01:20:09,720
And I got to be honest with you. I don't recall giving it any thought. Like I don't recall

971
01:20:10,360 --> 01:20:17,560
actually feeling like I was doing something wrong. I remember being kind of annoyed that I had to

972
01:20:17,560 --> 01:20:20,920
listen to them talk. Because at that point in my life, I feel like the only thing that mattered

973
01:20:20,920 --> 01:20:24,120
was getting a meal. And I was sort of like, hey, why don't you just shut up and let me eat?

974
01:20:24,680 --> 01:20:25,960
Let us enjoy. Yeah.

975
01:20:25,960 --> 01:20:32,840
But I honestly don't think it crossed my mind, Vinay, that this is the beginning of a very slippery

976
01:20:32,840 --> 01:20:39,000
slope. They're not buying me lunch, giving me pens and eventually buying me dinner because

977
01:20:39,000 --> 01:20:44,680
they think I'm a nice guy. I find that a little bit disturbing that I was too stupid to see

978
01:20:44,680 --> 01:20:49,800
through that. I guess I'll give myself some grace and say the sleep deprivation may have played a

979
01:20:49,800 --> 01:20:55,240
role in it. But have things changed in the last 20 years? I assume that pharma is not allowed in a

980
01:20:55,240 --> 01:20:59,800
hospital anymore. Well, in many, but not every place. But I guess I'd say a couple of things

981
01:20:59,800 --> 01:21:03,880
about that. One, I guess I don't blame you so much. You're a resident at the time. And to be honest,

982
01:21:03,880 --> 01:21:08,040
I actually don't know what you described, although it is problematic. Those pens are collector's

983
01:21:08,040 --> 01:21:11,560
items. So if you have any good ones, send them my way. No, just kidding. They really were

984
01:21:11,560 --> 01:21:15,640
spectacular. They really were. Many people did collect them. And the next thing I would say is

985
01:21:15,640 --> 01:21:20,120
that there is evidence that shows even so much as a meal that is paid for by the industry is

986
01:21:20,120 --> 01:21:24,440
associated with a statistically significant but very small increase in prescribing patterns. So

987
01:21:24,440 --> 01:21:29,240
it is a strategy that pays dividends. I guess I would say that I see the problems with that. And

988
01:21:29,240 --> 01:21:32,920
I think that they are problematic and they have largely been curtailed through a number of sort

989
01:21:32,920 --> 01:21:37,560
of well-intentioned efforts. But one of the things I talk a little bit about in the book

990
01:21:37,560 --> 01:21:42,440
is that we haven't yet curtailed things on the high end. So here we spend a lot of time trying

991
01:21:42,440 --> 01:21:46,760
to reform on the low end, the people getting the pens and the meals. There are many senior

992
01:21:46,760 --> 01:21:51,640
oncologists who receive over $100,000 a year in consulting payments from companies, even more.

993
01:21:51,640 --> 01:21:56,200
They receive millions of dollars in research funding. We have shown in some publications that

994
01:21:56,200 --> 01:22:01,400
even controlling for research funding, controlling for prior publications, controlling for seniority,

995
01:22:01,400 --> 01:22:05,320
personal payments from the industry are associated with greater publication in the future.

996
01:22:05,320 --> 01:22:08,760
It's likely it's a positive feedback loop. Working with the industry helps your career,

997
01:22:08,760 --> 01:22:12,600
which helps you work with the industry, which helps your career. The people in those roles,

998
01:22:12,600 --> 01:22:17,720
the most conflicted people, people who are earning as much from the industry in their side hustle as

999
01:22:17,720 --> 01:22:22,920
the average household income in America, those people, they write the guidelines. And the guidelines

1000
01:22:23,480 --> 01:22:29,160
by law in the US make Medicare pay for drugs for off-label purposes. So the people who write the

1001
01:22:29,160 --> 01:22:34,840
guideline that mandates Medicare must pay for this drug, no price negotiation, that person is

1002
01:22:34,840 --> 01:22:39,880
being paid by the industry, even for the exact same drug. And so those relationships, I think,

1003
01:22:39,880 --> 01:22:44,680
are an order of magnitude more concerning than the resident, hungry resident taking a meal.

1004
01:22:44,680 --> 01:22:48,120
In fact, I try not to talk too much about those sort of little things, but I think you're right

1005
01:22:48,120 --> 01:22:52,280
that it's a slippery slope and part of it is to get you accustomed to the fact that this is no

1006
01:22:52,280 --> 01:22:56,840
big deal. So I think you're absolutely right. I guess I would say the way I would sort of structure

1007
01:22:56,840 --> 01:23:00,440
the solution is the way I sort of structure lots of solutions, which is I don't want to be the

1008
01:23:00,440 --> 01:23:04,760
person setting rules and hard rules with punishments because I don't think that that's

1009
01:23:04,760 --> 01:23:08,840
really what gets people to change behavior. I want to create a different set of incentives that get

1010
01:23:08,840 --> 01:23:13,880
people to maybe do something differently. And so some incentives, I think, are we still need

1011
01:23:13,880 --> 01:23:19,240
guidelines to help decide what to cover off-label. Why don't we incentivize people who don't have

1012
01:23:19,240 --> 01:23:24,520
conflict to join those guidelines? And what if we had rules and policies that favored faculty members

1013
01:23:24,520 --> 01:23:29,080
who didn't take money from the industry to be on the guidelines? Suddenly, career incentives look

1014
01:23:29,080 --> 01:23:33,480
very different. I mean, right now, if I'm a junior faculty member and Eli Lilly offers me sort of an

1015
01:23:33,480 --> 01:23:38,600
ad board consulting opportunity, I might jump at it and I can still have open the opportunity of

1016
01:23:38,600 --> 01:23:42,760
sitting on a guidelines committee. But maybe in exchange for one opportunity, I should lose the

1017
01:23:42,760 --> 01:23:46,600
other opportunity. And if I want to keep the other opportunity, it's my choice. And the more you kind

1018
01:23:46,600 --> 01:23:50,680
of build in those kind of structural incentives, I think people will do things that preserve different

1019
01:23:50,680 --> 01:23:55,480
opportunities. So I think that's sort of one way you tackle the problem of conflict of interest,

1020
01:23:55,480 --> 01:24:00,840
which is that you just sort of tinker with incentives. You create opportunities for academics

1021
01:24:00,840 --> 01:24:05,800
to not take money and still have robust and rich careers. I think in the current system,

1022
01:24:05,800 --> 01:24:09,800
all the opportunities are tied to taking money from the industry. And so I guess how can I blame

1023
01:24:09,800 --> 01:24:13,480
anybody, just like I don't blame the industry for putting a thumb on the scale, I guess I don't

1024
01:24:13,480 --> 01:24:19,720
really blame the faculty members for doing it. Is there evidence that physicians are, for the

1025
01:24:19,720 --> 01:24:24,600
most part, honestly disclosing these things? I mean, I know there was a very famous case at

1026
01:24:24,600 --> 01:24:30,040
Memorial Sloan Kettering where there was a huge blow up over this. So if a system like the one you

1027
01:24:30,040 --> 01:24:35,880
described were in place, do we believe that we could believe people? Yeah, it's a great point.

1028
01:24:35,880 --> 01:24:39,400
I mean, I guess I would say that most of the available evidence suggests that disclosure is

1029
01:24:39,400 --> 01:24:43,640
incomplete and often inaccurate. Some of the available evidence suggests that people don't

1030
01:24:43,640 --> 01:24:47,240
even believe that they have the conflict. And so some of it is an honest error that they don't even

1031
01:24:47,240 --> 01:24:51,000
see that they're being conflicted. They may have forgotten. You cite a really sort of high profile

1032
01:24:51,000 --> 01:24:56,680
example of Jose Baselga, who had found to omit conflicts in dozens and dozens of articles and

1033
01:24:56,680 --> 01:25:01,080
conflicts that were really pertinent to what he was talking about. And he ultimately was pushed

1034
01:25:01,080 --> 01:25:05,320
out of his role at Memorial Sloan Kettering, which is a form of punishment. But very shortly,

1035
01:25:05,320 --> 01:25:09,800
he became vice president of AstraZeneca, a job that probably pays him several times as much

1036
01:25:09,800 --> 01:25:14,600
money as he was making earlier. So I hope someday somebody punishes me in the same way and pushes

1037
01:25:14,600 --> 01:25:18,200
me out of my job, but then pays me a whole lot more money to do something in the same space. I want to

1038
01:25:18,200 --> 01:25:22,920
be punished like that. But I mean, I think that speaks to the fact that we don't really take

1039
01:25:22,920 --> 01:25:26,920
disclosure seriously. I guess the other thing I would say about disclosure is disclosure has been

1040
01:25:26,920 --> 01:25:30,760
one of the methods that we have confronted conflict of interest with, but I'm not exactly

1041
01:25:30,760 --> 01:25:34,280
sure it makes the world better. There's some psychology evidence, some business evidence that

1042
01:25:34,280 --> 01:25:40,520
suggests that once people disclose, there may be actually greater trust placed in the disclosure.

1043
01:25:40,520 --> 01:25:45,880
And it doesn't actually correct, I think, the bias. It just leads the patient to have more trust in

1044
01:25:45,880 --> 01:25:50,920
their doctor, having seen the doctor disclosed. So I think maybe divestment is a better way,

1045
01:25:50,920 --> 01:25:55,720
maybe sort of different rules and incentives for people is a better way to kind of separate these

1046
01:25:55,720 --> 01:26:00,200
things. I think that these conflicts are problematic in one respect that I'll kind of flesh out a

1047
01:26:00,200 --> 01:26:04,920
little bit. So in cancer medicine, so often you've got a drug that costs $100,000 a year and it

1048
01:26:04,920 --> 01:26:09,800
changes something like how big the tumor is on a cat skin with all the problems of measurement.

1049
01:26:09,800 --> 01:26:13,880
We don't know if you live longer, we don't know if you live better, and you got to take the drug.

1050
01:26:13,880 --> 01:26:18,440
Who decides whether or not we should recommend that routinely or not? That guideline decision is

1051
01:26:18,440 --> 01:26:23,000
of huge importance. It will shape how many, many people practice. And I want people to make that

1052
01:26:23,000 --> 01:26:27,080
decision who are oncologists, who have experience, who may know a lot about how to read studies and

1053
01:26:27,080 --> 01:26:31,480
interpret data, but who don't have any financial relationships with the company that makes that

1054
01:26:31,480 --> 01:26:36,520
product. I think that leads to sort of a cleaner decision-making process. And it's the same kind of

1055
01:26:36,520 --> 01:26:40,680
way we set up a courtroom. Can you imagine a courtroom where the prosecutor and the defendant

1056
01:26:40,680 --> 01:26:44,120
are both being paid by the defendant? I mean, I think many of us would say that's not really a

1057
01:26:44,120 --> 01:26:48,840
balanced courtroom. And yet in medicine, we do have many imbalanced courtrooms where almost

1058
01:26:48,840 --> 01:26:53,800
everybody in the courtroom is getting money from the company. And so I guess I'm just suggesting

1059
01:26:53,800 --> 01:26:57,800
that we don't take away the industry's incentive. We keep the profit. Of course, I think it does

1060
01:26:57,800 --> 01:27:03,320
drive innovation, but we remove it from the people that's supposed to provide opposition force and

1061
01:27:03,320 --> 01:27:09,320
try to preserve a healthy sort of dynamic there. What is the opposition to your idea, which is so

1062
01:27:09,320 --> 01:27:14,200
obvious, it's painful to listen to you articulate it? The opposition is probably principally that

1063
01:27:14,200 --> 01:27:20,040
the people who are best in the position to fix the situation are the ones who are personally getting

1064
01:27:20,040 --> 01:27:25,560
the most money. I mean, I guess I'd say that our professional societies are driven by the industry

1065
01:27:25,560 --> 01:27:30,040
payments, the professional societies, senior leadership at universities and academic medical

1066
01:27:30,040 --> 01:27:33,560
centers are often on the boards of directors of companies. They're the ones getting the biggest

1067
01:27:33,560 --> 01:27:38,120
payments. The most senior and famous oncologists are the ones getting the most money. And so I

1068
01:27:38,120 --> 01:27:42,760
think it's hard in any system where the people you need on your side to change the system, the

1069
01:27:42,760 --> 01:27:47,240
people with the power are the ones benefiting most from the system. Doctors are smart people. We can

1070
01:27:47,240 --> 01:27:51,240
all come up with reasons why what we're doing is not bad. And in fact, probably, I believe this

1071
01:27:51,240 --> 01:27:54,840
might be true for everybody. We all believe we're all ethical actors. I mean, I don't think anyone

1072
01:27:54,840 --> 01:27:58,920
thinks that they are an unethical person. They just feel that they're a product of their circumstances.

1073
01:27:58,920 --> 01:28:03,320
And so I think the real way to fix this problem is somebody external to the medical profession has

1074
01:28:03,320 --> 01:28:06,920
to come in and do it. I don't think it's ever going to happen from self policing because the

1075
01:28:07,000 --> 01:28:10,760
people who are in power are the ones who are getting the most payments. And very likely the

1076
01:28:10,760 --> 01:28:13,160
case that there's a reason why they're the ones getting the most payments is because

1077
01:28:13,720 --> 01:28:16,360
the industry knows that that's what it takes to keep the system going.

1078
01:28:17,000 --> 01:28:22,920
All right. So the second hallmark, second hallmark evidence. Yeah. What does that mean?

1079
01:28:22,920 --> 01:28:27,240
Evidence is I describe it as measure what matters and do it fairly. There are a lot of technical

1080
01:28:27,240 --> 01:28:31,880
things that I won't get into for this podcast that I talk about in the book. But I guess basically,

1081
01:28:31,880 --> 01:28:36,200
evidence is the following that when we give cancer drugs, we care about two things, people living

1082
01:28:36,200 --> 01:28:40,680
longer or living better, increased survival or health related quality of life. We have a system

1083
01:28:40,680 --> 01:28:45,800
where two thirds of cancer drugs that are being approved. One third, I know they shrink tumors

1084
01:28:45,800 --> 01:28:52,600
more than 30% in a fraction of people. The other third, I know they delay tumor growth by 20%. And

1085
01:28:52,600 --> 01:28:57,880
then the other third, I know you live longer, live better. That to me is a strange reorientation of

1086
01:28:57,880 --> 01:29:02,120
you're spending so much money on costly drugs. Only one third do you measure what actually matters to

1087
01:29:02,120 --> 01:29:06,600
people. The other two thirds, you're measuring tumor size on CAT scans. And I'll say one more

1088
01:29:06,600 --> 01:29:11,720
thing that I think is interesting here, which is we use this cutoff of tumor shrinkage of 30%.

1089
01:29:12,440 --> 01:29:15,800
In writing this book, I spent a lot of time trying to get the bottom of why it is this 30%.

1090
01:29:16,520 --> 01:29:22,440
And I found out it goes back to a 1976 paper where this Mayo Clinic doctor got a bunch of marbles and

1091
01:29:22,440 --> 01:29:26,760
he put them on a dining table and he rolled out foam rubber. And he got 16 oncologists to come

1092
01:29:26,760 --> 01:29:31,960
to his house with calipers and measure the marbles. And he asked, at what size difference

1093
01:29:31,960 --> 01:29:36,680
can two doctors reliably tell the marble has gotten bigger or gotten smaller? And the answer

1094
01:29:36,680 --> 01:29:42,440
was certain cut point. And that cut point is the same cut point we use today. We use a cut point

1095
01:29:42,440 --> 01:29:49,000
to measure tumor shrinkage as a response or not response, because a bunch of men in 1976 measuring

1096
01:29:49,000 --> 01:29:53,720
marbles through foam rubber, which was how we measured tumors in the day before imaging,

1097
01:29:53,720 --> 01:29:58,440
that's what they could tell apart. So these cutoffs that we have sort of confused as measures

1098
01:29:58,440 --> 01:30:03,720
of efficacy, we're really just sort of operational measures to sort of get some interrater reliability

1099
01:30:03,720 --> 01:30:07,880
get us to agree. And so the moment you start to know that you realize like, why am I putting so

1100
01:30:07,880 --> 01:30:12,280
much stock in drugs with a 10% 20% response rate, I really don't know if the patient lives longer,

1101
01:30:12,280 --> 01:30:17,000
lives better. And this cutoff is really arbitrary. So that's what I mean by evidence. Let's measure

1102
01:30:17,000 --> 01:30:22,200
more, not always, but more what matters living longer, living better. And what would be the

1103
01:30:22,200 --> 01:30:27,800
implications of that? I mean, it basically would imply that two thirds of the treatments out there

1104
01:30:27,800 --> 01:30:33,080
go in the waste bin. At least they won't be approved at the same moment in time. So what

1105
01:30:33,080 --> 01:30:38,440
might happen is that if you really created a policy where generally the industry is obliged to show

1106
01:30:38,440 --> 01:30:42,920
survival or quality of life benefit, a lot of things are going to change in the industry. So one

1107
01:30:42,920 --> 01:30:48,360
thing is, they're going to run more trials in patients who have relapsed cancer than trials in

1108
01:30:48,360 --> 01:30:51,880
patients with low risk early stage cancers. They're going to go into the last line setting,

1109
01:30:51,880 --> 01:30:55,560
because that's where the event rate is highest. They're going to be more selective. I think about

1110
01:30:55,560 --> 01:31:00,840
drugs that they deploy, they're not going to deploy drugs that may change tumor scans, they're really

1111
01:31:00,840 --> 01:31:03,800
going to try to think about drugs that improve survival or quality of life. So they might be more

1112
01:31:03,800 --> 01:31:07,880
picky in which drugs they advance in the process. I think it changes a number of things in the drug

1113
01:31:07,880 --> 01:31:11,640
development pipeline. And I guess I kind of talk about that in the book. But to your point, which

1114
01:31:11,640 --> 01:31:15,400
is, does it actually change the number of drugs that come to market? I guess I would say that of

1115
01:31:15,400 --> 01:31:20,040
those two thirds that just change tumors on scans, maybe some of them actually do help you live

1116
01:31:20,040 --> 01:31:23,560
longer and live better. Those are still going to come to the market. But the ones that don't are

1117
01:31:23,560 --> 01:31:27,080
going to fall short, they're not going to come to the market. So we'll have probably fewer drugs

1118
01:31:27,080 --> 01:31:31,320
come to market. But the drugs we have will probably be better drugs. My friend who posed that

1119
01:31:31,320 --> 01:31:36,600
dilemma to me many years ago, he'll probably be more satisfied. All right, what's the third hallmark?

1120
01:31:36,600 --> 01:31:39,800
Yeah, the third hallmark is something that we talked a lot about on this podcast is called

1121
01:31:39,800 --> 01:31:43,800
relevance. And it basically means that we should do more studies and people that look like average

1122
01:31:43,800 --> 01:31:47,880
Americans. And I think we kind of talked about how we're studying populations that don't reflect

1123
01:31:47,880 --> 01:31:52,200
what average people have and experience. Why don't we just study average people and let's find out if

1124
01:31:52,200 --> 01:31:56,920
the drugs work as we prescribe them. And again, that'll have the same effect, which is it's

1125
01:31:56,920 --> 01:32:03,800
effectively going to reduce the success rates, probably due to both decreased tolerability and

1126
01:32:03,800 --> 01:32:09,400
actual poor response. Yes, I think that that's the case. Yeah. Okay, the next one is near and dear

1127
01:32:09,400 --> 01:32:16,280
to my heart. Affordability. Tell us about that. How did we fix that issue? That's a thorny problem.

1128
01:32:16,280 --> 01:32:20,440
We've written a number of review articles that have looked at so many different solutions. I mean,

1129
01:32:20,440 --> 01:32:24,360
I can't even get into all the solutions just to put them in a few buckets. They're solutions that

1130
01:32:24,360 --> 01:32:31,160
rely on existing legal structures to make progress here. They're solutions that require novel legal

1131
01:32:31,160 --> 01:32:37,000
structures. One solution, I think maybe one of the clearest solutions here is that in the US, we have

1132
01:32:37,000 --> 01:32:42,040
a system that any time a cancer drug is approved, Medicare, which is perhaps the single largest

1133
01:32:42,040 --> 01:32:47,080
payer in this country, Medicare has to pay for the drug and they can't negotiate the price. Not only

1134
01:32:47,080 --> 01:32:52,360
that, they have to pay for any drug recommended at a level 2A or higher by a number of guidelines,

1135
01:32:52,360 --> 01:32:56,440
like this National Comprehensive Cancer Network, which has got a lot of people writing the guidelines

1136
01:32:56,440 --> 01:32:59,800
who are funded by the industry. So I think one way we could kind of make some headway in the

1137
01:32:59,800 --> 01:33:04,920
affordability is to give Medicare the ability to decline to pay for drugs. Many years ago,

1138
01:33:04,920 --> 01:33:08,600
a couple of states went to SEMA Verma and colleagues at HHS and they said,

1139
01:33:08,600 --> 01:33:13,480
let us at least experiment. States are the laboratory of experiment. Let us say Massachusetts

1140
01:33:13,480 --> 01:33:18,120
doesn't have to cover all these drugs for Medicaid, but they were denied the ability to experiment

1141
01:33:18,120 --> 01:33:21,400
at a state level. I think we should encourage state level experimentation and how to bring

1142
01:33:21,400 --> 01:33:25,640
down drug prices. And there are a number of other things that are really sort of technical solutions.

1143
01:33:25,640 --> 01:33:31,000
But I guess what I think is important here is just the recognition that a drug that you cannot

1144
01:33:31,000 --> 01:33:34,680
actually get in the hands of somebody is no better than no drug at all. And we have

1145
01:33:34,680 --> 01:33:39,960
affordability crisis in this country, but globally, we have a huge affordability crisis. These drugs,

1146
01:33:40,520 --> 01:33:45,080
for the most part, are out of reach of billions and billions of people in this world.

1147
01:33:45,080 --> 01:33:49,400
And so it's one thing to live in 2020 when you have access to the greatest medicines.

1148
01:33:49,400 --> 01:33:55,960
It's really sad, I think, when there are some drugs that are transformative and the best centers

1149
01:33:55,960 --> 01:34:00,200
globally just really do not have access. And in the book, I give an example of Trasetuzumab,

1150
01:34:00,200 --> 01:34:04,120
a drug developed in the late 1990s for a certain type of breast cancer that is really

1151
01:34:04,760 --> 01:34:10,120
a terrific drug. But in a study that came out of India just five years ago, only one in a hundred

1152
01:34:10,120 --> 01:34:14,360
people who could have gotten that drug got that drug. And this is at a referral center in sort of

1153
01:34:14,360 --> 01:34:18,280
one of the biggest cities in the country, at a really sort of premier center. And it's still

1154
01:34:18,280 --> 01:34:21,720
so difficult out of reach of so many people globally. I think that's a real tragedy.

1155
01:34:22,360 --> 01:34:28,120
Do we have a sense of, on average, how much more a drug cost in the United States than,

1156
01:34:28,760 --> 01:34:33,320
say, in Europe or Asia? Yeah, I mean, I guess, I mean, a couple of figures are, of course,

1157
01:34:33,320 --> 01:34:38,040
of the pharmaceutical space, about 50% of spending in the US comes from the US, even though we account

1158
01:34:38,040 --> 01:34:43,960
for maybe 4% of the world's population. And how much of that do we know if it's volume versus price?

1159
01:34:43,960 --> 01:34:48,280
Like, obviously, there's an accessibility issue, which says in the United States, you're going to,

1160
01:34:48,280 --> 01:34:53,080
on a per capita basis, more people are going to receive the drug as well. It just seems like,

1161
01:34:53,080 --> 01:35:00,840
as a casual observer of oncology, that basically the United States kind of has a sort of tacit

1162
01:35:00,840 --> 01:35:08,280
quid pro quo with pharma, which is here's the deal. Do the lion's share of the R&D inside the United

1163
01:35:08,280 --> 01:35:15,000
States? Let us have first access to the drugs. We're going to subsidize the cost for the rest

1164
01:35:15,000 --> 01:35:20,280
of the world. Directionally, that seems to me what's happening. Is that about accurate?

1165
01:35:20,280 --> 01:35:24,600
That is a fair summary of the lay of the land. I guess I say a couple things. I mean, one of the

1166
01:35:24,600 --> 01:35:28,200
points you made earlier was what about the price versus volume? I mean, I think one helpful

1167
01:35:28,200 --> 01:35:33,240
comparison would be like two nations with comparable GDP, US and Norway, and we pay roughly

1168
01:35:33,240 --> 01:35:38,200
double what Norway pays for cancer drugs. The other thing I would say about subsidization is,

1169
01:35:38,200 --> 01:35:43,640
I think people like me who are reformers, I guess the real question I have in my mind is,

1170
01:35:44,200 --> 01:35:48,600
why do we subsidize marginal drugs the same way we subsidize transformative drugs? I think that's

1171
01:35:48,600 --> 01:35:54,440
the kind of crux of the space. You named a couple of drugs Avastin. This is a drug that has multiple

1172
01:35:54,440 --> 01:35:58,440
approvals in many different tumor types. It doesn't cure a single person. Immediate survival about one

1173
01:35:58,440 --> 01:36:03,800
to two months, a tremendous price. Global lifetime earnings of that drug are close to $100 billion.

1174
01:36:03,800 --> 01:36:08,920
I mean, that is a massive global lifetime earnings drug. You get another drug, Gleevec, it doesn't

1175
01:36:08,920 --> 01:36:13,560
cure everybody, but for the cancers, it works and it works rather dramatically. Massive transformational

1176
01:36:13,560 --> 01:36:16,840
benefit. And I don't know off the top of my head, lifetime earnings, but it's still in the several

1177
01:36:16,840 --> 01:36:22,280
billions and billions of dollars. And then you get drugs that are really marginal toxic drugs that

1178
01:36:22,280 --> 01:36:27,320
don't do that much and still can accrue billions of dollars. I guess what I want to say is that

1179
01:36:27,320 --> 01:36:32,440
one of the ways in which I think we can correct the market here is if we just incentivize the

1180
01:36:32,440 --> 01:36:36,760
drugs we want, which are drugs that have bigger benefits, more substantive benefits. And if we

1181
01:36:36,760 --> 01:36:41,880
didn't pay so much for those drugs that really don't add a lot, don't cure people and just prolong

1182
01:36:41,880 --> 01:36:47,160
survival very modestly in very select cohorts. So I guess I'm not opposed to paying for things

1183
01:36:47,160 --> 01:36:51,080
that really do work. I guess I'm opposed to paying for things that we don't know work,

1184
01:36:51,080 --> 01:36:55,000
that have a lot of uncertainty or that we know don't work. I think that's more what my criticism

1185
01:36:55,000 --> 01:37:00,760
is. Yeah. And it's funny, I find myself struggling with this one a lot because on the one hand,

1186
01:37:01,320 --> 01:37:06,280
I really do agree that pharma needs to be incentivized to innovate and the cost of

1187
01:37:06,280 --> 01:37:11,640
innovation is staggering. Now that said, I also think pharma grossly overstates the risk they're

1188
01:37:11,640 --> 01:37:19,080
taking. I mean, they've basically outsourced research to biotech. So if you really want to

1189
01:37:19,080 --> 01:37:25,320
think about it, drug discovery is now a venture capital problem, which means it's a private

1190
01:37:25,320 --> 01:37:33,960
citizen funded risk. So private citizens and pensions basically fund VCs that take the risk

1191
01:37:33,960 --> 01:37:42,440
to take biotech from IND to phase one to phase two. Pharma then says, okay, this is de-risked

1192
01:37:42,440 --> 01:37:47,960
enough. I'll come in and on my balance sheet, I can do phase three. So their success rate has gone

1193
01:37:47,960 --> 01:37:53,480
way up. That seems to be a pretty efficient model, I guess. I think pharma has basically decided we

1194
01:37:53,480 --> 01:37:58,600
don't want to own all of those risks. We don't want to own technical and market risk all the way

1195
01:37:58,600 --> 01:38:04,920
through. But the flip side of that is do we really want CMS and you'll have to forgive my

1196
01:38:04,920 --> 01:38:10,680
skepticism, but I don't know that I trust CMS to be the one negotiating price because what's the

1197
01:38:10,680 --> 01:38:16,040
knockoff effect of that going to be in terms of incentivizing pharma? Even though I agree,

1198
01:38:16,680 --> 01:38:21,160
CMS shouldn't have to pay sticker price. Like I guess this is why I'm so glad I don't do what you

1199
01:38:21,160 --> 01:38:26,280
do because these are some of the hardest decisions one has to think through. If you're trying to do

1200
01:38:26,280 --> 01:38:31,720
this through the lens of what is a policy, what is a logical and reasonable and fair policy around

1201
01:38:31,720 --> 01:38:37,720
the incentives, it seems obscene that CMS doesn't have the ability to negotiate. But at the same

1202
01:38:37,720 --> 01:38:41,960
time, I don't know that I want anybody in the US government making a decision when it comes to

1203
01:38:41,960 --> 01:38:47,480
healthcare because I've stopped trusting these entities. Just using Congress as an example,

1204
01:38:47,480 --> 01:38:54,920
right? I mean, we've got what, 550, 600 people make up the US Congress. Like two of these people

1205
01:38:54,920 --> 01:39:01,160
have a degree in science. Oh, yes, it's tragic. They're just not a group of people I want ever

1206
01:39:01,160 --> 01:39:06,920
making scientific decisions. Same for the Supreme Court. Yeah. Yeah, yeah, exactly. So how do we

1207
01:39:06,920 --> 01:39:13,560
think about balancing that? Is your solution of let the states do it better where there's less

1208
01:39:13,560 --> 01:39:18,840
collateral damage? But I can also think of ways that that goes sideways also. Yeah, I mean, I

1209
01:39:18,840 --> 01:39:22,600
guess I would say that, I mean, one thing to acknowledge is that no one has solved the problem.

1210
01:39:22,680 --> 01:39:27,240
So the problem remains unsolved. I guess the next thing I would say along these lines is that

1211
01:39:28,040 --> 01:39:33,400
having a CMS the way we have a CMS, it is a certain type of incentive. It's a very interesting

1212
01:39:33,400 --> 01:39:38,120
incentive. The incentive is basically we will pay for any drug that gets FDA approval, whatever you

1213
01:39:38,120 --> 01:39:42,920
charge. So companies will always keep cranking it up as much as they can. And some of the things

1214
01:39:42,920 --> 01:39:46,600
about how they crank it up, they'll crank it up in lockstep. So they'll all move together upward,

1215
01:39:46,600 --> 01:39:49,800
they won't go too much, nobody will be an outlier because then you're going to be on 60 minutes,

1216
01:39:49,800 --> 01:39:54,200
and that's no good for your brand. So you want to go up together slowly, but you can keep cranking

1217
01:39:54,200 --> 01:39:57,880
it up. You can do a 9% year over year over year over year and when the frog doesn't jump out of

1218
01:39:57,880 --> 01:40:02,760
the pot when the water is boiled slowly. So that's one, two, then the next thing they build into CMS

1219
01:40:02,760 --> 01:40:07,480
is once the drug is approved for one use, we're going to let CMS pay for the drug for any other

1220
01:40:07,480 --> 01:40:12,360
use if these expert doctors believe it should be used in that way. And we're going to detail those

1221
01:40:12,360 --> 01:40:14,920
expert doctors and give them a lot of money and make them come on our side and they're going to

1222
01:40:14,920 --> 01:40:18,120
see things from our point of view. So we're going to get all that market share that way too. And

1223
01:40:18,120 --> 01:40:21,960
sometimes that market share can be even more lucrative than the initial approval market share.

1224
01:40:21,960 --> 01:40:26,040
So I guess what I think this creates in the system is an incentive, a powerful incentive that

1225
01:40:26,600 --> 01:40:31,000
the hurdle for a drug is drug approval. You can charge whatever you want. You really don't have to

1226
01:40:31,000 --> 01:40:34,360
look too deep into like what it costs you to make the drug or what the benefit of the drug is. You

1227
01:40:34,360 --> 01:40:37,880
can charge what you want as long as you're not too much of an outlier, you're not going to stick out

1228
01:40:37,880 --> 01:40:42,920
too much. We had that drug, the CAR T cells, they cost $300,000, $400,000, but then they'll rationalize

1229
01:40:42,920 --> 01:40:46,360
and say, well, it's a one-time dose. It's not every month after month and those other drugs,

1230
01:40:46,360 --> 01:40:49,880
they cost $200,000 a year anyway. Well, of course, that's how you got us to where we are.

1231
01:40:50,440 --> 01:40:55,240
So I guess what I would say about CMS negotiating is I guess I think I want to say that not

1232
01:40:55,240 --> 01:41:01,160
negotiating is a decision of sorts. It's a decision to just really put a lot of incentive here

1233
01:41:01,160 --> 01:41:05,880
without any sort of care as to what you're incentivizing. You're incentivizing an approval.

1234
01:41:05,880 --> 01:41:09,800
The next piece of that puzzle is, well, how are they approving drugs? And I think many of us who

1235
01:41:09,800 --> 01:41:14,680
look at the FDA approval process will see that the bar for approval is getting lower and lower.

1236
01:41:14,680 --> 01:41:18,440
Used to be able to trip on it. Now I think you can probably go over right over it,

1237
01:41:18,440 --> 01:41:21,880
but it's getting lower and lower. I mean, the number of patients for approval not having a

1238
01:41:21,880 --> 01:41:27,160
control arm. There's a drug which I always forget the name. It's a drug that lowers the rate of a

1239
01:41:27,160 --> 01:41:31,800
blood protein, but almost everybody who takes the drug suffers ocular impairment and some people

1240
01:41:31,800 --> 01:41:36,280
suffer severe ocular impairment. So there's a lot of blindness going on or pre-blindness like

1241
01:41:36,280 --> 01:41:39,480
conditions, but I don't know if people live longer. I don't know if they live better. I know they have

1242
01:41:39,480 --> 01:41:45,320
to see the ophthalmologist a lot that adds more healthcare costs. Costs a ton. Why is this approved?

1243
01:41:45,320 --> 01:41:49,160
I mean, why can't we wait for a little bit better data? So I think that's part of the puzzle. I think

1244
01:41:49,160 --> 01:41:55,240
that among the many ideas out there, one idea is sort of the value-based pricing model, which a

1245
01:41:55,240 --> 01:41:59,000
number of sort of commenters have kind of developed, which is this idea that we should pay more for

1246
01:41:59,000 --> 01:42:03,720
drugs that provide us more value, that have more incremental survival benefits than drugs that are

1247
01:42:03,720 --> 01:42:09,160
more modest or mediocre. And I think if there's some way in the system to build in greater

1248
01:42:09,160 --> 01:42:13,080
incentive for the drugs we want, well then that would be good. It would have a lot of good

1249
01:42:13,080 --> 01:42:18,520
secondary effects. But I agree with you that all policy, it's so hard because there's always

1250
01:42:18,520 --> 01:42:23,560
unintended consequences and people will always sort of find paths of least resistance you couldn't

1251
01:42:23,560 --> 01:42:30,040
foresee. The reason I do like state-level expansions, state-level initiatives, is that to some degree,

1252
01:42:30,040 --> 01:42:35,160
good policy is experimentation. You experiment, you see what happens, you course correct. You run

1253
01:42:35,160 --> 01:42:38,680
experiments in parallel, you see what works, you adopt that broadly. I think you do need

1254
01:42:38,680 --> 01:42:42,680
experimentation in the policy space. And in medicine, we've suffered because we have not

1255
01:42:42,680 --> 01:42:47,880
done enough of it in the long run. Now, if CMS makes up, I don't know, I have no idea what it

1256
01:42:47,880 --> 01:42:54,040
is today, but let's just say it's 25 to 35% of the payer in the world in the United States, right? So

1257
01:42:54,040 --> 01:43:02,360
25 to 35% of all insurance is CMS. What about the private payers? Why have they not banded together

1258
01:43:02,360 --> 01:43:06,360
and say, well, there's no law that's preventing us from negotiating with these clowns. We're going

1259
01:43:06,360 --> 01:43:10,840
to do it. Now, I know that part of it, of course, is that the majority of their business is probably

1260
01:43:10,840 --> 01:43:15,320
ASO. So they're not on the hook for risk. They don't care. I mean, they're basically administering

1261
01:43:15,320 --> 01:43:22,120
a service and it's the employer that's doing so. Is that why? Is it just, is there's no one big

1262
01:43:22,120 --> 01:43:27,720
enough to negotiate? Clearly, CMS is bigger than any one entity. Yeah, I think there's probably

1263
01:43:27,720 --> 01:43:33,880
a couple of reasons why we haven't seen more activity from private payers. One is that if

1264
01:43:33,960 --> 01:43:38,760
they don't cover what CMS covers, I think they will look bad and they can easily be on the nightly

1265
01:43:38,760 --> 01:43:43,160
news where you'll find a cancer patient who's angry that they didn't get some drug or the other,

1266
01:43:43,160 --> 01:43:46,840
and that's not good for your brand. That's not good for your reputation. And there have been some

1267
01:43:46,840 --> 01:43:51,240
high profile situations in the last 30 years where people went on the news and I think insurers,

1268
01:43:51,240 --> 01:43:55,640
they may not want to take that risk. And the next thing is some of these drugs may cost a lot,

1269
01:43:55,640 --> 01:43:58,840
but the budgetary impact might be a little bit lower to the insurer because it's a small

1270
01:43:58,840 --> 01:44:02,200
population. So in those cases, it might be cheaper for them just to pay for the drug than it is to

1271
01:44:02,200 --> 01:44:08,120
deal with the pushback from not paying for it. I think the worst thing is that the insurers

1272
01:44:08,760 --> 01:44:12,520
may have a different incentive altogether. What do I mean? One of the provisions of the Affordable

1273
01:44:12,520 --> 01:44:17,880
Care Act was to cap the amount of profit that could come off revenue through an insurance

1274
01:44:17,880 --> 01:44:23,560
company, 20% profit on revenue, the medical loss ratio, the MLR. The moment that that is inserted

1275
01:44:23,560 --> 01:44:30,440
into a system, it's a really unique incentive. The insurance industry, now they're being told that

1276
01:44:30,440 --> 01:44:34,520
no matter what, you can never earn more than 20% profit on revenue. That's the most profit you can

1277
01:44:34,520 --> 01:44:40,360
earn. That's a law. So if I tell you you're really hungry and you can only eat 20% of the pizza,

1278
01:44:40,360 --> 01:44:44,440
what size pizza should I order? The answer is going to be extra large. So what I think has

1279
01:44:44,440 --> 01:44:49,320
happened in the insurance industry is that although we hear a lot about insurers who

1280
01:44:49,320 --> 01:44:53,800
have preauthorization requests and all these things that are a pain in the ass for doctors

1281
01:44:53,800 --> 01:44:57,880
and doctors hate, although we hear a lot about insurers pushing back on this or that,

1282
01:44:57,880 --> 01:45:02,200
I think they do that for a couple of reasons. One, that the insurer's incentive is to make sure

1283
01:45:02,200 --> 01:45:07,000
year-to-year variability of costs is predictable and that they can model that out and make sure

1284
01:45:07,000 --> 01:45:11,400
their premiums go where they need them to go to ensure their profit revenues. They're really

1285
01:45:11,400 --> 01:45:15,320
nervous about year-to-year variability and that's why hep C drugs come along and they can blow the

1286
01:45:15,320 --> 01:45:20,600
whole model because the population is massive. But I think the insurers in the long run keeping

1287
01:45:20,600 --> 01:45:24,760
costs down, I think that they may not have enough skin in the game. They don't have enough skin in

1288
01:45:24,760 --> 01:45:29,960
the game to keep costs down. So they really don't care. And so I think the narrative is of course

1289
01:45:29,960 --> 01:45:33,240
that insurers are the downward force, but I don't think they're the downward force that we think

1290
01:45:33,240 --> 01:45:39,960
they are. I agree with you and I think it's the employer that has the most skin in the game,

1291
01:45:39,960 --> 01:45:47,000
but the problem is they're so disaggregated and so spread out that they can't speak in a unified

1292
01:45:47,000 --> 01:45:53,560
way and they can't fight in a unified way. But if you were to look at the sort of my, this is a loose

1293
01:45:53,560 --> 01:45:59,640
way to think about it, but let's just call it a handful of buckets of insured, Medicare, Medicaid,

1294
01:46:00,280 --> 01:46:07,560
private insurance that is administered only through the insurance company. So the ASO employer-based

1295
01:46:07,560 --> 01:46:14,200
versus the insurer-based insurer, private insurer, I think the biggest one has to be the employer.

1296
01:46:15,000 --> 01:46:21,560
And even if it's not, it's the one that would have the most sway. But yeah, it's like how does this

1297
01:46:21,560 --> 01:46:27,800
company of 300 people band together with this company of a thousand people and that company,

1298
01:46:27,800 --> 01:46:32,680
which again, I think speaks to the pain of all of this. One last thought on this space is there's

1299
01:46:32,680 --> 01:46:37,240
also a tragedy to the commons problem. Many years ago when autologous stem cell transplant for breast

1300
01:46:37,240 --> 01:46:41,160
cancer gained popularity, there were a couple of insurers that did pay for some of the clinical

1301
01:46:41,160 --> 01:46:45,560
trials that ultimately debunked that procedure. But now one of the risks is, let's say there's

1302
01:46:45,560 --> 01:46:48,760
some new drug out there and a lot of people say like, this is not, doesn't work so well,

1303
01:46:48,760 --> 01:46:52,840
this trial is really contrived and insurer could come along and they could do the right study.

1304
01:46:52,840 --> 01:46:56,840
They could really test how it works in a different population. They could fund such a study. The

1305
01:46:56,840 --> 01:47:00,200
moment they fund that study, that information is generated while all their competitors will get

1306
01:47:00,200 --> 01:47:04,120
access to that information. So there's a bit of that challenge as well. But yeah, I mean,

1307
01:47:04,120 --> 01:47:08,840
I think you're asking terrific questions about this space. So let's get back to Hallmarks. Number

1308
01:47:08,840 --> 01:47:15,480
five is possibility. What does that mean? I guess I call it the preclinical pipeline must be expanded

1309
01:47:15,560 --> 01:47:19,640
possibility. So I guess I would say we spent a lot of time talking about drug development from the

1310
01:47:19,640 --> 01:47:24,840
point of view of the fledgling biotech to the pharmaceutical industry. One of the things that

1311
01:47:24,840 --> 01:47:28,360
a part of drug development that gets forgotten about a little bit is the role of the NIH,

1312
01:47:28,360 --> 01:47:33,960
of course, in funding basic science. And the NIH, to some degree, does shoulder some risk in this

1313
01:47:33,960 --> 01:47:38,760
space. They fund a lot of sort of science for science sake, identifying novel targets that are

1314
01:47:38,760 --> 01:47:44,040
ultimately some of which are clinically exploited. I guess I would say that although it feels like

1315
01:47:44,040 --> 01:47:49,240
we spend a lot of money on the NIH, $30 billion or so per year, I think it's not nearly enough

1316
01:47:49,240 --> 01:47:53,720
that science is like the greatest thing that people have ever done. And if I were in charge

1317
01:47:53,720 --> 01:47:58,120
of anything, I would crank up the funding for science because I think science is possibility.

1318
01:47:58,120 --> 01:48:02,040
And that's what I mean by this possibility. We need to increase the funding for science.

1319
01:48:02,040 --> 01:48:05,880
We should increase it slowly and steadily. I think there are problems that happen when you

1320
01:48:05,880 --> 01:48:09,960
give a lot more money to people who are not used to getting that. I think there's a lot more waste.

1321
01:48:09,960 --> 01:48:13,960
But if you slowly grow something, we need to kind of separate science funding from political

1322
01:48:13,960 --> 01:48:18,280
cycles. It shouldn't be that just because the red team or the blue team is in power that science

1323
01:48:18,280 --> 01:48:22,520
funding is on the chopping block or getting more. We need some sort of stability for science funding.

1324
01:48:22,520 --> 01:48:27,320
Science needs slow, steady growth. And then the last thing I kind of talk a lot about in this is,

1325
01:48:27,320 --> 01:48:31,160
how do you give out the money? It's fascinating to me. We've been giving out billions of dollars in

1326
01:48:31,160 --> 01:48:35,560
research funding. We've never really studied how you give out the money. If you look at the way the

1327
01:48:35,560 --> 01:48:39,880
money is given, it's really kind of lopsided. There are a few people who get a lot of funding.

1328
01:48:39,880 --> 01:48:44,760
Their labs are flush with money. Some of their labs are so big. One wonders if the boss ever

1329
01:48:44,760 --> 01:48:47,720
meets all the people who work in the lab. There are hundreds of people who work in the lab.

1330
01:48:47,720 --> 01:48:52,360
They're really like financial operations run by a financial manager who's the boss and then some

1331
01:48:52,360 --> 01:48:56,920
scientists underneath it. And there are a lot of people on the other end of the spectrum whose labs

1332
01:48:56,920 --> 01:49:01,160
struggle to get money, even basic funding. And so I guess in this kind of section of the book,

1333
01:49:01,160 --> 01:49:05,640
I kind of explore, are there ways we can kind of bring some experiments to giving out money,

1334
01:49:05,640 --> 01:49:10,440
do some small, simple randomized control trial studies, follow people and look to see measures

1335
01:49:10,440 --> 01:49:15,000
of equity, of who gets the money, measures of research satisfaction, burnout, measures of

1336
01:49:15,000 --> 01:49:20,040
how translation occurs, some kind of controlled studies in grant giving. And then the last thing

1337
01:49:20,040 --> 01:49:24,680
I just say is blue sky science. So there's this branch of science typically called blue sky

1338
01:49:24,680 --> 01:49:29,080
science, where it's just science for science sake. You don't come to me and say, I want to do this

1339
01:49:29,080 --> 01:49:32,280
experiment because I'm going to cure melanoma. You come to me and you say, I just want to know

1340
01:49:32,280 --> 01:49:37,320
how the cell does this. I just want to know why does this happen? And that type of science,

1341
01:49:37,320 --> 01:49:42,840
that type of inquiry that we all have as I think kids and high school kids, that inquiry is really,

1342
01:49:42,840 --> 01:49:47,160
really difficult when you are an academic doctor because there's not a lot of money unless you

1343
01:49:47,160 --> 01:49:51,240
make promises, I'm going to cure this disease, I'm going to cure that. If you want to say,

1344
01:49:51,240 --> 01:49:54,920
I just want to understand how this works, it's really hard to get funding. And I think it should

1345
01:49:55,000 --> 01:49:59,560
be the other way around. Some of the greatest advances in science were people who pursued things

1346
01:49:59,560 --> 01:50:03,480
purely because that interested them and the finding and the translation was serendipitous.

1347
01:50:03,480 --> 01:50:07,160
And so I think that's what I mean by possibility. I couldn't agree with you more on this. I have

1348
01:50:07,160 --> 01:50:11,080
many friends who sit on study sections in NIH, which means they're basically the people that

1349
01:50:11,640 --> 01:50:18,040
watch how NIH gives money. And their biggest complaint is, look, we are really not permitted

1350
01:50:18,040 --> 01:50:24,440
to take big risks here. I mean, we are very incremental in how we fund because we are on

1351
01:50:24,440 --> 01:50:33,000
a funding cycle. They cannot fund pie in the sky, blue sky, really, really fundamental basic

1352
01:50:33,000 --> 01:50:38,840
questions. Because when I go back to Congress in two years and say, I need this much money,

1353
01:50:38,840 --> 01:50:45,480
the answer better be because I did X, Y, and Z with this. And I've seen some of this firsthand.

1354
01:50:45,480 --> 01:50:51,480
And again, I understand it. I mean, we don't want resources to be wasted. So it almost suggests

1355
01:50:51,480 --> 01:50:56,760
you want to have two arms. That would be my take. You want to have two funding arms. You want to

1356
01:50:56,760 --> 01:51:04,760
have the translational funding arm that is meant to be incremental, that is meant to look at basic

1357
01:51:04,760 --> 01:51:11,080
science and ask, is this ready to go to the next step? Is this ready to be taken to a clinical

1358
01:51:11,080 --> 01:51:16,920
pathway? If so, are you leaping too much? Are you not leaping enough? And then I think you have to

1359
01:51:16,920 --> 01:51:23,640
have a totally separate group that is responsible for funding a totally different type of science,

1360
01:51:23,640 --> 01:51:29,080
which is all about basic inquiry and the advancement of natural knowledge, regardless of where it takes

1361
01:51:29,080 --> 01:51:33,320
us. Because as you pointed out in the book, actually, there are lots of examples of some of

1362
01:51:33,320 --> 01:51:39,400
the most exciting discoveries in the history of the last 400 years, which is effectively the era

1363
01:51:39,400 --> 01:51:45,480
of modern science that came from nothing other than pure inquiry. I think that's well said. It's

1364
01:51:45,480 --> 01:51:50,280
a very thoughtful way to look at the problem. One of the people that you cited, Jim Allison, whose

1365
01:51:50,280 --> 01:51:55,960
work ultimately led to a blockbuster class of drugs, an immunotherapy doctor, it wasn't that long

1366
01:51:55,960 --> 01:52:01,080
ago when people, I remember people made jokes that that line of inquiry was foolish and misguided,

1367
01:52:01,080 --> 01:52:05,320
and that was never going to succeed. Lo and behold, it turned out to be sort of a Nobel prize-winning

1368
01:52:05,320 --> 01:52:09,560
discovery. So I think you're right. Yeah, that's the right way to think about it. So the final

1369
01:52:09,560 --> 01:52:14,280
hallmark is agenda. What do you mean by that? Agenda is something that there have been a number

1370
01:52:14,280 --> 01:52:18,440
of researchers, including people who work with me and myself, who have gotten interested in the last

1371
01:52:18,440 --> 01:52:22,920
couple of years, which is what happens when you take a 30,000 foot view of cancer and look at the

1372
01:52:22,920 --> 01:52:29,240
clinical trials agenda. And it's very interesting that some spaces in cancer medicine, you got the

1373
01:52:29,320 --> 01:52:36,120
same drug or similar drugs, 20 such drugs, Coke, Pepsi, and 18 other Cokes and Pepsis. And people

1374
01:52:36,120 --> 01:52:39,960
are running redundant and duplicative trials. They're all testing them in the same tumors,

1375
01:52:39,960 --> 01:52:44,840
in the same setting, with the same old controls. Sometimes they're running many different clinical

1376
01:52:44,840 --> 01:52:50,760
trials with drugs that have low promise. And one of the observations that we make is, well, boy,

1377
01:52:50,760 --> 01:52:54,600
by chance alone, aren't some of these trials going to be positive? I mean, we're not using a very

1378
01:52:54,600 --> 01:52:58,600
stringent nominal cutoff for significance, we're using a P of 0.05 usually, some are going to be

1379
01:52:58,600 --> 01:53:03,000
positive by chance alone. How do you account for that? How do you account for this duplication?

1380
01:53:03,000 --> 01:53:08,680
Who's keeping track? It's like you need a Bonferroni correction factor for the number of trials as

1381
01:53:08,680 --> 01:53:14,280
opposed to the number of looks, right? Absolutely. In fact, we did a paper where we corrected one with

1382
01:53:14,280 --> 01:53:18,360
a Bonferroni. We got a lot of pushback from those peer reviewers. It's a totally novel

1383
01:53:18,360 --> 01:53:22,840
application of how you would use a Bonferroni correction factor. But you instantly see what

1384
01:53:22,840 --> 01:53:28,280
I'm getting at, which is that within a study, if you do a lot of comparisons, you take into account

1385
01:53:28,280 --> 01:53:32,520
the number of comparisons to some degree. We can debate what statistical procedure to use, but we

1386
01:53:32,520 --> 01:53:36,200
do take into account that we're looking at this data many times so we could fool ourselves. But

1387
01:53:36,200 --> 01:53:40,680
when you're running many, many studies, shouldn't you also do the same? It's a philosophical question,

1388
01:53:40,680 --> 01:53:44,360
it's not really a statistical question. And the answer, I believe, is yes. So that's what we talk

1389
01:53:44,360 --> 01:53:49,000
about in this section in agenda. We talk about what does it mean to take that into account. And also,

1390
01:53:49,560 --> 01:53:53,560
we forget sometimes that the most critical resource in cancer medicine are the patients

1391
01:53:53,560 --> 01:53:59,240
themselves, their scarce resource. In some tumor types, there are now more trials ongoing than

1392
01:53:59,240 --> 01:54:03,880
there are even people with that condition, which may sound ridiculous, but it's because everybody

1393
01:54:03,880 --> 01:54:07,960
chases the ball. Somebody recently told me this great story about it's like three-year-olds and

1394
01:54:07,960 --> 01:54:11,160
five-year-olds playing soccer. They all run after the ball, nobody plays positions. And that's what

1395
01:54:11,160 --> 01:54:13,880
happens sometimes with the pharmaceutical industry. They're all running after the ball, they're all

1396
01:54:13,880 --> 01:54:17,240
running duplicative trials. They're not enough patients with this condition anymore. We're

1397
01:54:17,240 --> 01:54:21,880
depleting that resource. And what do we have to show for it? And how do we interpret those studies?

1398
01:54:21,880 --> 01:54:25,880
So I think that's what we talk about, which is that somebody has to think about the big picture,

1399
01:54:25,880 --> 01:54:30,120
or at least we have to look at it and be recognized what the agenda look like across the entire field.

1400
01:54:31,080 --> 01:54:38,920
So coming back to oncology, is there a role anywhere for tumor genome sequencing? There

1401
01:54:38,920 --> 01:54:45,160
are many companies that do this commercially. If there's one email or call I hate getting,

1402
01:54:45,160 --> 01:54:51,240
and unfortunately, I get it every two weeks, it's the friend of the friend or the friend of the

1403
01:54:51,240 --> 01:54:58,200
family, the patient's cousin, whatever, that has just been diagnosed with essentially an

1404
01:54:58,200 --> 01:55:05,560
uncurable cancer. They're basically saying, Peter, what advice do you have? And it's always

1405
01:55:05,560 --> 01:55:11,160
heartbreaking because I don't have any. It's by definition, if I'm getting that call, it's because

1406
01:55:11,160 --> 01:55:17,560
their oncologist has already said there is nothing to do here. These calls go in many different ways.

1407
01:55:17,560 --> 01:55:23,640
I just had one a week ago, they were asking about some supplement. And generally, when I get asked,

1408
01:55:23,640 --> 01:55:29,320
when I go down the path of, well, this person has this hyperbaric oxygen protocol with this supplement

1409
01:55:29,320 --> 01:55:34,040
protocol with this, this thing and the other, the first thing I like to inquire about is the cost.

1410
01:55:34,040 --> 01:55:39,800
My view on this, which may differ from yours is, first, we don't want to increase the harm of this

1411
01:55:39,800 --> 01:55:43,320
person. I don't think there's anything wrong with hope. I think there's a lot of things we don't

1412
01:55:43,320 --> 01:55:50,280
know if they work or don't work. But I'm certainly pretty squirrely when someone's offering a panacea

1413
01:55:50,280 --> 01:55:56,520
treatment for $20,000 of vitamin C, rubbing garlic on your testicles or whatever the concoction is.

1414
01:55:57,320 --> 01:56:02,040
So we go down that whole path, which is, is there anything out there that's in the quote unquote

1415
01:56:02,040 --> 01:56:07,960
holistic world that's going to work? Another thing that tends to come up is, is there a role for tumor

1416
01:56:07,960 --> 01:56:14,520
genome sequencing? And I've certainly had within my own family and that of friends, sent people to

1417
01:56:14,520 --> 01:56:19,240
foundation medicine. Truthfully, I haven't really had anything come out of it that's been a game

1418
01:56:19,240 --> 01:56:24,520
changer. Probably the only time in my life with this experience that I've gotten kind of lucky was

1419
01:56:25,160 --> 01:56:32,520
a friend that had pancreatic cancer. I mean, this was several years ago, but he had Lynch. And I had

1420
01:56:32,520 --> 01:56:39,240
just remembered reading a paper about how patients with Lynch were more likely to have

1421
01:56:39,240 --> 01:56:45,640
checkpoint mutations. This was prior to the approval of chitruta, but it was in trials. And so

1422
01:56:45,640 --> 01:56:51,080
we're able to get him into a trial of chitruta. And he turned out to be a remarkable responder.

1423
01:56:51,080 --> 01:56:56,840
And to this day, he went from unresectable to cancer free, which he remains about a decade

1424
01:56:56,840 --> 01:57:02,760
later or nearly a decade later. Long-winded preamble, talk to me about your view on tumor

1425
01:57:02,760 --> 01:57:08,200
sequencing, off-label drug use, targeting therapies, that sort of thing. Just to allude to

1426
01:57:08,200 --> 01:57:12,200
the other part of what you were saying that I thought was really right, which is, I think we

1427
01:57:12,200 --> 01:57:15,320
probably share the same philosophy, which is that often people come to me and they talk about some

1428
01:57:15,320 --> 01:57:18,840
alternative or complementary approach that they want to bring into their care. And my view is

1429
01:57:18,840 --> 01:57:22,440
probably maybe not dissimilar from yours, which is as long as it's not too costly, as long as it

1430
01:57:22,440 --> 01:57:26,680
doesn't have an opportunity cost to interfere with what we're wanting to do. It's not a hill I want to

1431
01:57:26,760 --> 01:57:30,600
die on because I think people should be allowed to pursue those things. And I do think that one of

1432
01:57:30,600 --> 01:57:34,840
the problems I get a little irritated by is there are people who want to die on that hill and really

1433
01:57:34,840 --> 01:57:38,600
draw a line in the sand. And I think you can poison a relationship with somebody and it's not worth

1434
01:57:38,600 --> 01:57:42,040
it. I mean, at the end of the day, if it doesn't cost too much, it's not really too harmful. And

1435
01:57:42,040 --> 01:57:45,320
somebody really is motivated to do it. And it's not going to interfere with what you want to do.

1436
01:57:45,320 --> 01:57:51,160
Have at it. And just the humility of like, what do we know? We have to be clear here. Like we don't

1437
01:57:51,160 --> 01:57:55,160
know a lot of stuff. Right. And by the way, it's not like we have a track record that says we're

1438
01:57:55,160 --> 01:58:01,080
winners. Right. I mean, I think that all goes into it. And that's also why I think that's all a part

1439
01:58:01,080 --> 01:58:05,480
of it. Yeah, right. And then to come to your NGS question, I guess I would say next generation

1440
01:58:05,480 --> 01:58:09,400
sequencing tumor genomics, I guess right off the bat, there are definitely some people who definitely

1441
01:58:09,400 --> 01:58:12,920
need to be tested for some mutations. So let's just talk about the ones that definitely need to

1442
01:58:12,920 --> 01:58:18,360
happen. You mentioned lung cancer. It's a no brainer. Right. Lung cancer, there are at least

1443
01:58:18,360 --> 01:58:23,240
six or seven mutations that we now have FDA approved therapies for. Definitely. There are

1444
01:58:23,240 --> 01:58:25,880
different ways you could test for the same mutations, but that's another thing to keep in

1445
01:58:25,880 --> 01:58:29,720
mind. But you need to know about EGFR. You need to know about elk, particularly if the person is

1446
01:58:29,720 --> 01:58:34,680
a younger person, non-smoker melanoma, you need to know about BRAF colon cancer, you need to know

1447
01:58:34,680 --> 01:58:38,440
about MSI high status. So there's a number of things we have approved drugs and we published

1448
01:58:38,440 --> 01:58:42,760
some papers, there's maybe 20 or 30 things and a good oncologist usually knows the mutations for

1449
01:58:42,760 --> 01:58:48,680
the tumor. The next thing is now we do have an approval for microsatellite instability high,

1450
01:58:48,680 --> 01:58:51,960
which is probably like your patient with the Lynch syndrome pancreas cancer, no matter what

1451
01:58:51,960 --> 01:58:56,120
tumor type, you know, that's something that we have an approved drug, very reasonable to test for.

1452
01:58:56,120 --> 01:59:00,840
Then I think the next way that NGS can be used is exactly as you used it. I'm a huge supporter of,

1453
01:59:00,840 --> 01:59:05,000
which is that you're using NGS to pair your patient with a clinical trial. I think that's a

1454
01:59:05,000 --> 01:59:10,360
perfectly acceptable way to use NGS to run a broad NGS panel. And if you find a trial that fits,

1455
01:59:10,360 --> 01:59:15,240
have edit. The next part about it, I think is tricky, which is that the part that I think that

1456
01:59:15,240 --> 01:59:19,800
I kind of have the most friction with some of my colleagues is after having done all these things,

1457
01:59:20,440 --> 01:59:24,040
you've looked at mutations, we have approved drugs, you've tried to find somebody trial.

1458
01:59:24,040 --> 01:59:30,120
Sometimes you do NGS on a patient and there is a mutation that is seductive. It looks like you have

1459
01:59:30,120 --> 01:59:35,960
a drug for it. There's no trial available. And you also have a standard drug that we normally give.

1460
01:59:36,760 --> 01:59:42,360
And in these situations, it is so seductive to believe that because we found the mutation and

1461
01:59:42,360 --> 01:59:46,680
the target, it's better to use the targeted drug than that older drug that may have a longer track

1462
01:59:46,680 --> 01:59:52,360
record. And I think that's where people get into a bit of trouble that sometimes you actually end up

1463
01:59:52,920 --> 01:59:57,400
eroding outcomes, not enhancing outcomes. You actually make worse choices in those cases,

1464
01:59:57,400 --> 02:00:03,800
because the truth is that some genomic mutations are mutations that are fueling the tumor. And that

1465
02:00:03,800 --> 02:00:09,160
if you fix those mutations, you would improve the outcomes. But some mutations are the product of a

1466
02:00:09,160 --> 02:00:13,640
genome that is undergoing massive instability and damage. Some of these tumors, when you look at the

1467
02:00:13,640 --> 02:00:17,800
sequence, it's like a dinner plate that's been dropped on the floor. It's in shards, it's broken

1468
02:00:17,800 --> 02:00:21,720
all over the place. You may find that for one of those two shards, you can put a little piece of

1469
02:00:21,720 --> 02:00:25,720
tape on it, but are you really going to help the whole person? And there are some studies that show

1470
02:00:25,720 --> 02:00:29,400
sometimes if you take biopsies from a bunch of different sites and you sequence them, you don't

1471
02:00:29,400 --> 02:00:33,240
even have the same mutations in different sites. And one study shows that if you sequence this part

1472
02:00:33,240 --> 02:00:37,400
of the body, different parts of the tumor, the tumor has spread to, let's say, the lung and the

1473
02:00:37,400 --> 02:00:40,840
liver. If you looked at the liver, you might give drug A. If you looked at the lung, you might give

1474
02:00:40,840 --> 02:00:45,960
drug B. That's not a very precise therapy. And then the other thing that's a complexity is some

1475
02:00:45,960 --> 02:00:49,400
researchers have sent the exact same tumor to a couple of different companies and they haven't

1476
02:00:49,400 --> 02:00:54,200
always gotten the same result. That also makes one a little concerned. So I guess I would say

1477
02:00:54,200 --> 02:00:59,800
approved drugs, this sort of very common mutations, all good doctors are going to test for,

1478
02:00:59,800 --> 02:01:04,600
to pair somebody with a trial as you did your friend, that's really terrific. I think particularly

1479
02:01:04,600 --> 02:01:09,240
people who suffer from rarer tumor types at younger ages, they should also keep an eye out

1480
02:01:09,240 --> 02:01:12,520
for fusion events. Those are really important to know about. They're often, there are a lot of

1481
02:01:12,520 --> 02:01:15,400
researchers who are interested in studying that there's some trials that's lone-kettering.

1482
02:01:15,400 --> 02:01:21,720
The place I think maybe you don't always have to do it is for somebody who is not doing well and

1483
02:01:21,720 --> 02:01:26,760
they have progressed through many lines of therapy and they have a tumor that we just don't have a

1484
02:01:26,760 --> 02:01:30,680
lot of mutations that we know a lot about. I mean, I don't think it's obligatory. And if you do find

1485
02:01:30,680 --> 02:01:34,680
something, I think sometimes you got to be careful that you don't just gravitate to what is a key in

1486
02:01:34,680 --> 02:01:38,760
a lock, what sounds like a key in a lock, but it may be sort of taking you away from something that

1487
02:01:38,760 --> 02:01:44,120
has a better track record. That has to be literally one of the most elegant and brief descriptions of

1488
02:01:44,120 --> 02:01:49,080
the pros and cons of that type of approach. So I'm glad I asked that question because I could have

1489
02:01:49,080 --> 02:01:54,600
spent two hours and said less. Are you bullish or bearish on liquid biopsies? You indirectly

1490
02:01:54,600 --> 02:01:58,760
alluded to it, which is, look, with leukemia, we're basically doing a liquid biopsy. Correct.

1491
02:01:58,760 --> 02:02:04,760
So now the question is when we, a year from now, presumably have at least phase two data on

1492
02:02:04,760 --> 02:02:09,560
liquid biopsies for solid organ tumors and lymphomas, are you optimistic and how do you

1493
02:02:09,560 --> 02:02:13,800
see that changing the way we practice potentially? I guess I'd say the interesting thing about

1494
02:02:13,800 --> 02:02:19,160
leukemia is it was very likely one of the first things we studied in part because we can access

1495
02:02:19,160 --> 02:02:23,160
it so frequently and so readily and we can actually track its volume in an era where

1496
02:02:23,160 --> 02:02:27,080
the doctors had to measure marbles through foam rubber with their calipers. And so that's why

1497
02:02:27,080 --> 02:02:30,600
leukemia has always sort of been a couple of steps ahead of everything else.

1498
02:02:30,600 --> 02:02:34,200
About liquid biopsies, I guess I would say that, you know, we do have a number of approved liquid

1499
02:02:34,200 --> 02:02:38,840
biopsy tests and they're certainly going to have a role. Anytime we've already conceded that there

1500
02:02:38,840 --> 02:02:41,640
are some mutations that are very important in certain tumors and we have drugs for those

1501
02:02:41,640 --> 02:02:45,960
mutations, if you can find out that mutational information without having to stick a needle in

1502
02:02:45,960 --> 02:02:50,200
someone, everyone is going to be grateful for that. I guess the questions that are always going to be

1503
02:02:50,200 --> 02:02:55,160
kind of, and they vary probably test by test, mutation by mutation is, is the test sensitive

1504
02:02:55,160 --> 02:02:58,920
enough in this case? If you find you don't have the mutation on the liquid test, are you still

1505
02:02:58,920 --> 02:03:03,320
going to want to go get tumor tissue or do you need to get tumor tissue for another reason to

1506
02:03:03,320 --> 02:03:06,360
figure out the subtype? And in that case, you might as well, you already have the information

1507
02:03:06,360 --> 02:03:11,480
you need. The real role for liquid biopsy might be the serial nature of it. You can track something

1508
02:03:11,480 --> 02:03:16,040
over time, see how it's doing over time. And that might be the real boon. But I guess I would say,

1509
02:03:16,040 --> 02:03:19,880
am I bullish about it? I'd say yes. I mean, I'm not known for being a bullish person, but I guess

1510
02:03:19,880 --> 02:03:23,640
I would say that's something that I would definitely study more and do sort of really

1511
02:03:23,640 --> 02:03:27,320
good clinical trials at the end of the day to see what the role can be. But yeah, they're already

1512
02:03:27,320 --> 02:03:32,120
in our clinics. We use them. Yeah. You've talked about this indirectly that, I mean, look, you're

1513
02:03:32,120 --> 02:03:39,320
naturally a skeptic, you're naturally critical. Those are very valuable tools. They've served you

1514
02:03:39,320 --> 02:03:44,200
well. And more importantly, they've served many people well, meaning your skepticism and your

1515
02:03:44,760 --> 02:03:50,200
critical thought has sort of made medicine better. How do we, or maybe I'll ask it more directly,

1516
02:03:50,200 --> 02:03:55,080
how do you navigate that balance? There are some people that are skeptical for the sake of being

1517
02:03:55,080 --> 02:04:00,440
skeptical. They're always the contrarian. No matter what the answer is, they're always going to take

1518
02:04:00,440 --> 02:04:08,040
the opposite side of that. How do you sort of, police is the wrong word, but how do you navigate

1519
02:04:08,040 --> 02:04:15,080
that internally? I guess I would say, I mean, there is an example of something where I was not

1520
02:04:15,080 --> 02:04:19,480
on the side, I'm usually odd and people gave me a hard time about, which was the dexamethasone in

1521
02:04:19,480 --> 02:04:24,120
COVID, the recovery study very recently. So I mean, let me just put it in a context. One of the

1522
02:04:24,120 --> 02:04:28,200
things that happens a lot in medicine these days is medicine by press release, which is one day,

1523
02:04:28,280 --> 02:04:32,120
Eli Lilly announces top line results are positive for this trial and this drug,

1524
02:04:32,120 --> 02:04:37,080
improved outcomes in these people. And this is the hazard ratio 0.7. But how long did they live?

1525
02:04:37,080 --> 02:04:39,880
Did it have side effects? None of that's in the press release. You know, it's just sort of a

1526
02:04:39,880 --> 02:04:44,280
fragment of information. And I think they put that out primarily for the shareholders and for

1527
02:04:44,280 --> 02:04:48,600
their information. And some doctors may act upon that. I mean, that might be something

1528
02:04:48,600 --> 02:04:52,360
that people find enough to act upon. And I'm always a big critic of that and say, let's wait

1529
02:04:52,360 --> 02:04:55,960
for the paper. Let's wait for more information. Recently, there was a recovery trial out of the

1530
02:04:55,960 --> 02:05:00,680
UK. And they say that dexamethasone for people who are hospitalized requiring O2 with SATH less

1531
02:05:00,680 --> 02:05:05,480
than 94 people who are mechanically ventilated, that they benefit from dexamethasone, all cause

1532
02:05:05,480 --> 02:05:09,000
mortality benefit. Whereas people who are hospitalized did not require supplemental O2.

1533
02:05:09,000 --> 02:05:12,440
They actually maybe they didn't benefit. They were harmed. And there's a significant interaction

1534
02:05:12,440 --> 02:05:16,040
coefficient. They put out that press release. And I went online and I looked and I'm like,

1535
02:05:16,040 --> 02:05:19,640
oh boy, you can read their protocol. Their protocol was published a month ago. It's available.

1536
02:05:19,640 --> 02:05:23,560
It's 35 pages. You can read it and you can read the statistical analysis plan that's also out

1537
02:05:23,560 --> 02:05:27,080
there. It tells you exactly what they were going to do, what the pre-specified endpoints are.

1538
02:05:27,080 --> 02:05:30,440
And we're in the midst of a pandemic where people have been trying things left, right,

1539
02:05:30,440 --> 02:05:34,120
and center, throwing the kitchen sink at people with COVID. And so I said, the evidence is good

1540
02:05:34,120 --> 02:05:37,800
enough. We got to do this right now. We got the press release. We got, you know, I like to say a

1541
02:05:37,800 --> 02:05:42,600
press release and a protocol. That's like a driver's license and a social security card and a paper.

1542
02:05:42,600 --> 02:05:46,360
That's like a passport. Do I have to show you my passport? I can show you the other two. It's

1543
02:05:46,360 --> 02:05:50,840
another form of identification. Okay. So anyway, so I say this and I got a lot of heat from the

1544
02:05:50,920 --> 02:05:55,080
usual critical club because they said, well, we got to wait for the paper. And I say there's such

1545
02:05:55,080 --> 02:05:59,160
a thing as too much skepticism. And that's what I think is a good example. I guess I would say that

1546
02:05:59,160 --> 02:06:04,200
probably the greatest way in which I can't even claim that I'm on the set point on the thermostat,

1547
02:06:04,200 --> 02:06:09,000
but I can say the only way to kind of keep this in balance between skepticism that's so bad that

1548
02:06:09,000 --> 02:06:13,640
it's paralyzing, you can't do anything, and blind acceptance that's so bad that you are cheerleader

1549
02:06:13,640 --> 02:06:18,680
for everything. The way to keep that in balance is I find going into the clinic because no matter

1550
02:06:18,680 --> 02:06:22,760
how skeptical you are about drugs, you have to have those conversations with real people. And

1551
02:06:22,760 --> 02:06:26,680
there's some people who are going to be taking drugs that you're skeptical of. And maybe I should

1552
02:06:26,680 --> 02:06:31,000
just make a point about what I think my clinical philosophy is. Some people sometimes ask me.

1553
02:06:31,000 --> 02:06:34,600
It's not the doctor's role to determine what treatment is right for someone. It's really the

1554
02:06:34,600 --> 02:06:39,480
doctor's role in my mind to empower the patient with what I know about the drug, what's been shown,

1555
02:06:39,480 --> 02:06:43,640
what hasn't been shown, what the benefit might be, what are the uncertainties, what are the known

1556
02:06:43,640 --> 02:06:48,200
toxicities and risks, and then to walk them through how they would decide. Is it worth it

1557
02:06:48,200 --> 02:06:52,680
to them to take those risks? And different people will choose differently. Some people will choose

1558
02:06:52,680 --> 02:06:55,880
differently than what I would choose for myself. Some people will choose differently than the next

1559
02:06:55,880 --> 02:06:59,880
person with the same information. I think those are all okay. The more you practice medicine,

1560
02:06:59,880 --> 02:07:03,720
the more you realize that this is an art. You're never going to have perfect information. You're

1561
02:07:03,720 --> 02:07:08,440
going to have to make decisions today with less than perfect information. And I think that's a way

1562
02:07:08,440 --> 02:07:14,680
that one keeps skepticism in check. I actually think this is one of the biggest challenges in this

1563
02:07:15,400 --> 02:07:21,480
art slash science of medicine. This is very different from experimental physics, where we

1564
02:07:21,480 --> 02:07:28,120
could all hang around a particle collider and debate the experimental results all day long

1565
02:07:28,760 --> 02:07:32,840
and red team, blue team. I mean, it is really different. And I struggle with this

1566
02:07:33,720 --> 02:07:39,560
so much. And I think you said it exactly correctly with respect to that thermostat is if you're too

1567
02:07:39,560 --> 02:07:45,000
much of a skeptic, you're not doing anything. You're going to sit on your hands forever. And

1568
02:07:45,000 --> 02:07:51,080
there's a cost of doing nothing. And that should never be forgotten. And if you blindly accept

1569
02:07:51,080 --> 02:07:56,360
everything, you are almost unquestionably subjecting patients to unnecessary treatments.

1570
02:07:56,920 --> 02:08:00,680
I feel lucky, right? Because I have two full time colleagues in my practice,

1571
02:08:00,680 --> 02:08:06,040
and then two part time colleagues. So there's a group of five of us basically can argue with

1572
02:08:06,040 --> 02:08:11,960
each other all day. But we never lose sight of the fact that it's not an intellectual pursuit.

1573
02:08:11,960 --> 02:08:16,440
At the end, a decision needs to be made, even if a decision is to do nothing. That's still a

1574
02:08:16,440 --> 02:08:21,320
decision. And I love that I actually find that to be some of the most enjoyable stuff we do is just

1575
02:08:21,320 --> 02:08:25,720
the debate. That's another nice thing that you brought out, which is that's another way to keep

1576
02:08:25,720 --> 02:08:32,040
skepticism in check is you go to a tumor board or you go to a multidisciplinary meeting, or you

1577
02:08:32,040 --> 02:08:35,960
sit around the table with a few of your colleagues and you present some cases. And you hear what

1578
02:08:35,960 --> 02:08:38,760
other people have to say at the end of the day that something's got to be done. They're either

1579
02:08:38,760 --> 02:08:42,920
going to do it or they're not going to do it. And sometimes it is important to know that people do

1580
02:08:42,920 --> 02:08:46,360
things differently than we might do it. And I think it keeps us in sort of a balance.

1581
02:08:47,160 --> 02:08:50,760
This was a fascinating discussion. Yeah, there's a lot of other things that we could talk about

1582
02:08:50,760 --> 02:08:55,560
here. But I also am wary of the fact that in the COVID environment, people have less and less

1583
02:08:55,560 --> 02:08:59,640
time for podcasts. So the longer this podcast goes on, the lower the probability people are

1584
02:08:59,640 --> 02:09:03,800
going to listen to it. And I want to make sure people hear this one. So I wish you the best

1585
02:09:03,800 --> 02:09:09,800
as you continue with your move into San Francisco. I'm sure we will speak again.

1586
02:09:10,360 --> 02:09:13,640
Thanks so much, Peter. Thanks for having me. It's a terrific discussion and great to chat with you

1587
02:09:13,640 --> 02:09:18,600
on these topics. Thank you for listening to this week's episode of The Drive. If you're interested

1588
02:09:18,600 --> 02:09:23,400
in diving deeper into any topics we discuss, we've created a membership program that allows us to

1589
02:09:23,400 --> 02:09:28,760
bring you more in-depth, exclusive content without relying on paid ads. It's our goal to ensure

1590
02:09:28,760 --> 02:09:34,040
members get back much more than the price of the subscription. Now to that end, membership benefits

1591
02:09:34,040 --> 02:09:39,720
include a bunch of things. One, totally kick ass comprehensive podcast show notes that detail every

1592
02:09:39,720 --> 02:09:44,920
topic, paper, person, thing we discuss on each episode. The word on the street is nobody's show

1593
02:09:44,920 --> 02:09:50,600
notes rival these. Monthly AMA episodes or Ask Me Anything episodes, hearing these episodes

1594
02:09:50,600 --> 02:09:56,360
completely. Access to our private podcast feed that allows you to hear everything without having

1595
02:09:56,360 --> 02:10:01,720
to listen to spiel's like this. The Qualies, which are a super short podcast that we release

1596
02:10:01,720 --> 02:10:06,120
every Tuesday through Friday, highlighting the best questions, topics, and tactics discussed on

1597
02:10:06,120 --> 02:10:11,400
previous episodes of The Drive. This is a great way to catch up on previous episodes without having

1598
02:10:11,400 --> 02:10:17,000
to go back and necessarily listen to everyone. Steep discounts on products that I believe in,

1599
02:10:17,000 --> 02:10:21,560
but for which I'm not getting paid to endorse and a whole bunch of other benefits that we continue

1600
02:10:21,560 --> 02:10:26,200
to trickle in as time goes on. If you want to learn more and access these member only benefits,

1601
02:10:26,200 --> 02:10:32,440
you can head over to PeterAttiaMD.com forward slash subscribe. You can find me on Twitter,

1602
02:10:32,440 --> 02:10:38,520
Instagram, and Facebook, all with the ID PeterAttiaMD. You can also leave us a review on Apple

1603
02:10:38,520 --> 02:10:44,280
Podcast or whatever podcast player you listen on. This podcast is for general informational

1604
02:10:44,280 --> 02:10:48,680
purposes only and does not constitute the practice of medicine, nursing, or other professional

1605
02:10:48,680 --> 02:10:54,600
healthcare services, including the giving of medical advice. No doctor patient relationship

1606
02:10:54,600 --> 02:11:00,120
is formed. The use of this information and the materials linked to this podcast is at the user's

1607
02:11:00,120 --> 02:11:05,240
own risk. The content on this podcast is not intended to be a substitute for professional

1608
02:11:05,240 --> 02:11:11,720
medical advice, diagnosis, or treatment. Users should not disregard or delay in obtaining

1609
02:11:11,720 --> 02:11:16,600
medical advice from any medical condition they have, and they should seek the assistance of

1610
02:11:16,600 --> 02:11:22,280
their healthcare professionals for any such conditions. Finally, I take conflicts of interest

1611
02:11:22,280 --> 02:11:28,120
very seriously. For all of my disclosures and the companies I invest in or advise, please visit

1612
02:11:28,120 --> 02:11:35,320
PeterAttiaMD.com forward slash about where I keep an up-to-date and active list of such companies.

