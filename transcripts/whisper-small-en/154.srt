1
00:00:00,000 --> 00:00:07,000
Hey everyone, welcome to the Drive podcast. I'm your host, Peter Attia.

2
00:00:07,000 --> 00:00:16,360
Hey Steve, it's awesome to see you today, although I was really hoping we were going

3
00:00:16,360 --> 00:00:19,480
to be able to figure out a way to do this in person if we waited long enough, but at

4
00:00:19,480 --> 00:00:23,720
some point I just thought we just need to sit down and do this and videos almost as

5
00:00:23,720 --> 00:00:26,960
good. So anyway, thanks so much for making time.

6
00:00:26,960 --> 00:00:29,240
So excited to be here. Really looking forward to it.

7
00:00:29,240 --> 00:00:34,960
I feel like I haven't seen you in person in, it's been too long. It's probably been four

8
00:00:34,960 --> 00:00:35,960
years.

9
00:00:35,960 --> 00:00:39,520
I think it was we drove around in that really noisy car of yours was the last time that

10
00:00:39,520 --> 00:00:43,080
I was with you. The little tiny thing that made so much noise. You know what I'm talking

11
00:00:43,080 --> 00:00:44,080
about?

12
00:00:44,080 --> 00:00:48,160
Yeah, I sure do. Didn't go very fast, but it made a lot of noise for going slow.

13
00:00:48,160 --> 00:00:54,000
So I think a lot of listeners probably are familiar with you, of course. They're familiar

14
00:00:54,000 --> 00:00:59,920
with you through obviously your books, podcasts, things like that. But maybe it's worth actually

15
00:00:59,920 --> 00:01:04,120
telling people a little bit about how you got interested in the study of economics.

16
00:01:04,120 --> 00:01:10,040
And obviously it's through that study that your work became available to a much broader

17
00:01:10,040 --> 00:01:15,000
swath of the population than just sort of economists. So what got you interested in

18
00:01:15,000 --> 00:01:17,400
studying economics?

19
00:01:17,400 --> 00:01:20,960
So I was never interested in economics and probably still not interested in economics,

20
00:01:20,960 --> 00:01:27,400
but the way I became an economist was really through the back door. I was the worst kind

21
00:01:27,400 --> 00:01:31,280
of student in college. I mean, I got good grades. I got into Harvard. I got good grades

22
00:01:31,280 --> 00:01:35,480
at Harvard, but I didn't care about learning at all. All I really cared about was gaming

23
00:01:35,480 --> 00:01:41,880
the system. And I had learned that the best way to pick classes was to find the classes

24
00:01:41,880 --> 00:01:46,120
that everybody took because it turned out those were both really good classes in general

25
00:01:46,120 --> 00:01:50,960
and really easy. And it just happened that at Harvard, Act 10, the introductory economics

26
00:01:50,960 --> 00:01:54,360
course was exactly one of those courses. So I took it not because I was interested in

27
00:01:54,360 --> 00:01:58,760
economics, but just because my rule of thumb was to take classes like that. And I remember

28
00:01:58,760 --> 00:02:03,640
roughly, I don't know, the fifth or the sixth lecture was on this topic called comparative

29
00:02:03,640 --> 00:02:09,920
advantage. And as the teacher walked us through it, I was just disgusted. And I thought, my

30
00:02:09,920 --> 00:02:16,280
God, this is the easiest topic in the entire world. I have literally known about the concept

31
00:02:16,280 --> 00:02:19,960
of comparative advantage since I was five years old. I mean, I understood it deeply in

32
00:02:19,960 --> 00:02:23,360
my soul. I didn't know the name, like he was putting a name on it. And I remember at the

33
00:02:23,360 --> 00:02:28,640
time going through and actually thinking, this is such a travesty. My parents are paying

34
00:02:28,640 --> 00:02:33,800
this much per lecture at Harvard. It was a big number for this total drivel. Like this

35
00:02:33,800 --> 00:02:39,920
is just an insult. And as I was walking out of the room, my best friend at college was

36
00:02:39,920 --> 00:02:45,000
in the same class and he walked out next to me and he said, Oh my God, that lecture was

37
00:02:45,000 --> 00:02:48,920
crazy. I said, I know how could they teach us such drivel? He said, no, that was the

38
00:02:48,920 --> 00:02:52,520
most confusing thing I've ever heard. I don't think I couldn't understand that in a million

39
00:02:52,520 --> 00:02:57,480
years. And I had this moment of clarity where I said, wait a second. I actually think like

40
00:02:57,480 --> 00:03:01,200
an economist. And though, even though I've never been interested in the topic from that

41
00:03:01,200 --> 00:03:06,540
moment on, it became clear and clear to me that I was just born thinking like an economist.

42
00:03:06,540 --> 00:03:09,720
And it's not advice I would give to other people in general. I don't think it's great

43
00:03:09,720 --> 00:03:15,760
advice to just do what you're good at, but I was so lazy and so intellectually unmotivated

44
00:03:15,760 --> 00:03:18,980
that for me, it turned out not to be a bad path was just to say, well, who cares what

45
00:03:18,980 --> 00:03:22,480
I'm interested in? Cause I wasn't really that interested in anything. Uh, let's just go

46
00:03:22,480 --> 00:03:28,120
be an economist. And, um, and I didn't go willingly. I didn't, it took a long time for

47
00:03:28,120 --> 00:03:33,560
me to be an academic economist. I, I graduated, I did management consulting. It was only my

48
00:03:33,560 --> 00:03:39,880
deep disgust and hatred with management consulting, which in desperation, as I look for any exit,

49
00:03:39,880 --> 00:03:42,440
I thought, well, the only thing I can even think of to get out of here is to go get a

50
00:03:42,440 --> 00:03:48,760
PhD in economics, uh, which is what led me to get a PhD in economics. And if I had known

51
00:03:48,760 --> 00:03:52,720
what it meant to compute a PhD in economics, I never would have done it. It was only complete

52
00:03:52,720 --> 00:03:56,680
ignorance of what I was getting myself into that, that led me down that path, which turned

53
00:03:56,680 --> 00:04:00,520
out to be a lucky path. I mean, things couldn't almost not have turned out better for me,

54
00:04:00,520 --> 00:04:08,080
but it was only a series of mistakes, miscalculations and ignorance that, that led me to this lucky

55
00:04:08,080 --> 00:04:12,760
place. But, but Steve, there's a lot of, um, I mean, there's a lot of, there's gotta be

56
00:04:12,760 --> 00:04:17,680
some challenge and pain and resilience that requires doing everything you just described.

57
00:04:17,680 --> 00:04:23,240
So, okay, maybe Econ 101 or whatever the course was, was a breeze, but at some point, you

58
00:04:23,240 --> 00:04:28,520
know, you're taking senior courses at Harvard in economics. Were they challenging? Did you find

59
00:04:28,520 --> 00:04:32,640
them interesting? I mean, what was kind of going through your mind as you're plotting your way

60
00:04:32,640 --> 00:04:41,120
through undergrad? You know, I, I didn't ever intend to be an academic economist, so I didn't

61
00:04:41,120 --> 00:04:46,720
approach it the way a normal, like someone who, who, who wants to be an academic economist

62
00:04:46,720 --> 00:04:51,200
approached it, taking a bunch of math. So it turns out at the highest level of economics is all about

63
00:04:51,200 --> 00:04:56,520
math. And I didn't take, I didn't take any math. I took exactly one math class at Harvard. It was

64
00:04:56,520 --> 00:05:03,520
called math 1A. It was because I got a two on the calculus AP exam and I couldn't place out. And so

65
00:05:03,520 --> 00:05:08,680
it was really high school calculus. It was the lowest math class they offered at Harvard. And, um,

66
00:05:08,680 --> 00:05:13,080
and I did do quite well in that class. It was essentially me and the, the football team and the

67
00:05:13,080 --> 00:05:18,480
hockey team were the only people who placed into that. And I smoked those guys, but, um, so I just

68
00:05:18,480 --> 00:05:24,240
did, I just had fun. And honestly, in college, I just had fun. And I liked the economics classes

69
00:05:24,240 --> 00:05:29,120
and they were, and they were easy for me because the intuition, economic intuition has always come

70
00:05:29,120 --> 00:05:36,080
pretty easily, especially in microeconomics, uh, pretty easily. And, um, and the classes just weren't

71
00:05:36,080 --> 00:05:41,320
that hard. They didn't require much math. I didn't know any math. And it was, I mean, college in general

72
00:05:41,320 --> 00:05:48,440
for me was just so much fun. Uh, I, I had good friends and good times and it was easy and I,

73
00:05:48,440 --> 00:05:53,560
independence and I just, you know, for me, those were the best four years of my life and I didn't

74
00:05:53,560 --> 00:05:58,200
take school too seriously, but I did well. So, you know, it was all, that was just a breeze. It was the

75
00:05:58,200 --> 00:06:03,400
shock of reality, the real world. I had no idea how unfun the real world was going to be when I

76
00:06:03,400 --> 00:06:08,920
actually got into it. Uh, but college was fun. That is so funny to hear you say that I, I, I will

77
00:06:08,920 --> 00:06:14,040
forever maintain that college was the worst four years of my life. And I hope I can say that now,

78
00:06:14,040 --> 00:06:17,400
meaning that there will be no four year period that is still in front of me. That's going to be

79
00:06:17,400 --> 00:06:22,120
as bad as those four years were. But I, I always enjoy hearing people talk about college being the

80
00:06:22,120 --> 00:06:28,920
best four years of their lives. Um, you mentioned microeconomics, um, is maybe now is as good a time

81
00:06:28,920 --> 00:06:35,000
as any to explain to people the difference between micro and macroeconomics because it is almost a

82
00:06:35,000 --> 00:06:41,000
disservice to talk about economics as though it's one broad topic. Yeah, you're absolutely right.

83
00:06:41,000 --> 00:06:45,960
And that, so microeconomics, well, let's start with macroeconomics. That's what most people think

84
00:06:45,960 --> 00:06:53,880
of when they think of economics. They think about inflation and, uh, and unemployment, unemployment

85
00:06:53,880 --> 00:07:00,600
and economic growth. Okay. And, and those are really important fundamental problems. Um, but they are,

86
00:07:00,600 --> 00:07:05,320
they turn out to be incredibly hard problems because the degree of complexity in the macro

87
00:07:05,320 --> 00:07:11,240
economy, you can think the macro economy is almost infinite, right? So it's, it's, it's this complex

88
00:07:11,240 --> 00:07:16,520
system built up of billions of individual actors and you've got companies, you've got, look, it's

89
00:07:16,520 --> 00:07:23,160
just a mess. Okay. And it turns out just making a long short, very short, a long story, very short

90
00:07:23,160 --> 00:07:27,800
that, um, that we haven't made much headway and really understanding it. Because we've, we've

91
00:07:27,800 --> 00:07:33,560
approached the profession with a kind of discipline, which says every model of macroeconomics

92
00:07:33,560 --> 00:07:38,760
needs to start with foundations that are really, um, rational in, in the, in the base micro

93
00:07:38,760 --> 00:07:42,360
foundations. And it turns out, I think the problems are just too hard and that approach in the end

94
00:07:42,360 --> 00:07:48,280
isn't going to win. Okay. But what is microeconomics? Microeconomics is instead the study of individual

95
00:07:48,280 --> 00:07:53,320
decision-making. So in a, in a world in which there's scarcity and there's competition, how do

96
00:07:53,320 --> 00:08:00,360
I decide how to spend my income? How do I decide what job to do and, and how many hours of, of work

97
00:08:00,360 --> 00:08:05,640
I want to do? How do firms decide what to make and where to locate and things like that? So these are

98
00:08:05,640 --> 00:08:12,760
much smaller decisions and for me, much more intuitive decisions. Um, and, and they're able

99
00:08:12,760 --> 00:08:18,360
to be analyzed both with the, the formal models of economics, which historically have been very

100
00:08:18,360 --> 00:08:23,320
heavily predicated on rationality simply because it makes the math easy, along with the last 40

101
00:08:23,320 --> 00:08:28,280
years of behavioral economics, where, uh, we've introduced a lot more psychology and mistakes and,

102
00:08:28,280 --> 00:08:34,360
and a lot more freedom into the models that we use. But anyway, so, um, really they're almost

103
00:08:35,880 --> 00:08:41,880
distinct micro and macroeconomics, uh, because, uh, really macro in the end is, is

104
00:08:42,600 --> 00:08:47,640
really complicated. Not to make all my macro colleagues incredibly angry at me,

105
00:08:47,640 --> 00:08:52,040
but very self-referential referential models because the problems are so hard that you need

106
00:08:52,040 --> 00:08:57,720
to abstract so greatly to try to deal with the macro problems that I think we don't have a good

107
00:08:57,720 --> 00:09:02,680
handle on them. So I've really steered clear of the macro problems to focus on the individual

108
00:09:02,680 --> 00:09:08,280
decision-making, which is, I think both in my own life, much more relevant, but also I got something

109
00:09:08,280 --> 00:09:13,880
to say because I have some intuition for it. So is there a hierarchy within economics, the way,

110
00:09:14,440 --> 00:09:18,520
you know, so in physics, you sort of have sort of the theoretical physicists and the experimental

111
00:09:18,520 --> 00:09:24,920
physicists and Nobel prizes will go back and forth between the two of those. Um, how does that work

112
00:09:24,920 --> 00:09:30,200
in economics? Do, do we disproportionate amount of Nobel prizes or whatever the highest, I mean,

113
00:09:30,200 --> 00:09:34,680
I guess the Nobel, it's not technically a prize, but it's close enough. When you look at the

114
00:09:34,680 --> 00:09:40,600
highest levels of excellence and awards that are given in the entire field, do they disproportionately

115
00:09:40,600 --> 00:09:45,000
lie on one side of those or the other? So historically economics has been a,

116
00:09:45,000 --> 00:09:53,160
an entirely theoretical discipline. And so the highest status positions historically have been

117
00:09:53,160 --> 00:09:59,320
in theory. Okay. Uh, where I make the distinction between theory and the analysis of data and

118
00:09:59,320 --> 00:10:03,960
econometrics and actually trying to, you know, take messy data and, and make sense of it.

119
00:10:04,360 --> 00:10:11,160
With the data revolution and the emergence of econometrics, it's been, uh, I think I switched

120
00:10:11,160 --> 00:10:16,840
in the profession. So there are two big prizes in economics. There's a Nobel prize and then

121
00:10:16,840 --> 00:10:20,840
there's something called the job. It's Clark medal, which is given to the, the most

122
00:10:20,840 --> 00:10:26,520
influential economist under the age of four. You've won that prize, correct? I did win that. Yeah.

123
00:10:26,520 --> 00:10:32,840
And, um, and so that more reflects what's going on in the present time. And so those prizes,

124
00:10:33,080 --> 00:10:40,920
the Clark medal has, um, mostly gone in the last 20, 30 years, not always, but often to people at

125
00:10:40,920 --> 00:10:47,000
the intersection of theory and data, which I think really is, is where the action is, um, in

126
00:10:47,000 --> 00:10:53,800
economics. It's people who can make sense of data, but are looking at it through a lens of economics

127
00:10:53,800 --> 00:11:01,320
and pushing economic thinking in that way. I think, um, I don't know if anyone cares about

128
00:11:01,320 --> 00:11:05,880
this, but economics is really at a crossroads because it turned out when data analysis

129
00:11:07,240 --> 00:11:12,760
really started to emerge and we could start to say things about causality and ways we couldn't

130
00:11:12,760 --> 00:11:21,480
before the empirical part of economics really took off, but all of the easy stuff has been done.

131
00:11:21,480 --> 00:11:27,000
And so we kind of estimated most of the parameters that we care about. And so the question is what

132
00:11:27,800 --> 00:11:33,880
comes next. And what's happening in economics is that the push has been in the direction of,

133
00:11:33,880 --> 00:11:40,200
okay, it's not enough to just estimate what's happened in the world in the past. That's really

134
00:11:40,200 --> 00:11:44,840
what, what empirical economics is really good at is saying, I can look at what's happened in the

135
00:11:44,840 --> 00:11:50,600
past. I can estimate a number, and then I can build models to try and say why now the task is

136
00:11:50,600 --> 00:11:54,360
become greater. We should say, I'd not, I don't just want to understand the past.

137
00:11:54,360 --> 00:11:59,960
I want to be able to build a model that has the flexibility to tell me what would have happened

138
00:12:00,520 --> 00:12:07,160
if instead of the U S being a democracy, we were a dictatorship. Okay. So things that are so far out

139
00:12:07,160 --> 00:12:12,600
of the realm of what actually happened, that it's almost like science fiction. And so what economists

140
00:12:12,600 --> 00:12:17,400
do now is they're asked not just to estimate parameters really well, but to embed those

141
00:12:17,400 --> 00:12:22,360
parameters into models, which then have enough degrees of freedom that you can start to imagine

142
00:12:22,360 --> 00:12:27,080
if I turn this dial to that tile, what would happen. And I actually personally think that's

143
00:12:27,080 --> 00:12:35,800
a terrible direction because I think, um, I think what has made economics great is that it's easy to

144
00:12:35,800 --> 00:12:41,640
understand the estimates we create and you can say whether you like them or not based on fact

145
00:12:41,640 --> 00:12:46,280
or the approach or whatever. But I think when we get into these more fanciful models, I think we're

146
00:12:46,280 --> 00:12:52,200
going to end up running into a huge dead end is that, um, is that they're really too complicated

147
00:12:52,200 --> 00:12:58,760
to be estimated well. And so we're just going to get into fantasy land of, of creating things that

148
00:12:58,760 --> 00:13:04,760
are really not, um, grounded in fact. So, um, there's a famous physicist and it's been attributed

149
00:13:04,760 --> 00:13:10,120
to so many that I actually don't know who really said it, but, uh, the, the spirit of the quote is

150
00:13:10,120 --> 00:13:18,280
all models are wrong. Some are useful. Um, and I feel like economics is certainly a discipline

151
00:13:18,280 --> 00:13:25,560
in which that applies, um, and suffers greatly. Um, I mean, look, look, even things, even something

152
00:13:25,560 --> 00:13:30,040
that I think was much simpler to try to understand was, you know, the extent to which coronavirus was

153
00:13:30,040 --> 00:13:39,080
going to be, um, a problem. I mean, those initial models were out to lunch. Um, and it's, I don't

154
00:13:39,080 --> 00:13:42,360
fault the people who tried to put those models together, the epidemiologists who worked on these

155
00:13:42,360 --> 00:13:50,120
models, but you know, the sensitivity around so many variables was so great that you couldn't

156
00:13:50,120 --> 00:13:55,880
even begin to extrapolate. And when you did it, it, the only thing I think the models helped you

157
00:13:55,880 --> 00:14:00,200
understand was how impossible it would be to predict what was going on. And if nothing else,

158
00:14:00,200 --> 00:14:04,600
it was going to show you what parameters mattered, but that, that was about the extent of what you

159
00:14:04,680 --> 00:14:09,720
could extract from that, which was, you know, the rate at which this thing spreads and the length of

160
00:14:09,720 --> 00:14:14,760
time that it can lay dormant, those things are going to matter a lot. And by the way, what we

161
00:14:14,760 --> 00:14:20,200
should have taken from that is earlier and more aggressive testing was a better solution as opposed

162
00:14:20,200 --> 00:14:26,040
to trying to pontificate or estimate whether 60 million Americans were going to get it in 5 million

163
00:14:26,040 --> 00:14:29,080
we're going to die versus 1 million we're going to die. I mean, those were, that was sort of the

164
00:14:29,080 --> 00:14:34,440
wrong thing to be thinking about, but again, all that's easy in hindsight, but I, but I hear you

165
00:14:34,440 --> 00:14:41,080
in terms of how that could be frustrating in, in, in this profession. Yeah. I mean, I have a very

166
00:14:41,080 --> 00:14:49,480
strong view about the relationship between data and theory. And my view is that the sensible way

167
00:14:49,480 --> 00:14:56,600
to do things is to make a really sharp distinction between the two that we should really understand

168
00:14:56,600 --> 00:15:04,360
you go to the data and you just understand what the three, five, nine, 12 facts are. And in my view

169
00:15:04,360 --> 00:15:09,640
is that those 12 facts should be things that if any reasonable person looked at the data,

170
00:15:10,440 --> 00:15:15,000
you'd more or less agree on those facts. And to just establish that that is the role of data

171
00:15:15,000 --> 00:15:19,320
analysis is to understand those facts. And obviously there's, you know, you want to get it

172
00:15:20,200 --> 00:15:24,120
in the direction of causality and whatnot, but you never do that with the data alone. The data alone

173
00:15:24,120 --> 00:15:29,640
are never enough except except in a randomized experiment to tell you about causality and, and

174
00:15:29,640 --> 00:15:34,280
rarely in economics do we get to do randomized experiments. And then I think if you can all agree

175
00:15:34,280 --> 00:15:40,520
on the facts, then I'm all in favor of models going forward. Because if I write down a model

176
00:15:41,160 --> 00:15:47,000
that is built off of the 12 facts I see, it turns out it's really easy, the kind of models that

177
00:15:47,000 --> 00:15:52,520
economists write down for other economists to say, okay, I understand whether that's a good model or

178
00:15:52,520 --> 00:15:57,400
bad model, what it's sensitive to and whatnot. And, and if you make this bifurcation between

179
00:15:57,400 --> 00:16:04,360
understanding the facts and then going into what, what those facts imply, because we really care not

180
00:16:04,360 --> 00:16:08,440
about the facts themselves, but we care about the implications of those facts for the future or

181
00:16:08,440 --> 00:16:12,520
what they tell us about causality or whatnot. I think it's a really powerful model. It's not

182
00:16:12,520 --> 00:16:17,880
the model that we use in economics and it's definitely not the model we use, say in journalism,

183
00:16:18,840 --> 00:16:23,720
that it's often incredibly hard or in debate, right? So it's often incredibly hard when a debate

184
00:16:23,720 --> 00:16:29,000
is going on to understand are these people disagreeing about the facts or are they disagreeing

185
00:16:29,000 --> 00:16:34,520
about the implication of the facts? And so like if I had my dream about how we would do journalism,

186
00:16:34,520 --> 00:16:39,640
every newspaper article would basically have a separate little feature that just said, these

187
00:16:39,640 --> 00:16:46,360
are the five facts on which I am writing this article. And then it would be easy to, for me,

188
00:16:46,360 --> 00:16:50,280
say if it's an area I'm expert in to say, oh, well, those facts don't make any sense or for a fact

189
00:16:50,280 --> 00:16:55,320
checker to affect them, or in a debate to say, Hey, before we debate, I would like each of the

190
00:16:55,320 --> 00:17:00,760
participants to lay out the 10 facts that they believe to be true and see if they disagree with

191
00:17:00,760 --> 00:17:04,840
each other's facts. If they disagree about facts, then we're in a very different situation than if

192
00:17:04,840 --> 00:17:09,480
we disagree about the implications of those facts. And somehow I think that's gotten a lot like that

193
00:17:09,480 --> 00:17:13,800
distinction. I've never really heard anyone talk about it except for me, because I'm probably not

194
00:17:13,800 --> 00:17:19,400
important, but it strikes me as being really, really important. Well, let's use an example of

195
00:17:19,400 --> 00:17:28,760
that where I think you and Dubner have gotten to some crosshairs of people is around climate change.

196
00:17:28,760 --> 00:17:33,960
I mean, you wrote about this, God, has it been 10 years ago now? I've lost track. Yeah. Was it

197
00:17:33,960 --> 00:17:40,040
super for economics or? Yeah, super for economics. So I think it's been about 10 years. Okay. So

198
00:17:41,000 --> 00:17:45,480
let's go back to, let's start with some facts. What did you actually write? Because I think

199
00:17:45,480 --> 00:17:51,320
that's been very misconstrued over time. Yeah. Okay. So one thing you can never trust is for people

200
00:17:51,320 --> 00:17:54,600
to tell you the truth about what they've done in the past. Number one, because they're going to

201
00:17:54,600 --> 00:17:59,000
lie, but number two, they don't even, like my, I don't really know what I've been, I haven't looked

202
00:17:59,000 --> 00:18:05,560
back at that book in a long time either, but here's what I think we wrote. So we basically wrote that

203
00:18:06,360 --> 00:18:13,400
at that time, there was evidence that the planet was getting warmer, but certainly it was not

204
00:18:13,960 --> 00:18:21,800
completely open and shut. And we said, but look, we're not scientists. And what we thought was that

205
00:18:23,000 --> 00:18:27,240
the people talking about climate change were asking the wrong question, right? What people

206
00:18:27,240 --> 00:18:32,680
talking about climate change were saying is how do we reduce the amount of carbon we put in the air?

207
00:18:32,680 --> 00:18:36,360
Okay. Which actually wasn't the right question. The real quick, right question was

208
00:18:37,640 --> 00:18:41,960
the planet is too hot. We're not happy about the planet, that the planet is getting hotter.

209
00:18:42,600 --> 00:18:48,200
What is the best way to cool down the planet? To me, that was really the right question. And like

210
00:18:48,200 --> 00:18:55,400
the example I give is, um, you know, so I've had a lot of, I have a lot of babies, I have six kids. So

211
00:18:55,960 --> 00:19:02,120
some of the kids, they poop all the time. Okay. And it's a problem. Okay. And so in, in, because

212
00:19:03,000 --> 00:19:06,840
they poop on the floor all day long, it's a real problem. Okay. So you could say, well,

213
00:19:06,840 --> 00:19:11,480
to solve that problem, let's figure out how to keep kids from pooping or how to stuff the poop

214
00:19:11,480 --> 00:19:15,240
back in the kid or whatever. It's like, but the answer is like, how do you get the poop? So it

215
00:19:15,240 --> 00:19:18,680
doesn't go on the ground. Well, it's a diaper. Okay. And it's like very different answer than

216
00:19:18,680 --> 00:19:23,080
if you get worried about like, well, poo shouldn't exist. So I think the same with carbon, the carbon

217
00:19:23,080 --> 00:19:26,520
is coming out and you don't necessarily have to stuff the carbon back in. If you can find another

218
00:19:26,520 --> 00:19:31,560
way to cool the planet, starting from that premise, we then talked to real scientists who

219
00:19:31,560 --> 00:19:36,760
knew something who were working on what's called geoengineering. So these are ways to try to cool

220
00:19:36,760 --> 00:19:42,200
the planet. Okay. And look, they're not perfect in all sorts of ways, but these really smart guys

221
00:19:42,200 --> 00:19:46,920
have, have come up with a lot of clever plans that we might potentially be able to cool the planet.

222
00:19:46,920 --> 00:19:51,720
So things like putting sulfur dioxide into the stratosphere turns out super cheaply,

223
00:19:51,720 --> 00:19:56,760
couple hundred million dollars a year, we could put sulfur dioxide in the stratosphere. It,

224
00:19:56,760 --> 00:20:02,840
it mimics, but in a much more efficient way, what big volcanic eruptions like Mount Pinatubo have done.

225
00:20:03,400 --> 00:20:09,320
And, and that could cool the planet. Another, like the best idea I think that was out there

226
00:20:09,320 --> 00:20:14,280
was it turns out that, that certain kinds of clouds that have the right kind of reflectivity

227
00:20:14,280 --> 00:20:19,880
in the right level can actually reflect a bunch of sunlight back and, and oceans are really dark.

228
00:20:19,880 --> 00:20:23,720
So they absorb lots of heat. Clouds are really light. They reflect a lot of heat. So

229
00:20:24,680 --> 00:20:29,000
a scientist to come up with a plan, never been tested even in the 10 years since we did it,

230
00:20:29,000 --> 00:20:34,760
where a bunch of solar powered dinghies would troll around in the ocean controlled by GPS,

231
00:20:34,760 --> 00:20:39,880
throwing up salt from the water, which then seats the clouds, which then leads to a bunch of clouds

232
00:20:39,880 --> 00:20:45,640
over the ocean, which you know, his claim, their claim from simulations is that that would be

233
00:20:45,640 --> 00:20:51,480
enough. 10,000 of those dinghies would be enough to offset all of, of, of climate change. Lebomi.

234
00:20:51,560 --> 00:20:55,160
Look, all of these are probably short-term solutions relative to a real solution,

235
00:20:55,960 --> 00:21:02,040
scientific solution or behavioral solution to, to try and deal with, with climate change.

236
00:21:02,040 --> 00:21:07,720
But so that's what we wrote about. And, and admittedly, in retrospect, I regret that we

237
00:21:07,720 --> 00:21:12,760
wrote about it in such a lighthearted way. I didn't realize that environmentalists had no

238
00:21:12,760 --> 00:21:17,960
sense of humor. And so you couldn't like poke fun at mistakes they'd made in the past or,

239
00:21:17,960 --> 00:21:23,240
and so we really aggravated a lot of people. But, but what we, what we did was we unleashed

240
00:21:23,800 --> 00:21:31,560
this incredible machinery that there was a whole machinery around the environmental movement,

241
00:21:31,560 --> 00:21:37,160
which, which basically decided we were enemy number, public enemy number one, and then set

242
00:21:37,160 --> 00:21:41,960
out to destroy us, not really worried about what we actually said or whether we were right. I mean,

243
00:21:41,960 --> 00:21:49,720
I think, I do think 10 years later, almost everyone now thinks that geoengineering is likely

244
00:21:49,720 --> 00:21:55,640
to be part of the solution. And it was interesting. One of the magazines, I think it was the Atlantic

245
00:21:55,640 --> 00:22:01,240
Monthly, basically wrote an article about us saying that we were the most evil, stupid

246
00:22:01,240 --> 00:22:07,400
morons who've ever existed in the history of the planet. Four years later, the same, the same

247
00:22:07,400 --> 00:22:13,560
magazine wrote essentially our chapter in our book with the exact same examples we had used

248
00:22:13,560 --> 00:22:17,720
with no citation at all of the fact we had written about it, saying how geoengineering was going to

249
00:22:17,720 --> 00:22:23,080
be part of the future. So I think, I think it's good not to worry about whether you're right or

250
00:22:23,080 --> 00:22:27,080
you're credit for it. I think you just have to be happy that the movement, although way too slow,

251
00:22:27,080 --> 00:22:32,920
has been in the right direction. I think for what's, what's inevitable is that the problem

252
00:22:32,920 --> 00:22:39,560
of climate change is one which will not, I claim, ever be solved by behavior change,

253
00:22:40,120 --> 00:22:45,320
because it's all about what economists call externalities, that my own behavior has almost

254
00:22:45,320 --> 00:22:51,640
no effect on me and a negative effect on the rest of the world. And I would say there's never in

255
00:22:51,640 --> 00:22:57,640
the history of mankind been a problem that is fundamentally about externalities, which has been

256
00:22:57,640 --> 00:23:02,840
solved by telling people, you should just do the right thing. And hoping that suddenly everyone

257
00:23:02,920 --> 00:23:06,440
is just going to start doing the right thing. Just literally has never happened. I don't,

258
00:23:06,440 --> 00:23:10,280
any reason I think it's going to start now. Yeah, that's interesting. Can you think of any

259
00:23:10,280 --> 00:23:16,280
examples where humans behave in that way? Where they do the right thing to help other people?

260
00:23:16,280 --> 00:23:20,040
Yes, even if it inconveniences them. Are there any? Can we think?

261
00:23:20,040 --> 00:23:27,400
I mean, there are social norms like on the roads. People don't really cut in line very much,

262
00:23:27,400 --> 00:23:29,720
you know, a little bit, but I don't think it's because-

263
00:23:29,800 --> 00:23:31,880
But is that also because there's direct feedback?

264
00:23:32,840 --> 00:23:35,240
Exactly. Because people yell at you because it's uncomfortable. I mean,

265
00:23:36,120 --> 00:23:42,840
but you know, very few things where we have private benefit and look at little things we

266
00:23:42,840 --> 00:23:46,360
do around the edges and certainly do it for family members and whatnot. But

267
00:23:49,160 --> 00:23:55,080
I really think, honestly, if you look at every big problem that's been solved for mankind,

268
00:23:56,040 --> 00:24:02,200
I challenge you to find one that wasn't solved by technology. And it goes from everything from

269
00:24:02,200 --> 00:24:07,800
disease to, I mean, I think, you know, you know much more about this than me. But if you think

270
00:24:07,800 --> 00:24:14,680
about the benefits, health benefits that have come from behavior change versus technology,

271
00:24:14,680 --> 00:24:21,000
invention, discovery, like I got to believe, you know, smoking being maybe the one example where

272
00:24:21,800 --> 00:24:28,200
in the face of overwhelming evidence, it still took what, 40 years for people to cut down on

273
00:24:28,200 --> 00:24:33,080
smoking and still a lot of people- Well, and I would argue that it wasn't really due to just a

274
00:24:33,080 --> 00:24:40,200
straight- It wasn't the knowledge that tobacco killed that actually led to the reduction. In

275
00:24:40,200 --> 00:24:45,400
fact, if you look at the timeline of this, it's an interesting case study in behavior change. So I

276
00:24:45,400 --> 00:24:52,200
think the Surgeon General's report on smoking, which was the first really unambiguous declaration

277
00:24:52,200 --> 00:24:57,960
that smoking killed. It had been suspected in the 40s and 50s, but it wasn't until, I want to say

278
00:24:57,960 --> 00:25:07,240
about 65-ish that you couldn't argue it anymore. I mean, we're talking hazard ratios of 14X,

279
00:25:07,240 --> 00:25:12,760
right? Like there's nothing in biology that produces a hazard ratio of 14X outside of

280
00:25:12,760 --> 00:25:20,520
parachuting without parachutes. This is going to end in a bad way. And if there was really no

281
00:25:20,520 --> 00:25:29,320
change in tobacco use after that for five, six more years, the first real dent in tobacco use

282
00:25:29,320 --> 00:25:35,320
came with advertising. When a law was passed that said you couldn't advertise, a tobacco company

283
00:25:35,320 --> 00:25:41,080
couldn't advertise tobacco unless the commercial was juxtaposed with an anti-tobacco ad. And it

284
00:25:41,080 --> 00:25:46,840
turned out the anti-tobacco ads were so successful that the tobacco companies voluntarily stopped

285
00:25:46,840 --> 00:25:52,200
advertising. And that was the first step in the reduction of tobacco. And then of course,

286
00:25:52,200 --> 00:25:56,120
you threw in other things, which was laws that said, hey, you can't have tobacco sitting at the

287
00:25:56,120 --> 00:26:01,400
counter. It has to be way behind. And obviously then you had excise taxes that were imposed,

288
00:26:01,400 --> 00:26:05,400
and then you had other environmental things you can't smoke on airplanes. And so you could even

289
00:26:05,400 --> 00:26:10,040
make the case that the reduction in smoking has been less about behavior change and more about a

290
00:26:10,040 --> 00:26:14,360
change in the default environment around it, which just speaks more to your point.

291
00:26:15,080 --> 00:26:20,440
Yeah. No, I think that's true. And I think also what's changed in the end is it used to be cool

292
00:26:20,440 --> 00:26:26,920
to smoke. And now, look, if you smoke, people think you're crazy, right? So I think that those

293
00:26:26,920 --> 00:26:33,720
social norms, again, speaking to your point, the costs and the benefits of smoking have changed

294
00:26:33,720 --> 00:26:39,000
dramatically. And if anything, it's surprising how little behavior change has followed that as

295
00:26:39,080 --> 00:26:43,240
opposed to how much. I mean, look at, I mean, again, we're in your territory in that mind,

296
00:26:43,240 --> 00:26:49,880
but look at diabetes and how little behavior change has been in response to diabetes. It's

297
00:26:50,840 --> 00:26:54,760
surprising in many ways. Let's go back to this climate change issue, because again,

298
00:26:54,760 --> 00:27:01,400
it's very interesting. And it's one of those things, I think, like nutrition, where

299
00:27:02,680 --> 00:27:07,960
there's a very gray area between true scientific expertise and propaganda.

300
00:27:09,400 --> 00:27:15,080
What is it about the use of science as a propaganda? I mean, frankly, I think in fact,

301
00:27:16,120 --> 00:27:22,280
I'm somewhat, what's the word? I think I'm somewhat sympathetic to people who

302
00:27:22,520 --> 00:27:27,160
are kind of losing faith in the scientific process a little bit. I'm talking lay people,

303
00:27:27,160 --> 00:27:33,400
people who have never studied science, but who are sort of sick and tired of hearing scientists

304
00:27:34,200 --> 00:27:40,280
stand up on TV and say something and they're a little bit skeptical. And so if you're sort of

305
00:27:40,280 --> 00:27:44,280
an elitist, you look at them and you say, well, they're morons. How can they not believe in

306
00:27:44,280 --> 00:27:49,080
science? But the reality of it is they've kind of been a bit misled, haven't they? And I think that

307
00:27:49,080 --> 00:27:55,800
this, like you look at the, using science as kind of a weapon, I guess, has become a bit

308
00:27:55,800 --> 00:28:06,760
problematic, right? Yeah. Honestly, I think the, I'm more favorable towards climate science

309
00:28:06,760 --> 00:28:11,320
than I used to be. So climate science suffers from the exact same thing that macroeconomics does,

310
00:28:11,320 --> 00:28:17,080
which is that there's observations, but really much of it is based on these models and the models

311
00:28:17,080 --> 00:28:22,120
can't possibly have the kind of specificity because it's such a complex system to understand.

312
00:28:22,200 --> 00:28:26,440
So like most of the leading models at the time we were looking at it had carbon dioxide in it,

313
00:28:26,440 --> 00:28:31,000
but nothing about water vapor. And it turns out that many people think water vapor is more

314
00:28:31,000 --> 00:28:34,680
important than the interaction between carbon dioxide and water vapor would be more important

315
00:28:34,680 --> 00:28:38,120
than anything. But the model is just like computing power and the complexity was too much.

316
00:28:39,240 --> 00:28:45,000
But I give some credit to, like there's a part of climate science which is focused on measurement,

317
00:28:45,000 --> 00:28:48,920
measuring the temperature of the oceans, the rising of the oceans. And I think that's kind

318
00:28:48,920 --> 00:28:54,440
of been done well. Now, what you're talking about though, is that there's such a blurring

319
00:28:54,440 --> 00:29:04,120
of the line between the job of measuring climate change and advocating on TV or in the newspapers

320
00:29:04,120 --> 00:29:12,840
for what that means for public policy. And those are very different tasks. And you, I think as a

321
00:29:12,840 --> 00:29:18,520
scientist, you want to be very careful about moving from one role to the other. And I think

322
00:29:18,520 --> 00:29:24,120
what climate science as an observer, a pretty close observer has done a very poor job on

323
00:29:24,120 --> 00:29:31,720
is helping people, lay people understand when one is in advocacy role versus when one is in

324
00:29:31,720 --> 00:29:36,600
a scientific role. And the same people have played both roles. And I think really it's

325
00:29:38,520 --> 00:29:44,520
an unusual branch of science because the people who go into climate science almost always share

326
00:29:44,520 --> 00:29:49,880
this view that the climate's really important and we should protect it and we should be doing

327
00:29:49,880 --> 00:29:56,760
things not to have climate change. Which is different, I think, from say physics, where

328
00:29:56,760 --> 00:30:02,600
physicists don't have an inherent belief about whether the universe should be expanding or

329
00:30:02,600 --> 00:30:09,000
contracting or whether a particular particle should or should not exist. It's just like that's

330
00:30:09,000 --> 00:30:16,840
a different thing. So, and the other thing I think which has been completely unrealistic

331
00:30:16,840 --> 00:30:22,520
and really problematic for climate science is that they don't think about economics enough.

332
00:30:22,520 --> 00:30:26,520
And they don't think about the reality of human behavior and how difficult it will be to have

333
00:30:26,520 --> 00:30:35,160
behavior change. And they don't think well about the cost of changing our economy. So the cost of

334
00:30:35,640 --> 00:30:43,080
going to a zero car, a net carbon world are really, really big. And we pay those costs right away.

335
00:30:44,760 --> 00:30:48,440
And I'm not saying that we shouldn't necessarily try to do that. It might actually be a good idea

336
00:30:48,440 --> 00:30:54,840
to do it. But to ignore the fact that it's an incredibly expensive solution that's being offered,

337
00:30:54,840 --> 00:31:02,520
I think has been part of the problem because it hasn't really, it's been a mix of ignoring that.

338
00:31:03,080 --> 00:31:10,040
And at this or at the same time saying, look, those costs are good because we're a moral,

339
00:31:10,040 --> 00:31:15,880
humans are awful, humans are ruining the planet and we should suffer for the fact we should have

340
00:31:15,880 --> 00:31:21,320
an enormous contraction in the economy. A gal Gore was upset that there are way too many kinds of

341
00:31:21,320 --> 00:31:27,320
cereal being sold in the grocery store. That's sinful that there are so many choices about cereal.

342
00:31:27,320 --> 00:31:32,840
But it's like a weird religious thing about we should suffer. And again, I think that doesn't

343
00:31:32,840 --> 00:31:38,920
work well to people who maybe aren't convinced or who don't want to suffer. So honestly, I think,

344
00:31:41,480 --> 00:31:48,520
you know, I think if I were able to do one thing in the world, I tried to do it and I failed at it,

345
00:31:48,520 --> 00:31:56,600
it would be to have a sensible approach to figuring out whether there is a cheap solution

346
00:31:56,600 --> 00:32:02,040
to climate change. Because right now, all we're talking about are incredibly expensive ones,

347
00:32:02,040 --> 00:32:07,560
like complete rethinking of capitalism, like, you know, and, you know,

348
00:32:09,720 --> 00:32:16,280
and I don't think that the capitalist structure has worked well on trying to solve the problem of

349
00:32:16,280 --> 00:32:21,800
like is if we put or let me put it, I think we should take if it were up to me, if I were

350
00:32:21,800 --> 00:32:29,080
president, the first thing I would do is I would have what I call a a Manhattan project for climate

351
00:32:29,080 --> 00:32:37,800
change. And I would try to convince the 1000 smartest scientists in the world to stop whatever

352
00:32:37,800 --> 00:32:43,720
they're doing, and to work on climate change, and to put them in a place like Los Alamos,

353
00:32:43,720 --> 00:32:47,880
and to give them resources and it because it's a hard problem. And it's an interdisciplinary

354
00:32:47,880 --> 00:32:53,400
problem. And say, look, for five years, let's just work and let's figure out whether or not

355
00:32:53,400 --> 00:32:58,600
there's going to be a great way to do carbon sequestration, or to do something, you know,

356
00:32:58,600 --> 00:33:02,760
I don't know, something out of the box to do it. And like, either either I think we'll get a solution

357
00:33:02,760 --> 00:33:08,120
in five years, or we'll figure out, oh, crap, this is never going to work. Like we have no way out of

358
00:33:08,120 --> 00:33:13,000
this other than other than stopping to produce carbon or living with the world. Look, and that's

359
00:33:13,000 --> 00:33:19,080
important information to know because if I knew that the only way out of this was we were going

360
00:33:19,080 --> 00:33:24,840
to have, you know, live with the carbon we put in there, then then the willingness of public policy

361
00:33:24,840 --> 00:33:30,760
to impose enormous costs on carbon producing behavior should go away up. But I kind of doubt

362
00:33:30,760 --> 00:33:35,320
that's true. I have a lot of faith in science. I think we had the best 1000 scientists within

363
00:33:35,320 --> 00:33:40,920
five years, we would come up with a reasonably cost effective way to deal with carbon, we'd pull it

364
00:33:40,920 --> 00:33:47,160
out of the air in some way, shape or form. And we kind of like have the thing solved, or we know

365
00:33:47,160 --> 00:33:51,640
we're never going to solve it. I don't disagree with you, Steve, I think where I'm a little less

366
00:33:51,640 --> 00:33:58,120
optimistic than you, and I love the idea of a Manhattan project for this. I'd back up one step

367
00:33:58,120 --> 00:34:02,120
and reiterate what you said earlier, because I think it's so important. I phrase it in a somewhat

368
00:34:02,120 --> 00:34:07,640
different way, but I like how you discussed it, which is we confuse the objective, the strategy,

369
00:34:07,640 --> 00:34:13,160
and the tactics all day long. I mean, that to me is like, if we were going to list the 10 human

370
00:34:13,160 --> 00:34:18,120
failures, just as a species, it's that we are not wired to differentiate between objectives,

371
00:34:18,120 --> 00:34:24,520
strategies and tactics. And so that's why 10 years ago you were getting lambasted for talking

372
00:34:24,520 --> 00:34:30,360
about geoengineering, which is a tactic, because the only tactic that was being discussed

373
00:34:30,360 --> 00:34:35,480
was the reduction of carbon. And you were saying, well, wait a minute, if you really want,

374
00:34:36,280 --> 00:34:41,480
we've lost sight of the objective. The objective is to slow warming, and warming is a much bigger

375
00:34:41,480 --> 00:34:46,120
problem than just how much carbon is in the atmosphere that also deals with insulation.

376
00:34:46,680 --> 00:34:53,160
So let's look at different tactics, right? So notwithstanding that problem, I think my biggest

377
00:34:53,160 --> 00:35:01,080
fear, Steve, is there's no question that in five years a well-staffed, well-resourced team could

378
00:35:01,080 --> 00:35:06,920
come up with what solutions might look like. I just don't know that it could ever be implemented

379
00:35:06,920 --> 00:35:14,520
from a policy perspective. I mean, for example, I mean, I don't think anybody who's really studied

380
00:35:14,520 --> 00:35:23,160
energy seriously disagrees that nuclear energy, next gen, I'm not talking about like the nuclear

381
00:35:23,160 --> 00:35:28,520
reactors of 70 years ago. I don't think there's anybody who doesn't think that next gen nuclear

382
00:35:28,520 --> 00:35:34,600
would play an important role in electricity generation. But it's just not a politically

383
00:35:34,600 --> 00:35:40,200
palatable solution. I don't know where geoengineering falls on that spectrum either, for that matter.

384
00:35:41,480 --> 00:35:46,200
So that would be my fear is you'd come up with solutions, but then you'd still be at kind of a

385
00:35:46,200 --> 00:35:53,240
gridlock in the implementation of these things. Yeah, so that is a good point, and that is

386
00:35:53,240 --> 00:36:04,200
possible. I do think, however, that even in that world, there is a real insurance value

387
00:36:04,760 --> 00:36:10,760
in this investment now. So we might not be able to come up with a sensible policy now,

388
00:36:12,040 --> 00:36:18,360
but if in 20 years, all of a sudden, the ice shelf on Greenland just like begins to slide into the

389
00:36:18,360 --> 00:36:25,400
ocean and things are desperate, then I believe that we will, so my prediction is we will do

390
00:36:25,400 --> 00:36:30,760
something like my Manhattan Project for climate change. The question is, when will we do it? Will

391
00:36:30,760 --> 00:36:38,200
we do it as we're staring down catastrophe, or will we do it in advance and potentially either

392
00:36:38,200 --> 00:36:43,960
avoid catastrophe or at least get five years earlier? I mean, not that different than COVID,

393
00:36:44,600 --> 00:36:52,200
so for my podcast, I interviewed Mansif Slaoui, and I don't know, six or seven years ago,

394
00:36:52,200 --> 00:36:59,400
he proposed, well, why don't we have in the bank a vaccine for like everything we'd ever want,

395
00:36:59,400 --> 00:37:02,760
but not make very much of it, just like have a little bit around so we know how to do it.

396
00:37:03,800 --> 00:37:08,040
Look, and everyone laughed at him and said, how can you suggest that? That would cost

397
00:37:08,600 --> 00:37:15,320
500 million dollars. We can't waste 500 million dollars. Then you come to a world where the US

398
00:37:15,320 --> 00:37:22,440
government is bleeding six billion dollars a day in deficits, and that would have been like

399
00:37:22,440 --> 00:37:27,960
literally the best investment of all time probably if we had done it. So I think that kind of insurance

400
00:37:27,960 --> 00:37:32,280
policy I think would be valuable even in a world of gridlock and poor policy making.

401
00:37:32,280 --> 00:37:37,000
I agree completely because the one thing that I think we haven't stated here, but I think we both

402
00:37:37,000 --> 00:37:45,800
agree on is this is such an asymmetric risk that it has to be taken seriously. That's probably the

403
00:37:45,800 --> 00:37:50,600
thing that bothers me the most about the other argument, the people on the other side of the

404
00:37:50,600 --> 00:37:56,040
argument that say, well, we don't have enough certainty, therefore it should be status quo

405
00:37:56,040 --> 00:38:01,080
business as usual, ignore this. It's true. I think the uncertainty, the error bars on these

406
00:38:01,080 --> 00:38:08,840
projections are bigger than people lead us to believe, but it's so asymmetric. It's so

407
00:38:08,840 --> 00:38:12,200
non-linear if this goes wrong that you have to take that seriously.

408
00:38:13,880 --> 00:38:18,120
There are very few things in economics that shocked me, but something that shocked me was

409
00:38:18,840 --> 00:38:25,480
an analysis done a long time ago, this was 10 years ago and more, where an economist,

410
00:38:26,120 --> 00:38:33,080
and I'm embarrassed because I can't remember exactly who it was, just worked through the kinds

411
00:38:33,080 --> 00:38:38,920
of utility functions, so the basic models that economists use, and said in a world in which bad

412
00:38:38,920 --> 00:38:44,840
things happen, how much are you willing to pay to avoid bad things? And it is completely shocking.

413
00:38:46,680 --> 00:38:50,600
I'm making these numbers up, but I think they're in the right ballpark. So if you have something

414
00:38:50,600 --> 00:38:57,720
like a 1% chance of dying, how much will you pay? How much would you pay of your total wealth

415
00:38:57,720 --> 00:39:02,840
to avoid a 1% chance of dying today? Can't you know the answer isn't 1%, it's more like 10 or

416
00:39:02,840 --> 00:39:08,600
15%. And as it goes up to 5%, you're willing to pay like 50% of your wealth to avoid it, because

417
00:39:08,600 --> 00:39:13,160
once you die, you're like done, so it's like otherwise really long flow. And it's very

418
00:39:13,160 --> 00:39:17,640
counterintuitive. It really caught me off guard and it really changed my thinking about the

419
00:39:17,640 --> 00:39:21,240
importance of dealing with climate change. Because I was kind of of, look, you know,

420
00:39:21,240 --> 00:39:27,080
climate change is all in the future, we discount the future, you know, why worry about it? But once

421
00:39:27,080 --> 00:39:33,560
I saw that, it really changed my perspective on it. So I've come to think that this investment,

422
00:39:33,560 --> 00:39:42,120
and the other thing to stress is that the cost of R&D is so low compared to the cost of

423
00:39:43,080 --> 00:39:48,520
disaster and of implementation, that to do the R&D is just a no brainer, right? So I think for

424
00:39:49,560 --> 00:39:57,560
for a billion dollars a year, my guess is you could fund, you know, you could fund this Manhattan

425
00:39:57,560 --> 00:40:01,400
project for a billion dollars. It's like the kind of money that actual philanthropists could do,

426
00:40:01,400 --> 00:40:05,480
as opposed to even government having to do. Look, if you actually wanted to put whatever solution is

427
00:40:07,000 --> 00:40:12,040
discovered into practice, it would probably be enormously expensive. But to figure out the answer

428
00:40:12,360 --> 00:40:18,040
is just like trivial. And the same is true of COVID, like, you know, talking to people who did

429
00:40:18,040 --> 00:40:23,160
the vaccines, I think at Modan, it took them like a day or two to come up with the vaccine, you know,

430
00:40:23,160 --> 00:40:29,080
and all the costs are in the testing and the production. But the actual R&D part is so cheap

431
00:40:29,080 --> 00:40:34,920
compared to everything else that to not invest in it just seems like a real mistake, first order

432
00:40:34,920 --> 00:40:41,960
mistake to me. Agree. I'm glad we got off on that tangent, because I actually did want to talk

433
00:40:41,960 --> 00:40:47,720
about that at some point, but it came up, it came up kind of quick. But I want to now go back to

434
00:40:49,560 --> 00:40:54,120
the beginning of the story, right? So you graduate from college, you go off to work in

435
00:40:54,120 --> 00:41:01,800
management consulting, which if I remember, you described as slightly, only slightly less

436
00:41:01,800 --> 00:41:09,800
uncomfortable than a root canal without no vecane. It's so painful that it in fact, exposes you to a

437
00:41:09,800 --> 00:41:14,920
PhD. So you go off to MIT. So you basically just didn't leave Boston, you went from, you know,

438
00:41:14,920 --> 00:41:21,320
Harvard to consulting to MIT. So you really have a pension for these cold winters. And then what

439
00:41:21,320 --> 00:41:24,920
did you decide to study in your PhD? Because now you actually you got to get serious, right? You

440
00:41:24,920 --> 00:41:30,680
actually have to pick a problem and work on it. So I have to say, I didn't like consulting,

441
00:41:30,680 --> 00:41:37,400
but I learned something incredibly important in it. And I learned it from the guy who happened to

442
00:41:37,400 --> 00:41:40,680
have the cubicle next to me, a guy named Jeff Thomas, who I'm still friends with. And he taught

443
00:41:40,680 --> 00:41:44,840
me how to look at the world strategically. And it was weird. I never did that before. It never

444
00:41:44,840 --> 00:41:49,960
occurred to me to do it. But then I got to grad school. And I did have a fundamental understanding

445
00:41:50,600 --> 00:41:54,920
while everybody else was really busy working on problem sets. And essentially, these were all

446
00:41:54,920 --> 00:41:59,160
people who got in straight A's their whole life. And they figured, well, what worked for me before,

447
00:41:59,160 --> 00:42:02,520
that's what I got to keep on doing in grad school, because that's how I'm going to get ahead.

448
00:42:02,520 --> 00:42:07,560
And I looked at it. And partly it helped that I knew no math. And I was completely overmatched.

449
00:42:07,560 --> 00:42:12,360
And I was like the worst I was I was I'm not exaggerating when I say that my classmates

450
00:42:12,360 --> 00:42:17,000
sat down about a month into it. And my friend Austin Goolsby, who's now gotten famous as part

451
00:42:17,000 --> 00:42:21,240
of the Obama administration, later told me that they went through the list of people in the class

452
00:42:21,240 --> 00:42:26,600
and tried to decide who is least likely to succeed. And like I was a unanimous choice that I was going

453
00:42:26,600 --> 00:42:34,280
to be the worst one. And but look, I had a great approach, which is I understood that you had to

454
00:42:34,280 --> 00:42:39,880
create research had to go from being a consumer of knowledge to a producer of knowledge. And,

455
00:42:41,160 --> 00:42:47,160
and, you know, and I also was kind of still, you know, I have to say, I didn't really, I wasn't

456
00:42:47,160 --> 00:42:52,680
internally motivated. So what I did was I said, well, look, who's at the top of the hierarchy,

457
00:42:52,680 --> 00:42:58,360
I might as well try to be that guy. And it turned out that that was macro economics at MIT at the

458
00:42:58,360 --> 00:43:04,840
time. So for the first semester, I tried to be a macro economist. And it became so clear to me so

459
00:43:04,840 --> 00:43:09,320
quickly that I was never going to be a macro economist, because I always thought that nobody

460
00:43:09,320 --> 00:43:13,720
had intuition for macro economics until I started talking to some of the people at MIT who actually

461
00:43:13,720 --> 00:43:19,480
understood, oh, the exchange rate between the the euro and the dollar goes up, then that's going

462
00:43:19,480 --> 00:43:23,400
to trickle through and how's that going to affect, you know, immigration like, oh, my god, like, it

463
00:43:23,400 --> 00:43:27,720
made no sense to me. So I quit, I was smart enough, I've always been smart enough to fail quickly. So

464
00:43:27,720 --> 00:43:33,000
I like realized I couldn't be a macro economist. The theorists were number two. And so I tried to

465
00:43:33,000 --> 00:43:36,200
be a theorist, and I actually spent a little longer being a theorist, actually wrote a couple

466
00:43:36,200 --> 00:43:41,720
papers being a theorist. And again, it became really clear to me that I didn't have what it

467
00:43:41,720 --> 00:43:47,400
took to be a theorist. So then I said, well, kind of the only thing left is doing data analysis. And

468
00:43:47,400 --> 00:43:50,760
it turned out I should have been smart enough to know that from the beginning, because I was a

469
00:43:50,760 --> 00:43:58,680
weird kid. I, I, the only thing I like to do was sit in my room, and essentially study data, like,

470
00:43:58,680 --> 00:44:06,440
as I look back on it, I would, this is how weird I was when I was, when I was maybe eight,

471
00:44:08,120 --> 00:44:14,440
I asked for and received a pocket calculator as my birthday present. And it was overjoyed. When I

472
00:44:14,440 --> 00:44:23,240
was maybe like nine, I graduated to a scientific calculator. And my pastime was I was into baseball,

473
00:44:23,240 --> 00:44:28,520
is I would go through and I would, by manually typing into a scientific calculator, I type in

474
00:44:28,520 --> 00:44:34,600
a column of, of like wins for a team. And then one by one, I would type in each of the statistics,

475
00:44:34,600 --> 00:44:40,520
like the team batting average or the team, you know, number of triples, and compute the partial

476
00:44:40,520 --> 00:44:44,920
correlation between those two. And, and, and what my dad wanted, my dad came home from work,

477
00:44:44,920 --> 00:44:48,920
he would ask me what the correlations were between the variables I computed. Like,

478
00:44:48,920 --> 00:44:53,240
that was how I spent my time. So it should have been obvious to me, that's what I want to do.

479
00:44:53,240 --> 00:44:59,560
And the other thing I did was when I was in is I, I would use data to try and answer real problems.

480
00:44:59,560 --> 00:45:03,640
Like when I was in college, I became obsessed with the racetrack. And I would try to, you know,

481
00:45:03,640 --> 00:45:07,320
use data to win at the racetrack, which wasn't very significant. But like, I should have been

482
00:45:07,320 --> 00:45:12,840
obvious to me that I, that I should have gravitated to, to, to doing empirical research. But,

483
00:45:12,840 --> 00:45:16,440
but I didn't, because I was still caught up in this idea of, well, of a status. So finally,

484
00:45:16,440 --> 00:45:25,160
I just said, look, I'll do what I'm good at. But, but honestly, I was so far behind the people

485
00:45:25,160 --> 00:45:29,080
around me. It was an interesting experience. It's kind of a satellite, but it's interesting.

486
00:45:29,800 --> 00:45:35,400
I'd been used to being really good at most of the things I did academically. And,

487
00:45:35,560 --> 00:45:40,200
and it happens for almost everyone. You, you, you some, you get to a point where you look around

488
00:45:40,200 --> 00:45:45,160
and you realize you were the dumbest person in the room and how you deal with that is really

489
00:45:45,160 --> 00:45:49,960
important. And strangely, and I don't know why that was roughly the greatest feeling I've ever

490
00:45:49,960 --> 00:45:54,440
had in my life. When I looked around and I was the dumbest person in the room, it was just sense

491
00:45:54,440 --> 00:45:59,640
of joy. It was really interesting. Somehow I think maybe I had felt pressure my whole life.

492
00:46:00,600 --> 00:46:07,000
Once I was a dumbest person in the room, I felt like, wow, I can do whatever I want. Like I don't,

493
00:46:07,000 --> 00:46:11,240
you know, I can, it's just, it was such a joy to be around such talented people.

494
00:46:11,240 --> 00:46:16,760
And it freed me up to be myself. And instead of always trying to follow the crowd, I started

495
00:46:16,760 --> 00:46:21,480
doing what I liked and what did I, and I thought, well, what do I like? And I said, well, I don't

496
00:46:21,480 --> 00:46:26,600
know. I like watching the TV show cops. I love to show cops. I watch it every day. Like, well,

497
00:46:26,600 --> 00:46:32,760
why don't I do research on it? And so I somehow like found myself and I began just studying the

498
00:46:32,760 --> 00:46:39,240
things I liked and, and much to my amazement, even though no other economists were really studying

499
00:46:39,240 --> 00:46:45,720
those questions, I really thought it was a, it was a one way ticket to, to like getting a PhD

500
00:46:45,720 --> 00:46:50,520
and then doing something outside of academics. Because, you know, why would it be the case that

501
00:46:50,520 --> 00:46:55,320
if these were topics, no one else researched, they'd be interested in. But I didn't, you know,

502
00:46:55,320 --> 00:46:59,560
I knew I couldn't compete on a level footing with these amazing people around me. So I had to

503
00:46:59,560 --> 00:47:04,680
compete on like in my own space. And in my shock, people liked it, you know, and they were excited

504
00:47:04,680 --> 00:47:10,520
about it. It got published in good journals. And, and I'm, I honestly, I look back and I say, I'm so

505
00:47:10,520 --> 00:47:18,280
lucky to have been so far below the bar that I didn't try to, I didn't try to do what I think

506
00:47:18,280 --> 00:47:22,680
most people do and what I had a tendency to do, which was to try to be like, figure out what I

507
00:47:22,760 --> 00:47:26,520
should be, because that's what other people are. And I just was myself and it was such a great

508
00:47:26,520 --> 00:47:31,240
lesson. And I've really practiced being myself ever since. Looking sometimes it works and sometimes

509
00:47:31,240 --> 00:47:35,560
it doesn't, but I, but the great thing about life is when things don't work, you can take a different

510
00:47:35,560 --> 00:47:42,280
path. And so it's just, you know, so I just try to be myself and, and I stopped doing stuff when

511
00:47:42,280 --> 00:47:49,080
being myself isn't a good recipe for it. So when you got to Chicago, I mean, that must have been

512
00:47:49,160 --> 00:47:53,480
another difficult decision because Chicago is a powerhouse in economics, right? You could have

513
00:47:53,480 --> 00:48:01,400
taken your sort of offbeat approach to economics and gone someplace that is where you're not going

514
00:48:01,400 --> 00:48:08,600
to be surrounded by a bunch of Nobel laureates. So why did you, I mean, were you so secure in your

515
00:48:08,600 --> 00:48:14,200
kind of off the beaten path approach that you said, Hey, I might as well go to another cold winter

516
00:48:14,200 --> 00:48:18,440
city and be around smart people, but I, but not have to compete with them. I mean, that,

517
00:48:18,440 --> 00:48:25,000
what was the, what was the decision like there? You know, so I had a lot of choices

518
00:48:26,200 --> 00:48:32,840
and I loved Cambridge and I had been at Harvard and MIT and, and I really, what had happened is

519
00:48:32,840 --> 00:48:40,040
I'd given a couple of, of, of presentations at Chicago and I had been shocked at how different

520
00:48:40,040 --> 00:48:47,800
the questions I got were. And I, I really went to Chicago because I wanted to get to know the enemy.

521
00:48:47,880 --> 00:48:53,640
At that time, so, so Gary Becker was one of the most influential economists of the, of,

522
00:48:53,640 --> 00:48:59,000
of the last 50 years. And he's since passed away, but was a real mentor to me. He was demonized. I

523
00:48:59,000 --> 00:49:02,360
mean, I think when I first met him, I expected him to be a monster and it turned out, well,

524
00:49:02,360 --> 00:49:08,360
he wasn't, he was super smart and he asked great questions. And, and so I, I very self-constantly

525
00:49:08,360 --> 00:49:14,440
said, I'm going to go to Chicago and I'm going to get to know the enemy from the inside for a couple

526
00:49:14,440 --> 00:49:20,360
of years before I go back to Harvard. And what I hadn't anticipated is that the thinking that goes

527
00:49:20,360 --> 00:49:26,680
on at Chicago, it looks like it's really simple. It looks like one could learn it. I thought, look,

528
00:49:26,680 --> 00:49:33,960
I'm good at economics. I can learn this, but I, I take even 25 years later, I'm still not very good

529
00:49:33,960 --> 00:49:40,440
at the kind of deep thinking that is done in Chicago economics. And, and I love it. And I

530
00:49:40,520 --> 00:49:45,400
think it's powerful, but I'm still more of a spectator than an actual doer of it. And I really

531
00:49:45,400 --> 00:49:53,320
got, I really got co-opted into the spirit of Chicago. But in some sense, I will, I will say

532
00:49:53,320 --> 00:50:00,920
that I've never, I've always been probably unreasonably overly confident in, in my own

533
00:50:01,640 --> 00:50:07,400
ability. And, and I was very, I always had ease. Like I was going to do what I was going to do

534
00:50:07,400 --> 00:50:11,480
and I thought I could do it better at Chicago than anywhere else. But it was, but I will say

535
00:50:11,480 --> 00:50:21,000
it was, it was unusual. So at that time, I think it must have been seven or 10 years since someone

536
00:50:21,000 --> 00:50:26,840
who had a lot of options had chosen to go to Chicago economics over another place, including

537
00:50:26,840 --> 00:50:31,720
the Chicago business school, which was really getting most of the town. And, and one of my

538
00:50:31,800 --> 00:50:38,520
advisors, so I told one of my advisors at Harvard, I was going to go to Chicago. And he said,

539
00:50:41,720 --> 00:50:47,480
if you do that, I will never speak to you again. And I said, why? And he said, because if you do

540
00:50:47,480 --> 00:50:53,640
that, it shows that you are so effing stupid that you're not worth me wasting my breath. And it was

541
00:50:53,640 --> 00:50:59,240
interesting. That was how weird it was to do that decision. But I did it. And he was happy to talk

542
00:50:59,240 --> 00:51:02,680
with me, you know, never stopped talking to me. And I think he would agree. I probably did the

543
00:51:02,680 --> 00:51:09,000
right thing. But, but, but, you know, it's funny now, because it used to be there were these

544
00:51:09,000 --> 00:51:13,480
enormous differences between departments. Now I think they're all kind of the same Chicago, Harvard,

545
00:51:13,480 --> 00:51:17,640
MIT, Stanford, Princeton. I mean, they're, yeah, they're like all these great departments. We've

546
00:51:17,640 --> 00:51:22,520
all like blurred the lines together. Not so different, but I, you know, but, but at the time,

547
00:51:22,520 --> 00:51:28,360
there really was a distinctive personality that for me was really, you know, really special. I'm

548
00:51:28,360 --> 00:51:34,520
so glad I did it. And I think it's, I've been a huge beneficiary of it. Do you think the field

549
00:51:34,520 --> 00:51:42,680
is better that the elite 10 programs have become more homogeneous? Or do you think it produced,

550
00:51:42,680 --> 00:51:47,320
did more for the field when you had these very distinctive schools of thinking and Stanford had

551
00:51:47,320 --> 00:51:53,880
its way of doing it and Chicago had its way and Harvard had its way? It's a great question. Such

552
00:51:53,880 --> 00:52:03,560
a hard, such a hard question. I remember, I remember maybe five for 10 years into my coming

553
00:52:03,560 --> 00:52:12,840
to Chicago, Milton Friedman came back and he was bemoaning the fact not just that Chicago was

554
00:52:12,840 --> 00:52:16,840
becoming more like other places, but really that what was distinctive about Chicago, what's called

555
00:52:16,840 --> 00:52:23,160
Chicago price theory, was basically dying out and how awful that was. And one of my colleagues,

556
00:52:23,160 --> 00:52:27,960
super smart colleagues, Casey Mulligan said, Milton, I thought you believed in market.

557
00:52:28,840 --> 00:52:33,320
It sounds to me like, like price is losing. And I thought, wow, that is exactly right.

558
00:52:34,280 --> 00:52:38,920
And even Milton Friedman in the end didn't really believe in markets when markets moved against him.

559
00:52:39,800 --> 00:52:46,920
I don't know. I do think. Like it sort of used to be that way in medicine, right? There was a

560
00:52:47,000 --> 00:52:54,280
very clear line between East Coast medicine and West Coast medicine. And there was a very

561
00:52:54,280 --> 00:53:01,240
different way cardiac surgery was done in Minnesota versus Stanford versus Boston. I mean,

562
00:53:01,240 --> 00:53:07,960
those were so different and they produced remarkable innovations in a sense. And I just,

563
00:53:07,960 --> 00:53:14,920
you know, so it. Yeah. No, I think you're right. Like letting, I think these diversity of approaches

564
00:53:14,920 --> 00:53:19,240
are useful because the number one, there's data, right? It's like, let's, let's especially take

565
00:53:19,240 --> 00:53:23,160
more something like cardiac surgery where you get real data. Look, you know, whether people are

566
00:53:23,160 --> 00:53:27,480
living or dying. And so you actually can figure out whether one is better than the other. So in

567
00:53:27,480 --> 00:53:32,520
that world, for sure, I think this diversity of views is really, really important. Now you might

568
00:53:32,520 --> 00:53:36,360
say, look, it doesn't have to be across departments. It can be within department, right? You have one

569
00:53:36,360 --> 00:53:42,040
brilliant surgeon innovating in this way or that way. But I do think in general that it's this,

570
00:53:43,000 --> 00:53:50,120
this diversity of views is good and it's especially good in a dynamic world, right? Because in a world

571
00:53:50,120 --> 00:53:56,280
that's static, it's kind of like doesn't matter as much. But when something radical happens,

572
00:53:56,920 --> 00:54:03,480
then often one model is much better situated than the other to deal with whatever this radical

573
00:54:03,480 --> 00:54:08,600
occurrence happens. Some new disease or some, I don't know, maybe, maybe radical things tend not

574
00:54:08,680 --> 00:54:12,120
to happen so much in medicine, but you know, sometimes radical things happen in the economy.

575
00:54:12,840 --> 00:54:20,760
And so I think it's, it's important. Again, I don't know how interested your listeners are

576
00:54:20,760 --> 00:54:27,080
in macroeconomics, but I think macroeconomics is a case where Chicago style macro more or less one.

577
00:54:27,080 --> 00:54:33,320
And so all of the world of macro now looks a lot like what was going on in Chicago 40 years ago.

578
00:54:33,960 --> 00:54:40,520
And I think that's been a problem because I think there isn't this diversity of thinking that puts

579
00:54:40,520 --> 00:54:47,160
you maybe in a better situation as different kinds of macroeconomic problems arise to have

580
00:54:47,800 --> 00:54:55,240
a range of possible approaches and solutions to it. So yeah, I think, so here's that. I think it

581
00:54:55,240 --> 00:55:01,720
probably would be better if there were greater differences between the departments, but the,

582
00:55:02,360 --> 00:55:07,480
but it's one of these things where like the facts of life are that it's impossible to maintain that

583
00:55:07,480 --> 00:55:12,760
equilibrium because there are all sorts of private forces that are pushing, pushing for this

584
00:55:13,640 --> 00:55:19,000
homogeneity that you just can't, so like, you just, like, there's no easy way to fight it

585
00:55:19,000 --> 00:55:23,240
in some sense. And so even though you wish it would happen, it's hard to see how to make it happen.

586
00:55:24,600 --> 00:55:30,520
So let's, let's go on to talk a little bit about a colleague of yours outside of the world of

587
00:55:30,520 --> 00:55:39,720
economics, Steve Dubner. How did you guys meet? So Dubner approached me to write an article about

588
00:55:39,720 --> 00:55:46,600
me for the New York Times. This is after I'd won the Clark medal, but well before free economics.

589
00:55:47,320 --> 00:55:56,200
And, and I really was incredibly hesitant to, to accept the, the invitation because I just didn't,

590
00:55:56,200 --> 00:55:59,960
I didn't really like to be written about, there was no, I didn't have anything to sell,

591
00:55:59,960 --> 00:56:04,760
so I didn't really, I wasn't in the business of trying to market myself. And, and in the end,

592
00:56:04,760 --> 00:56:10,120
I have this, my mom though, really likes it when I'm in the newspaper and on TV and stuff. And so

593
00:56:10,120 --> 00:56:14,440
I really, I remember thinking, Oh God, I'll take this hit from my mom. Cause it will make my mom

594
00:56:14,440 --> 00:56:21,080
so happy if there's a piece about me in the New York Times magazine. And, and then Dubner came out

595
00:56:21,080 --> 00:56:27,560
and it was unlike any experience I had ever had with a journalist. I honestly think he had read

596
00:56:27,560 --> 00:56:33,720
every academic paper I'd ever read. I mean, I'm talking about like 50 papers and he came out and

597
00:56:33,720 --> 00:56:40,680
he ended out interviewing me for, I don't think I'm exaggerating if I say maybe 25 hours over three

598
00:56:40,680 --> 00:56:48,520
days. And there was never a silence. So I would, he would ask me a question. I would answer it.

599
00:56:49,080 --> 00:56:54,040
And as soon as I answered it, he would ask me another question. And this went on for 25 hours

600
00:56:54,040 --> 00:56:58,360
and it was unbelievably painful to me because it's like, it was the last thing I wanted to do.

601
00:56:58,360 --> 00:57:03,080
And I thought he was going to say for three hours, not 25. And, and one of the notable things about

602
00:57:03,080 --> 00:57:10,680
it is that I literally did not ask him a question for the first 24 and a half hours. And I only

603
00:57:10,680 --> 00:57:17,480
thought of him as like this, this parasite that was like sucking the life out of me. And after 24

604
00:57:17,480 --> 00:57:22,280
and a half hours, I actually had the thought to say, well, like, who have you written about before?

605
00:57:22,280 --> 00:57:26,520
And he started telling me these fantastic stories. He was the only guy who had been able to interview

606
00:57:26,520 --> 00:57:31,080
the Unabomber and he had been in a rock band like, and he was really interesting. And I,

607
00:57:31,080 --> 00:57:35,480
and I learned something there, which is like, especially when you're tired of being asked

608
00:57:35,480 --> 00:57:39,880
questions, ask the questions yourself. It's almost always more interesting to ask the questions than

609
00:57:39,880 --> 00:57:45,560
to answer them. But still we parted and I would have said I would never see that guy again. I mean,

610
00:57:45,560 --> 00:57:50,760
we were not friendly in any way. There was no like meeting of the minds or anything.

611
00:57:51,880 --> 00:57:57,000
But he did write a piece about me in the New York times that people loved. And he gets so pissed at

612
00:57:57,000 --> 00:58:02,040
me because I always say like he created this, this personality about me as this incredible

613
00:58:02,040 --> 00:58:07,400
wonder boy genius who you give me a problem. I type away at my computer. I solve it an hour later.

614
00:58:07,400 --> 00:58:12,200
And, and people love that persona. And even though it was completely and totally off, I mean,

615
00:58:12,200 --> 00:58:17,880
it wasn't right at all. But, but like, I've been writing that for the last 15 years. So I can't

616
00:58:17,880 --> 00:58:24,200
complain. Like I've, I've milked it for everything I could. And people loved it. And then we ended

617
00:58:24,200 --> 00:58:29,320
up, you know, not, you know, not through an easy Bible. We ended up deciding to write Freakonomics

618
00:58:29,320 --> 00:58:34,040
together, not because we had some passion to tell our story, but just because we both wanted to

619
00:58:34,040 --> 00:58:41,080
make some bucks and, and the publishers were willing to pass to write this book. And, and in

620
00:58:41,160 --> 00:58:46,920
the end, we had an amazing agent, Suzanne Gluck, who I interviewed for my podcast,

621
00:58:46,920 --> 00:58:51,560
people I mostly admire. And like, she, you know, we relive that story. But she just like totally

622
00:58:51,560 --> 00:58:58,840
out negotiated the publishers. And there we were with a super lucrative book deal.

623
00:58:58,840 --> 00:59:02,920
And no idea at all what we're going to write about. We like had no conception of what this

624
00:59:02,920 --> 00:59:08,200
book was going to be. And I think part of the fun of it was that we both assumed that we had just

625
00:59:08,200 --> 00:59:13,720
pulled off like a bank heist and that like no one was ever going to read this book. And so we

626
00:59:13,720 --> 00:59:19,640
could do whatever we wanted, because it wasn't like my colleagues cared what I wrote about this

627
00:59:19,640 --> 00:59:24,680
book that no one would read. And he was like a serious, you know, memoirist and his, you know,

628
00:59:24,680 --> 00:59:27,960
people would understand he was just doing this to make some cash and they wouldn't hold it against

629
00:59:27,960 --> 00:59:32,520
them. And it freed us up, I think, to write a book that was very different than what we would have

630
00:59:32,520 --> 00:59:37,000
written if we actually had the fear that people were going to read it and judge us based on, on,

631
00:59:37,080 --> 00:59:42,120
on what we wrote. So we had a lot more fun with them. We would have otherwise. And we kind of

632
00:59:42,120 --> 00:59:46,920
broke a bunch of rules and we were super lucky, I think, to be in the right place at the right time

633
00:59:47,480 --> 00:59:51,240
to have a book that ended up selling a whole bunch of copies.

634
00:59:51,240 --> 00:59:53,160
Did it come out in 05 or 06?

635
00:59:55,000 --> 00:59:58,840
Oh, I don't know. I mean, I, I don't know. I mean, no idea.

636
00:59:58,840 --> 01:00:03,400
I'm so good at dates. Usually I can almost, I think probably, I mean, I remember reading it as

637
01:00:03,400 --> 01:00:09,640
soon as it came out. I still remember where I first learned about it. I remember, it's, it's,

638
01:00:09,640 --> 01:00:14,440
it's so odd that I would remember this, but I was in the OR waiting for a patient to wake up,

639
01:00:15,000 --> 01:00:18,920
you know, waiting to come out of anesthesia. So, you know, we'd finished operating and it was,

640
01:00:18,920 --> 01:00:22,360
you know, I was just sort of writing the orders to get ready to take the patient to

641
01:00:22,360 --> 01:00:30,200
the recovery room. And the chief resident said, I just read this book for economics.

642
01:00:30,200 --> 01:00:36,120
You got to read it. It is unbelievable. It's totally incredible. And he just started raving

643
01:00:36,120 --> 01:00:43,400
about it. And so I just went and picked it up and I couldn't put it down. I mean, it was just,

644
01:00:43,400 --> 01:00:49,000
and it was very unusual for me to read anything outside of medicine. You know, it was very hard

645
01:00:49,000 --> 01:00:54,760
for me to make time to do anything that wasn't immediately related to sort of what I needed to,

646
01:00:54,920 --> 01:01:02,360
to do for work. What, so I'm trying to think which my favorite one was in there.

647
01:01:04,920 --> 01:01:09,640
Which one did you guys get the most blowback on? Was it the seat belt one, the car seat one? Did

648
01:01:09,640 --> 01:01:15,480
that, did that create the most blowback? You know, we literally had like three lines on, on car seats.

649
01:01:15,480 --> 01:01:21,080
We had one, one paragraph on car seats and that probably did create about as much negative

650
01:01:21,080 --> 01:01:25,960
feedback as anything. Strangely, we really thought abortion was going to be the lightning rod and

651
01:01:25,960 --> 01:01:31,160
everyone was going to get upset about it, but abortion and crime. But, but it turned out that

652
01:01:31,160 --> 01:01:37,880
when we actually, well, the backstory is that when, when I couldn't control the story, like when the

653
01:01:37,880 --> 01:01:43,560
media reported on my academic results about abortion and crime, which by the way, that was,

654
01:01:43,560 --> 01:01:48,440
that was in the late nineties, wasn't it? Or early 2000s, but the actual 2000, 2001. Yeah.

655
01:01:48,440 --> 01:01:54,040
And that, in that decade. And, and, and we had no control over the way it was portrayed. It was

656
01:01:54,040 --> 01:01:59,480
deeply mis-portrayed. It was simplified and it was a, it was a nightmare. It just, it, it triggered

657
01:02:00,520 --> 01:02:07,720
hostility from all corners of the, of the right and the left. And, and it just kind of made us

658
01:02:07,720 --> 01:02:14,520
look stupid because it was easy to parody it because if you didn't understand how simple and

659
01:02:14,520 --> 01:02:18,120
obvious things we were talking about and how powerful they were in the data, we kind of looked

660
01:02:18,120 --> 01:02:23,720
like we were crazy people and in part because nobody talks publicly about abortion unless they

661
01:02:23,720 --> 01:02:30,840
have a, a stake in it, right? So all of the conversation about abortion is either stridently

662
01:02:30,840 --> 01:02:35,240
pro-life or stridently pro-choice, but like we didn't have a stake. We were just doing something

663
01:02:35,240 --> 01:02:40,760
really different, which is, we said something simple, which is the abortion was legalized in the

664
01:02:40,760 --> 01:02:46,760
U S and it turns out that there's this really strong relationship between unwantedness. So if

665
01:02:46,760 --> 01:02:53,640
a kid is unwanted by his or her parents that they tend to have a hard life. And after abortion was

666
01:02:53,640 --> 01:02:58,120
legalized, there appears to have been a dramatic decline in number of unwanted children that were

667
01:02:58,120 --> 01:03:03,000
born. And so just like in a simple empirical like statement of fact, fewer unwanted children with

668
01:03:03,000 --> 01:03:08,600
legalized abortion should lead to less crime. 18 years later, you know, it's, it's like gotta be

669
01:03:08,600 --> 01:03:13,400
true. It's not hard to see why that's true. It's just a matter of is it a big number or a small

670
01:03:13,400 --> 01:03:17,960
number. And, and back in the unbelievable calculations, Sota said it could be a really

671
01:03:17,960 --> 01:03:23,880
big number based on other studies of unwantedness. And when you looked at the data, not perfect,

672
01:03:23,880 --> 01:03:27,720
we obviously didn't want to randomize experiment. We didn't get to choose who didn't didn't get

673
01:03:27,720 --> 01:03:33,720
abortions. We looked at, you know, things like, you know, after Roe versus Wade happened, you

674
01:03:33,720 --> 01:03:37,960
look exactly at some states, it was easy to get abortion. Some states it was hard. And then you

675
01:03:37,960 --> 01:03:42,920
look 20 years later and you see that well, for the first 15 years, the crime patterns look really

676
01:03:42,920 --> 01:03:46,520
similar between those states. And they only started to diverge once the kids who were

677
01:03:46,520 --> 01:03:51,640
exposed to legalized abortion were old enough to have, you know, start killing crimes. So like,

678
01:03:51,640 --> 01:03:56,360
you know, it was a really simple, straightforward paper that almost anyone could look at the data

679
01:03:56,360 --> 01:04:02,280
and make some sense of whether what we were doing was reasonable and sensible. And so when we

680
01:04:02,280 --> 01:04:09,080
actually got to tell our story in free economics, nobody complained. And what was really interesting,

681
01:04:09,400 --> 01:04:14,760
not just from a data perspective, but from a storytelling perspective, is it was one of the

682
01:04:14,760 --> 01:04:21,080
great successes of storytelling in that people who were pro choice read that chapter and would

683
01:04:21,080 --> 01:04:26,840
pat me on the back and say how amazing it like what a great chapter that was and how it pushed

684
01:04:26,840 --> 01:04:31,720
the pro choice agenda. And people who are pro life would slap me on the back and say, you know,

685
01:04:31,720 --> 01:04:36,280
I'm glad someone finally told the truth about pro life. And it was interesting, the exact same

686
01:04:36,280 --> 01:04:41,320
chapter was read totally different. And everybody liked it. It was, I gotta say, of all the things

687
01:04:41,320 --> 01:04:47,320
I've ever done, that that was one of the weirdest and most unexpected things that ever happened,

688
01:04:47,320 --> 01:04:53,320
is that we wrote something that everybody liked when we expected everybody to hate it. So I

689
01:04:53,320 --> 01:04:58,760
honestly, we didn't get so free economics, we wrote all these stories, and people just liked it. And

690
01:04:58,760 --> 01:05:04,680
we, you know, we kind of had this, like, we offended lots of groups, like we talked about

691
01:05:04,680 --> 01:05:10,840
how real estate agents were like the KKK. But like in a lighthearted way that even most of the

692
01:05:10,840 --> 01:05:16,360
real estate agents didn't get that mad at us. And I got invited to speak at the National Realtors

693
01:05:16,360 --> 01:05:20,600
Association. So it was like, everyone's like a good sport about it. Then we wrote a second book,

694
01:05:21,320 --> 01:05:26,680
people got super pissed off, because I guess we hit a little closer to home with stuff like climate

695
01:05:26,680 --> 01:05:31,960
change. Then we wrote a third book. And by that time, we had alienated everyone. It turns out that

696
01:05:32,200 --> 01:05:36,440
there was someone- It's the third one's think like a freak, right? Yeah. So by that time,

697
01:05:37,720 --> 01:05:42,600
every person on the right and the left thought we were jerks by the time we were done with the third

698
01:05:42,600 --> 01:05:47,000
one, which is more or less why we stopped writing books, because we had gone from, oh, these are

699
01:05:47,000 --> 01:05:51,160
these fun-loving economists who poke punted everything to like, those jerks, they're like,

700
01:05:51,160 --> 01:05:55,720
offended me deeply, and I'm never going to buy another book from them again. And then remember,

701
01:05:55,720 --> 01:06:02,680
one night we were having dinner, this was at my house in San Diego years ago, maybe six or seven

702
01:06:02,680 --> 01:06:08,200
years ago. And I think you were toying with the idea of writing a fourth book about like the

703
01:06:08,200 --> 01:06:13,480
Freakonomics approach to golf. And you really spent the entire night trying to convince me to

704
01:06:13,480 --> 01:06:18,200
be the protagonist of that book. Do you remember this discussion? Oh my God, yeah. So I actually

705
01:06:18,200 --> 01:06:26,920
wrote large chunks of a book about the Freakonomics of golf and liked it and enjoyed doing it because

706
01:06:26,920 --> 01:06:37,000
I love and loved golf and I wasn't that good at it, but at the age of 40, I really, in a semi-serious

707
01:06:37,000 --> 01:06:41,720
way, dedicated myself to becoming a professional golfer and never got nearly good enough, but

708
01:06:41,720 --> 01:06:47,080
improved a lot. But I knew I couldn't do it based on physical talent. I knew I had to do it based on

709
01:06:47,080 --> 01:06:54,200
being smart and using data in a particular way. And so I've developed all of these sets of tools

710
01:06:54,200 --> 01:07:00,920
that I think could be really helpful to a golfer. And what I needed was I needed someone who had

711
01:07:00,920 --> 01:07:08,120
amazing physical talent and was a complete maniac in the sense that I know you are, I'm like willing

712
01:07:08,120 --> 01:07:14,040
to put 10,000% into anything that you started. And I thought, well, what a great chapter would be to

713
01:07:14,040 --> 01:07:20,760
take someone who's never golfed before and to see how good you could get, that person could get in

714
01:07:20,760 --> 01:07:25,880
a year. And you were my number one, you were my number one student. We talked about it seriously.

715
01:07:25,880 --> 01:07:30,440
I mean, I really did consider it just out of curiosity, which was if I devoted one year to

716
01:07:30,440 --> 01:07:36,120
this, like I had devoted one year to swimming, like how far could you get in a year if you were

717
01:07:36,120 --> 01:07:42,280
willing to put in two to three hours a day, a very deliberate practice, especially with your

718
01:07:42,280 --> 01:07:48,520
guidance, which as I understood it was going to make a lot of shortcuts. But in the end, I think

719
01:07:48,520 --> 01:07:53,640
my fear was just the addiction that would come of that. Like golf, golf is one of those things that

720
01:07:53,640 --> 01:07:59,400
just, it could consume someone like me. It would be a terrible waste of your talent as well. So I

721
01:07:59,400 --> 01:08:10,840
only did, I did eventually take on a one student and it's a funny story. So Larry Summers, who was

722
01:08:10,920 --> 01:08:17,400
the secretary of the treasury and the president of Harvard and whatnot. So he came to Chicago one

723
01:08:17,960 --> 01:08:26,600
one time to give a speech. And this is not too many years ago when he was, was and is incredibly

724
01:08:26,600 --> 01:08:36,360
eminent. And I got an email that was addressed to like five Nobel laureates, the president of the

725
01:08:36,360 --> 01:08:45,000
university and to me, demanding our presence when he comes to entertain him or, you know,

726
01:08:45,000 --> 01:08:50,360
that like, I don't know. So he could, he could hold court or whatever. And I, and I was completely

727
01:08:50,360 --> 01:08:58,280
befuddled at how I got on this list of these folks. So I showed up and I was maybe the fourth one to

728
01:08:58,280 --> 01:09:02,280
arrive and he was deep in conversation with a couple of our Nobel laureates and the president.

729
01:09:03,080 --> 01:09:08,440
And he literally like stopped. I walked in the room and he like broke off the conversation. And

730
01:09:08,440 --> 01:09:13,080
he said, sorry, gentlemen, this is who I need to talk to. And I'm like, wow, like my stature and

731
01:09:13,080 --> 01:09:18,200
the profession of economics is really going up. And he says, Steve, I'm this, I don't even really

732
01:09:18,200 --> 01:09:22,840
know him. I've talked to him like twice in my entire life. He says, Steve, I have been needing

733
01:09:22,840 --> 01:09:27,960
to talk to you. I'm like, wow, I'm like, what, what, what am I going to tell Larry Summers? And he says,

734
01:09:28,680 --> 01:09:34,680
I've heard that you're the one guy in the world who can get my handicap from 20 down to five in

735
01:09:34,680 --> 01:09:41,320
golf. How are we going to do that? And it was hilarious that the thing he cared about was golf.

736
01:09:41,320 --> 01:09:46,440
And so I had failed with you, Peter. I thought, okay, what a great chapter this will be. I'm

737
01:09:46,440 --> 01:09:53,880
taking Larry Summers from a 20 handicap down to a five. And he turned out to be the worst student

738
01:09:53,880 --> 01:10:00,200
that ever existed in the planet. He would, he like wouldn't do anything. Like he, I sent him all my

739
01:10:00,200 --> 01:10:05,160
stuff. We worked out, you know, we were going to work out plans for talking and like he wouldn't

740
01:10:05,160 --> 01:10:10,200
practice. It was, he was the exact opposite. Like the reasons I wanted you to be my student,

741
01:10:10,200 --> 01:10:15,800
were the exact reasons that Larry turned out not to be a very good student. Let's hold on to this.

742
01:10:15,800 --> 01:10:19,400
There may be a day, you know, I don't know, maybe when my kids are in college or something,

743
01:10:19,400 --> 01:10:22,680
and I have a little bit more bandwidth, I might be willing to pick this up. But I,

744
01:10:23,480 --> 01:10:30,680
you have me very intrigued by this. The title Freakonomics, you got that from your sister, right?

745
01:10:32,120 --> 01:10:38,200
I did. So my sister, who unfortunately has since passed away from cancer, she was,

746
01:10:39,720 --> 01:10:47,960
she was a force of nature. She, she was, you know, it's funny, when you grow up, you're around a

747
01:10:47,960 --> 01:10:53,400
relatively small set of people and you kind of gauge your own stature in the world relative to

748
01:10:53,400 --> 01:11:01,560
people around you. And my sister was so amazing that I always thought I must be like below average

749
01:11:01,560 --> 01:11:12,040
in terms of creativity and things because like relative to her, I was awful. And, and so,

750
01:11:12,760 --> 01:11:18,520
but, but she was also odd in the sense that she didn't, she didn't really function that well in

751
01:11:18,520 --> 01:11:25,080
the world. Like she wasn't that successful in traditional metrics, because she was kind of,

752
01:11:25,080 --> 01:11:29,640
she was a little bit like Robin Williams. Like, like the person most like her that I've ever,

753
01:11:29,640 --> 01:11:35,080
you know, seen is Robin Williams and that she was incredibly creative, but like, I don't know,

754
01:11:35,080 --> 01:11:39,320
didn't necessarily fit in that well with how the world worked. So anyway, we tried to write this

755
01:11:39,320 --> 01:11:44,520
book about nothing, right? So this was a book where we, we didn't have a theme and we were just,

756
01:11:44,520 --> 01:11:47,960
you know, we didn't have a title. That's for sure. We had the worst set of titles.

757
01:11:48,520 --> 01:11:53,080
And, and I knew immediately my sister was the only one who would come up with a title for this book.

758
01:11:53,080 --> 01:11:59,400
And so I told her what we were doing. And she came back a half an hour later with, I would say,

759
01:11:59,400 --> 01:12:03,560
10 titles that were better than any title we had come up with. But she basically just said,

760
01:12:03,560 --> 01:12:07,640
look, but the title of the book is for economics. And I'm like, you're right. The title of the book

761
01:12:07,640 --> 01:12:14,280
is for economics. And, and even Dubner was, you know, okay, yeah, for economics that works.

762
01:12:14,280 --> 01:12:20,200
So we went to the publishers and they just flipped. They're like, we paid way too much for this book

763
01:12:20,200 --> 01:12:25,560
to call it for economics. They like refuse the title. They fought it. And it was months of back

764
01:12:25,560 --> 01:12:29,480
and forth before they grudgingly said that they'd call this book for economics. Well, what, what,

765
01:12:29,480 --> 01:12:33,800
what were they advocating for? What were some of their titles? Do you remember? Oh my God. You know,

766
01:12:33,800 --> 01:12:43,160
the one we were at, I think I was forgetting. So I was, um, um, uh, I think, I think one of the

767
01:12:43,160 --> 01:12:49,320
leading candidates just to put you where, how bad we were off was E-Ray vision. Like we're E stood

768
01:12:49,320 --> 01:12:58,200
for economics. And I think that was a not ruled out title before for economics. Um, but, uh,

769
01:12:58,200 --> 01:13:05,320
but in the end, I think that, you know, I understand this same book with a different

770
01:13:05,320 --> 01:13:13,400
title. Why does salt no copies? I mean, it's, it's, um, public life in general publishing

771
01:13:13,960 --> 01:13:21,480
specifically, you just a huge luck component to it. Um, it depends on, like, I think our book,

772
01:13:21,480 --> 01:13:26,440
you know, little things happen that made all the difference. Um, we got a good review in the wall

773
01:13:26,440 --> 01:13:32,360
street journal. Um, the guy went on the daily show with John Stewart and somehow he like made

774
01:13:32,360 --> 01:13:37,400
it seem really cool. And like little things happen with books that kind of determine whether they

775
01:13:37,400 --> 01:13:46,040
fly or they don't fly. And, um, and I think, I think we, we ran back time and, uh, and thought

776
01:13:46,040 --> 01:13:52,520
about this book and it going on, I think 99 times out of a hundred, it would probably wouldn't have

777
01:13:52,520 --> 01:13:56,280
sold very many books. We're just like super lucky in a lot of things that happen. I mean,

778
01:13:56,280 --> 01:13:59,240
I mean, I think it was a good book. I might like the book. I'm not trying to say it's a bad book,

779
01:13:59,240 --> 01:14:04,280
but, um, but what is really amazing is if you actually sit down and read books and so people

780
01:14:04,280 --> 01:14:09,240
send me books all the time because they want me to blurb them. It's incredible how good these books

781
01:14:09,240 --> 01:14:15,000
are. Like, you know, random books that I expect to be awful. There's so many good books and, and,

782
01:14:15,000 --> 01:14:21,320
um, look at, nobody reads them and, uh, you know, so many good podcasts that nobody listened to, but

783
01:14:21,320 --> 01:14:26,600
it's just really hard in the clutter of this world to, to break, to break through. And, um,

784
01:14:27,320 --> 01:14:33,400
and I think the, you know, the, the tightness of the correlation between how good something is and

785
01:14:33,400 --> 01:14:37,080
how much attention it gets much lower than I think most people give credit for.

786
01:14:39,080 --> 01:14:43,800
Steve, something you've been talking about quite a bit lately, we spoke about it when I was on your

787
01:14:43,800 --> 01:14:50,360
podcast. I've heard you speak about it on other podcasts, um, is kind of our shared appreciation

788
01:14:50,680 --> 01:14:58,840
in mental health and how underappreciated it is. Um, how long has this been something that's kind of

789
01:14:58,840 --> 01:15:06,440
been, at least to you as important as I think it is now, is this recent or is it something that's

790
01:15:06,440 --> 01:15:12,360
always really mattered and you've only kind of come to appreciate how underutilized these tools are?

791
01:15:12,840 --> 01:15:26,760
Yeah. So I, um, look, I was raised, um, by a father who was like no nonsense and, um, the,

792
01:15:27,960 --> 01:15:34,600
the idea of therapy, I mean, such an embarrassment. Oh my God. I mean, like no real man would ever

793
01:15:34,600 --> 01:15:41,000
think about, uh, you know, certainly crying. Like, like I would, you were not allowed to cry. And,

794
01:15:41,560 --> 01:15:46,040
you know, but you, you had a grandfather that committed suicide. Was it your father's father

795
01:15:46,040 --> 01:15:52,120
or your mother's father? My father's father who, no, but it wasn't a mental, like my father,

796
01:15:52,120 --> 01:15:57,960
my grandfather's suicide was not a mental health issue. My grandfather, uh, who loved life more

797
01:15:57,960 --> 01:16:03,960
than anything and was, I don't know, maybe 90 years old, his wife who he loved dearly had terminal

798
01:16:03,960 --> 01:16:10,840
cancer. And so he decided to commit suicide, not out of unhappiness, just because he had lived a

799
01:16:10,840 --> 01:16:16,120
great life and he was satisfied and he didn't want to be a burden on people. So, so it wasn't,

800
01:16:16,760 --> 01:16:22,040
it was in no sense, it was, it was actually in some sense the opposite. It was, it was, it was,

801
01:16:22,040 --> 01:16:27,720
um, like it just a, an acceptance of that, that death is part of the national, like in many ways,

802
01:16:27,720 --> 01:16:34,280
he was extremely far along in the mental health, uh, domain because he like accepted death as being

803
01:16:34,280 --> 01:16:39,640
a natural part of the cycle of life and was not troubled by it. Um, I think honestly, for me,

804
01:16:39,640 --> 01:16:48,120
the big change was, um, I got divorced and I met, met my wife Suzanne who, um,

805
01:16:49,400 --> 01:16:57,800
is much more in tune with these issues and got me thinking about them in a way that I never had

806
01:16:57,800 --> 01:17:05,240
before, whether it's the kind of more esoteric stuff in the, in kind of the, um, spiritual domain

807
01:17:05,240 --> 01:17:13,880
or purely about mental health. And, um, and I somehow shook off like a lifetime of, uh, of,

808
01:17:13,880 --> 01:17:19,800
of teaching, which told me that mental health was a joke and you just gotta like bite through it.

809
01:17:19,800 --> 01:17:23,720
And, uh, and came to appreciate, you know, seven years ago, really is when I started thinking about

810
01:17:23,720 --> 01:17:30,760
these issues. And, uh, I think you're right in saying that I've come to believe now that they are,

811
01:17:31,720 --> 01:17:39,320
you know, dramatically more important than our education system or our society has traditionally

812
01:17:39,320 --> 01:17:46,520
given them credit for. So you've, you've spoken about this a little bit. I mean, you've spoken

813
01:17:46,520 --> 01:17:50,520
about it with me and you've explained that you'd be even comfortable talking about it today,

814
01:17:51,160 --> 01:17:57,480
but you know, one of your children struggles with an eating disorder. Um, how hard has that been for

815
01:17:57,560 --> 01:18:05,560
you and how have you sort of navigated your appreciation for mental health with both her

816
01:18:05,560 --> 01:18:11,560
struggle, um, and your struggle? Because I, I, I think that's hard for both of you in different

817
01:18:11,560 --> 01:18:23,160
ways, obviously. You know, it's interesting that I, um, I'm really, if there's one thing I'm really

818
01:18:23,240 --> 01:18:31,080
good at, it's make it's the, um, I forget the, um, Serenity, the Serenity prayer. I'm really good at

819
01:18:31,080 --> 01:18:37,000
understanding the difference between things I can and cannot control and not worrying very much

820
01:18:37,000 --> 01:18:41,800
about the ones I can control. And if there's one thing that was clear to me that I could not control

821
01:18:41,800 --> 01:18:48,840
directly, it was, um, my daughter's eating disorder and that no amount of trying to push or pull or

822
01:18:48,840 --> 01:18:56,200
whatever was going to do it. So it was weird. I really, I approached it with a real calm. Um,

823
01:18:56,200 --> 01:19:03,320
I mean, it was, you know, awful to watch. Um, but I didn't struggle in the way that I think most

824
01:19:03,320 --> 01:19:08,440
parents struggle, which is the feeling of like, I should be doing something about it. And, um,

825
01:19:08,440 --> 01:19:14,840
I honestly just, I mean, it was, it, it, you know, it's, look, these are often lethal, you know, that,

826
01:19:15,080 --> 01:19:22,360
that, um, you know, and she was a, you know, incredibly accomplished as an anorexic. She,

827
01:19:22,360 --> 01:19:28,200
she, she was extremely effective at, um, at, um, you know, at torturing herself.

828
01:19:29,720 --> 01:19:42,840
And in a weird way, I think, um, that, that calm was helpful, um, in, in letting her be comfortable

829
01:19:43,560 --> 01:19:50,600
with me about it. And eventually I think I played, you know, some very small role in her recovery. I

830
01:19:50,600 --> 01:20:02,600
mean, obviously 99.999% of recovery was her own. Um, but, uh, so in a way it was, it was odd. I mean,

831
01:20:02,600 --> 01:20:07,240
that's probably not the answer you expected me to give, but, um, but in a strange way that was not,

832
01:20:07,240 --> 01:20:17,320
um, I, you know, I, maybe more generally with my adult children, I, I really am maybe better than

833
01:20:17,320 --> 01:20:24,360
most parents at understanding that they're their own people and that they live their own lives and

834
01:20:24,360 --> 01:20:30,760
that I'm just an observer and, um, you know, I can offer advice, but not, you know, but I have no

835
01:20:30,760 --> 01:20:37,960
control over it. Um, so I don't know. Where do you, where do you think that transition takes place

836
01:20:37,960 --> 01:20:43,720
age wise? I mean, I, you know, a friend of mine, Rick Elias, who was on this podcast once said

837
01:20:43,720 --> 01:20:49,320
something that has stuck with me. I don't, I don't think a day, maybe two days would go by that. I

838
01:20:49,320 --> 01:20:54,520
don't think of what he said, which is raising children is playing a game of tug of war that you

839
01:20:54,520 --> 01:21:01,000
have to lose. Um, and you know, but he, and he said it more eloquently, but you know, it was like,

840
01:21:01,000 --> 01:21:05,720
you, you lose it gradually, obviously. Right. So you're resting, you know, when you're holding the

841
01:21:05,720 --> 01:21:10,680
tug of war with your five year old, you're, you're still winning, but, and with your 13 year old,

842
01:21:10,680 --> 01:21:15,400
you're really starting to slip. And by the time it's the child's 18, it's their rope, right? Or

843
01:21:15,400 --> 01:21:20,920
whatever. Um, you seem to have navigated that better than most, perhaps.

844
01:21:20,920 --> 01:21:27,880
Yeah. I think, you know, I had a, I had a son who died when he was one and that was far and away,

845
01:21:27,880 --> 01:21:35,880
like a million times, a thousand times harder than anything I've ever experienced. Um, in part,

846
01:21:35,880 --> 01:21:43,960
because your own, like your real job as a parent, in some sense, your biggest job is to keep your

847
01:21:43,960 --> 01:21:50,680
children safe. And it was so difficult for me to navigate the knowing that I had not kept him

848
01:21:50,680 --> 01:22:01,320
safe. And I, and he was my first son and six kids later. Um, I think, I think that really affected

849
01:22:01,320 --> 01:22:11,320
my parenting and, you know, he didn't die because I was abusive or neglectful. He died because of a,

850
01:22:11,960 --> 01:22:17,320
a terrible disease, meningitis that came out of nowhere and, and I couldn't protect him from it.

851
01:22:18,040 --> 01:22:23,640
And, and I, and I think there's one or two ways you can go. Um, you can become incredibly

852
01:22:23,640 --> 01:22:29,080
fearful. Like I just happened to watch Finding Nemo yesterday with my kids. And so it's like,

853
01:22:29,080 --> 01:22:33,400
you can be like Marlon, the dad in Nemo who becomes incredibly protective and doesn't want

854
01:22:33,400 --> 01:22:41,240
Nemo to do anything. Or you can just understand that the world is one of uncertainty and of loss

855
01:22:41,320 --> 01:22:47,400
and of, um, risk. And I really, for whatever reason, I went that second way and I just said,

856
01:22:47,400 --> 01:22:52,120
look, I can't control the world around me and I'm just going to do the best I can

857
01:22:52,120 --> 01:22:58,840
knowing that that's true. And I think so even from a very young age with my kids, I, um,

858
01:22:59,560 --> 01:23:04,760
I have a lot of acceptance of their autonomy and their independence and the, and the fact that I

859
01:23:04,760 --> 01:23:13,080
can't mold them the way I'd like to or shape them. Um, so I think, um, so even with, look,

860
01:23:13,080 --> 01:23:18,920
do I want my four year old to do what I tell her to do? I get super pissed off when she doesn't,

861
01:23:18,920 --> 01:23:24,760
um, you know, go to bed when I tell her yes. But on the other hand, I put an enormous value

862
01:23:24,760 --> 01:23:31,480
on that autonomy and that independence. And, um, and when I'm, you know, calm and quiet, uh,

863
01:23:32,120 --> 01:23:36,920
I'm more or less for better, worse putty in her hands and we'll do essentially whatever she'll

864
01:23:36,920 --> 01:23:43,240
want because I think a big part of her growing up in life is, is learning to have control over

865
01:23:43,240 --> 01:23:49,400
situations and what better, what better way for to learn about control than to, um, then to be able

866
01:23:49,400 --> 01:23:53,720
to, to have some control in her interactions with me, which is totally different than the way I was

867
01:23:53,720 --> 01:23:58,680
raised. I think in the way, um, that, um, maybe I would have raised kids absent the death of my

868
01:23:58,680 --> 01:24:06,920
son Andrew, but, but I really have gone in that direction. How deliberate was that process when

869
01:24:06,920 --> 01:24:13,560
he died that you made a, what sounds like a very conscious or even the subconscious decision to

870
01:24:13,560 --> 01:24:23,400
accept it, um, and move on without, you know, as you said, basically, well, well, look, I, I back up

871
01:24:23,400 --> 01:24:30,600
and say the following, right? I mean, I, I think one could say that many parents who lose a child

872
01:24:30,600 --> 01:24:35,320
are never the same again. And I don't say that in a glib way. You're not the same again. So let me

873
01:24:35,320 --> 01:24:39,880
rephrase that no parent who loses a child is the same again, but you could make the case that many

874
01:24:39,880 --> 01:24:46,840
parents are so damaged after that, that, that they're, they're even externally never the same again.

875
01:24:48,360 --> 01:24:52,920
Most people, I mean, I know this about you, but my guess is many people would interact with you

876
01:24:52,920 --> 01:25:00,760
and never know that you'd lost a son. Um, so, I mean, what strikes me about it, Steve, is that

877
01:25:00,760 --> 01:25:06,200
it's the, one of the core tenants of, of, of something called dialectical behavioral therapy,

878
01:25:06,200 --> 01:25:10,840
which I've become very fond of, which is this idea called radical acceptance, which is super hard,

879
01:25:11,560 --> 01:25:16,200
but it, as its name suggests, radical acceptance is basically just radically accepting things, not

880
01:25:16,200 --> 01:25:21,320
saying that they're good. Radical acceptance doesn't mean it's good that my wife got cancer.

881
01:25:21,320 --> 01:25:29,080
It's, I radically accept that she got cancer. Um, I, I work on this every day and it's the hardest

882
01:25:29,080 --> 01:25:35,160
thing in the world. It's super hard. Um, it's even hard with silly things, by the way. You know what

883
01:25:35,160 --> 01:25:41,480
I mean? Like it's even hard with the most irrelevant things at times. Um, so I want to understand that

884
01:25:41,480 --> 01:25:49,640
a bit more. How did you possibly come to that type of profound radical acceptance of something

885
01:25:49,640 --> 01:25:58,360
so awful? So it certainly wasn't conscious. It, it, you know, and I wasn't, I think,

886
01:26:00,360 --> 01:26:04,120
it wasn't from deep thinking or anything. I just, I would say it happened. And in many ways,

887
01:26:04,120 --> 01:26:10,840
I would say my hunch is that radical acceptance of, um, big things, it must, must be easier than

888
01:26:10,840 --> 01:26:19,320
radical acceptance of little things because it's so obviously beyond control. Um, look, uh, my son

889
01:26:19,320 --> 01:26:29,880
had died. There was definitely no undoing that. And, um, and I think it, it, it, um, I, I, I, I have

890
01:26:29,880 --> 01:26:37,720
no real self-awareness, um, of how it happened. Uh, it just did. And it wasn't, I didn't work at it.

891
01:26:37,720 --> 01:26:43,480
Let me put it that way. I didn't work at it. It wasn't something like where I went to therapy

892
01:26:43,480 --> 01:26:53,160
and I came to eventually accept it. Just, just happened. I mean, it was, um, um, almost, um,

893
01:26:54,680 --> 01:27:00,920
disjoint from the grief. Uh, I mean, the grief is one thing. The grief is real and is, you know,

894
01:27:00,920 --> 01:27:09,880
semi-permanent, but how, how you, how you, how you, how you behave in other situations,

895
01:27:10,840 --> 01:27:16,440
to me, that's just, you know, I don't know why, but that's just how I reacted to it. Um,

896
01:27:18,440 --> 01:27:29,800
if that makes some sense, um, it was, it was really long before I consciously invested any time

897
01:27:30,440 --> 01:27:38,040
effort, resources into thinking about mental health or about, um, you know, about more

898
01:27:38,760 --> 01:27:45,400
spiritual things for lack of a better word. In the time that you've now come back and reflected

899
01:27:45,400 --> 01:27:49,800
more heavily on these issues around mental health in the past, as you said, seven years,

900
01:27:50,760 --> 01:27:54,520
have you learned anything new about that experience when Andrew died?

901
01:27:56,280 --> 01:28:00,360
Yeah, you know, I've never literally, I have not thought about it in those, I mean, obviously I

902
01:28:00,440 --> 01:28:06,600
think about his death all the time, but, um, but I've never, I've never, um,

903
01:28:07,880 --> 01:28:18,120
I've never tried to make any sense of it. Um, um, uh, that's the answer. I literally, uh,

904
01:28:19,000 --> 01:28:23,880
haven't thought about the way in which I processed it. Um, I haven't really talked about it. I mean,

905
01:28:23,880 --> 01:28:29,640
other than talking to you right now, I'm not sure. Um, I've talked about his death and, um,

906
01:28:29,640 --> 01:28:35,480
the aftermath of it, but I've really never talked or in many ways thought about, um,

907
01:28:36,440 --> 01:28:43,400
much about the implications of it in a weird way. Um, probably seems strange, but it's, uh,

908
01:28:43,400 --> 01:28:52,760
it's, it's true. What do you think would be necessary to infuse this idea of self-care and

909
01:28:52,760 --> 01:28:58,280
mental health into education? You talked earlier about how this might be even more important than

910
01:28:58,280 --> 01:29:03,640
education, which I don't think anybody would argue is not itself important. How would you,

911
01:29:04,200 --> 01:29:09,400
how would you operationalize that if you were education czar, you know, whatever that might be,

912
01:29:09,400 --> 01:29:15,480
then you, you had to hold over the entire K through 12 system. How would you infuse this type of,

913
01:29:15,480 --> 01:29:19,720
this type of thinking? Let me take a step back and just first make the case for it.

914
01:29:19,720 --> 01:29:26,440
Cause I think that's worth with doing. I think we went back to square one and we thought about what

915
01:29:26,520 --> 01:29:32,360
we should, what we should teach children about navigating the world. We would

916
01:29:33,480 --> 01:29:38,680
radically overhaul the curriculum and like we'd change math and stuff, but, but, but much more

917
01:29:38,680 --> 01:29:45,240
fundamentally, I think we would try to teach them the set of skills that will make them happy

918
01:29:46,040 --> 01:29:55,480
and able to resolve conflict with other people and to, you know, you know, just get along in the

919
01:29:55,480 --> 01:30:00,760
world. I think like, at least for my kids, I'm much more worried about that than whether they're

920
01:30:00,760 --> 01:30:05,960
like really good at calculus. That matters less to me that they have a good lives and they're,

921
01:30:05,960 --> 01:30:12,360
and they know, they know themselves and they make good choices and like that. I think that wasn't

922
01:30:12,360 --> 01:30:17,800
the spirit when we started school. And so our curriculum is very much built off of things from

923
01:30:17,800 --> 01:30:22,840
a hundred years ago and this wasn't in the air and maybe we didn't have the tools then. Okay.

924
01:30:22,840 --> 01:30:29,160
So that's my case for why I think we should be teaching. So, and that doesn't answer your

925
01:30:29,160 --> 01:30:34,360
question at all about how to do that. Okay. It's just my belief that as a parent or as a school,

926
01:30:34,360 --> 01:30:39,640
our goal should be that we raise well-adjusted kids who have a set of tools that are helping

927
01:30:39,640 --> 01:30:46,600
them cope with the world around them. Okay. And, and that many adults like you and me have made a

928
01:30:46,600 --> 01:30:52,600
lot of investment in these tools later in our lives. And maybe we could reduce the need for that

929
01:30:52,840 --> 01:30:59,000
if we had invested more earlier as a society in that. Okay. How did that, I have no idea. I mean,

930
01:30:59,000 --> 01:31:03,640
it's like impossible because it's impossible to change anything. What's the problem? The problem

931
01:31:03,640 --> 01:31:10,760
is that time during the day is scarce in school and every minute you spend focusing on mental

932
01:31:10,760 --> 01:31:17,160
health or self care, whatever is a minute taken away from something else. And that's really hard

933
01:31:17,160 --> 01:31:21,480
to do. The second thing is who in the world actually knows how to do this? Well, I'm not,

934
01:31:21,480 --> 01:31:29,960
I'm not sure I know how to do this well. And even if I did know a person who knew how to do this well,

935
01:31:30,600 --> 01:31:37,000
how do I know how to scale it? Absolutely not. I don't know what kind of training you give teachers

936
01:31:37,000 --> 01:31:41,960
or who you'd hire. So like, I think it's, it's very amorphous in my mind. It's, it's a belief

937
01:31:41,960 --> 01:31:48,440
that it's important. And I think it'll be a long, let's just say y'all agree it should happen. It

938
01:31:48,440 --> 01:31:53,480
will be a long arduous process. What I believe the right way to do it is for the department of

939
01:31:53,480 --> 01:31:59,400
education to mandate, like we have for the common core and math and math, that we should have a

940
01:31:59,400 --> 01:32:04,440
common core and mental health and that there should be these 17 things that are hit each year,

941
01:32:04,440 --> 01:32:08,520
you know, different ones for anything. Like, no, obviously this is the kind of thing where I think

942
01:32:09,320 --> 01:32:14,520
maybe you'd want to experimentation, right? So let a bunch of models run, see what seems to work and,

943
01:32:15,080 --> 01:32:19,320
you know, and grow into it. I would be very much in favor of that where we encouraged

944
01:32:19,960 --> 01:32:28,120
subsidized school districts to try out things and encourage innovation. But I think we're,

945
01:32:28,760 --> 01:32:33,320
I think we're a long ways away from that. I had the privilege of talking to the Biden transition

946
01:32:33,320 --> 01:32:39,880
team not too long ago about education. And, and I threw out three or four ideas and I,

947
01:32:39,880 --> 01:32:45,000
and they, they, they listened quite intently to each of the ideas, except for the mental

948
01:32:45,000 --> 01:32:49,080
health ones. Like they could not get away. They could not get off that topic fast enough.

949
01:32:50,120 --> 01:32:55,000
I like total silence in the room when I, when I suggested that as an important thing to do.

950
01:32:55,000 --> 01:32:59,080
Have you ever gone back and spoken with your dad? Because your dad is still spry as can be.

951
01:32:59,080 --> 01:33:03,400
Doesn't he still work? He's still practicing. Yeah, my dad is still a practicing physician

952
01:33:03,400 --> 01:33:08,440
at the VA hospital. So have you ever gone back to your dad and revisited this idea of mental health

953
01:33:08,440 --> 01:33:15,560
and his lack of interest in this topic? I haven't, you know, it's funny. So I'm,

954
01:33:15,560 --> 01:33:24,120
I'm actually thinking hard about interviewing my dad for my podcast and, and, and I'm pondering.

955
01:33:25,000 --> 01:33:30,440
So it seems, so it's, it's funny. I feel like somehow doing it via podcast should be the opposite,

956
01:33:30,440 --> 01:33:34,600
but I somehow feel like, well, if I'm doing it on the podcast, I can ask my dad all sorts of

957
01:33:34,600 --> 01:33:40,040
questions that I couldn't ask him otherwise, which clearly like logically it makes no sense at all.

958
01:33:40,040 --> 01:33:43,800
Why would recording something and playing it for hundreds of thousands of people

959
01:33:44,600 --> 01:33:48,600
put questions on the table instead of taking them off the table? But in a weird way,

960
01:33:49,160 --> 01:33:55,720
I think somehow it would be easier for me to ask my dad these questions like this in the context

961
01:33:55,720 --> 01:34:02,280
of a podcast. And we'll see what happens. I, I, he's a very reluctant potential podcast guest.

962
01:34:02,840 --> 01:34:08,840
And, and I'm not sure he can do technology well enough to hit the button to actually record it

963
01:34:08,840 --> 01:34:13,400
on his end. So we'll see whether it happened, but not literally never talked to my father about any

964
01:34:13,400 --> 01:34:20,760
of these issues ever. Super interesting. I've not talked to my father about my sister's death

965
01:34:21,720 --> 01:34:30,040
or his father and mother's death. Just, we're a, we're a family that doesn't talk about stuff

966
01:34:30,040 --> 01:34:37,480
like that. We're, you know, we were, it was a nice enough family, but the word love never,

967
01:34:38,280 --> 01:34:43,080
never used in my household except with reference to the family dog.

968
01:34:44,120 --> 01:34:48,360
People love the dog, but you would never ever say you loved another family member.

969
01:34:49,080 --> 01:34:52,920
Now, how different is that with your current children and your current family? Is that

970
01:34:52,920 --> 01:34:57,880
different? So I have self-consciously been very different about that, that, that, that,

971
01:34:58,520 --> 01:35:03,800
I tell my kids and they tell me they love me, you know, every time we hang up the phone or whatever,

972
01:35:05,000 --> 01:35:08,440
which I don't know if that matters for anything, but it is, it is, that is like,

973
01:35:08,440 --> 01:35:13,480
I don't do that many things self-consciously in my life, but that is one that I very self-consciously

974
01:35:13,480 --> 01:35:18,600
tried to, tried to propagate was the idea that you could express your feelings of,

975
01:35:18,600 --> 01:35:27,560
of love and affection to, in my, in my current, you know, in my family, the family I, I'm the

976
01:35:27,560 --> 01:35:34,200
father figure. So you're fond of decision making. What's the best decision you've ever made in your

977
01:35:34,200 --> 01:35:39,240
life and what's the worst decision you've ever made in your life? I'm probably going to give you

978
01:35:39,240 --> 01:35:47,560
a terrible answer to this. I think my worst decisions, I'm too embarrassed to talk about.

979
01:35:47,560 --> 01:35:50,040
That's fine. I probably wouldn't be able to publicly admit that.

980
01:35:50,040 --> 01:35:54,760
But I've made a lot of like, look, I will tell you all of my worst decisions have been about

981
01:35:54,760 --> 01:36:00,440
inaction, not action. I know that, that, that like I have not made, I've never made a terrible,

982
01:36:01,560 --> 01:36:06,600
of the hundred worst decisions I've made, I'm sure that 99 of them were not doing something

983
01:36:06,600 --> 01:36:10,680
as opposed to doing something. Interesting. There are very few really bad things I've done.

984
01:36:10,680 --> 01:36:16,200
By the way, I feel the opposite, Steve. My worst decisions have been decisions of bad action,

985
01:36:16,200 --> 01:36:21,480
not inaction, but that's interesting. And what's the best decision you think you've ever made or a

986
01:36:21,480 --> 01:36:35,000
subset of the best decisions you've ever made? I think the, I don't know if you'd actually call

987
01:36:35,000 --> 01:36:42,040
a decision, but the decision, it's sort of a decision to not care what people think about me.

988
01:36:42,520 --> 01:36:46,120
It's not a decision in like, oh, I went back to school, I didn't go back to school, but,

989
01:36:46,120 --> 01:36:50,120
but I somehow just made a choice. It's a choice. I don't know if you call it a choice. I made a choice

990
01:36:50,120 --> 01:36:56,120
to just not care what other people thought about me. And which was a big choice for me because

991
01:36:56,120 --> 01:37:01,400
like most high school kids, I, all I cared about was what other people thought about me.

992
01:37:01,960 --> 01:37:09,400
And at some point I just broke with that and, and was happy to live with the consequences.

993
01:37:09,560 --> 01:37:17,000
And it's been, it's, it's just a, it's just a great way to live. To, it just frees you up from,

994
01:37:17,000 --> 01:37:24,520
from so much burden. I think all of my best decisions I wanted have relieved burden.

995
01:37:24,520 --> 01:37:29,000
I'll give you another one, which is trivial in comparison, but important is I used to be super

996
01:37:29,000 --> 01:37:35,960
cheap. I used to worry about every transaction I made and like prided myself on being frugal

997
01:37:36,440 --> 01:37:41,240
and thinking about, oh, you know, should I spend that dollar on a bottled water in the airport or

998
01:37:41,240 --> 01:37:47,640
not? And at some point I just said, Hey, this is like, I could free up a lot of mind space. If,

999
01:37:47,640 --> 01:37:53,960
if I just didn't worry about decisions that were like under $5 and it was hard for me to do, but

1000
01:37:53,960 --> 01:37:57,800
I said, look, I'm going to stop. If it's under $5 when I start to go into this friends, I'm going

1001
01:37:57,800 --> 01:38:03,320
to forget about it. And then I moved it up to like 10 or 20. And like the highest, like for me,

1002
01:38:03,640 --> 01:38:08,440
it's like the higher, the better. And it is just so liberating for me that I just have this rule

1003
01:38:08,440 --> 01:38:15,160
of thumb that like, if it's under some, you know, ridiculously large amount of money, I just don't

1004
01:38:15,160 --> 01:38:22,120
worry about it. And, and look, it adds up. I probably, you know, I, I waste tens of thousands,

1005
01:38:22,120 --> 01:38:29,720
if not more dollars a year, because I'm not getting these decisions right. And that's awesome

1006
01:38:30,440 --> 01:38:36,440
because I free up, you know, 10% of my time to do whatever the hell I want. And I love doing the

1007
01:38:36,440 --> 01:38:41,960
stuff I do. And, and it's, it's just super liberating. So I think many of the things I'd

1008
01:38:41,960 --> 01:38:48,440
like are these things that have been liberating where I stop worrying about stuff. And maybe it's

1009
01:38:48,440 --> 01:38:52,120
not that far off than your idea of radical acceptance. Like, I don't know, it's like not

1010
01:38:52,120 --> 01:38:56,680
radical acceptance, but it's like, it's just a total acceptance that there's going to be these

1011
01:38:56,760 --> 01:39:01,000
total acceptance that there's going to be these imperfections. I'm just not going to worry about

1012
01:39:01,000 --> 01:39:07,240
getting things exactly right. Now, do you think in general, we as a species are good at decision

1013
01:39:07,240 --> 01:39:11,000
making? What is there a way to, to evaluate this? I mean, like,

1014
01:39:15,400 --> 01:39:20,840
Good compared to what I'm not sure, but good compared to compared to optimal? No, we're

1015
01:39:20,840 --> 01:39:26,280
terrible. I mean, it's like, it's like behavioral economics of the last 40, 50 years shows how bad

1016
01:39:26,280 --> 01:39:32,440
we are at making decisions. And I think the best evidence of this is the people who know

1017
01:39:32,440 --> 01:39:42,280
the most about decision making. So whether it's this really prominent economist who've, you know,

1018
01:39:42,280 --> 01:39:49,800
thought enormous amounts about backward induction and, and optimized behavior in dynamic systems,

1019
01:39:50,520 --> 01:39:55,960
or whether it's Danny Kahneman, who has spent more time thinking about behavior. These are the worst

1020
01:39:55,960 --> 01:40:00,440
decision makers I've ever seen. I mean, Kahneman even is like very forthright. He said, like,

1021
01:40:00,440 --> 01:40:05,400
the reason I studied decision making is that I'm awful at it. And with everything that he's

1022
01:40:05,400 --> 01:40:14,520
learned, he's still awful at it. And so I think, yeah, I don't think, I don't think humans are good

1023
01:40:14,520 --> 01:40:21,000
at making decisions. And, and I think we can arm ourselves with tools. Like there are a lot of,

1024
01:40:21,720 --> 01:40:27,880
a lot of economics is really common sense codified into helping you make better decisions. I think a

1025
01:40:27,880 --> 01:40:38,040
fair amount of psychology and pop psychology is that as well. But look, and there's no one who

1026
01:40:38,040 --> 01:40:46,280
talks more than me about how we don't quit enough and, you know, and how people get, get stuck in

1027
01:40:46,280 --> 01:40:52,760
bad situations. I know that. And I, and I have so much trouble quitting. I mean, it's stuff I should

1028
01:40:52,760 --> 01:40:58,680
quit. I just, I eventually do quit it, but it takes me usually like on average two years longer to quit

1029
01:40:58,680 --> 01:41:03,560
something than, than I know what I know. I should have quit it two years before I do it. I can, I

1030
01:41:03,560 --> 01:41:09,400
understand all of the problems. I understand exactly what the situation is and the, you know, but,

1031
01:41:09,480 --> 01:41:17,320
but to overcome the complexity of the brain and all these forces, it's super hard to make good

1032
01:41:17,320 --> 01:41:20,680
decisions. What do you think, do you think there's an evolutionary force behind them?

1033
01:41:23,080 --> 01:41:30,040
I think that the evolutionary, when, when we were evolving, the kind of choices you had to make are

1034
01:41:30,040 --> 01:41:35,080
so different than the ones that we face now, that it's not a surprise that evolution would not

1035
01:41:35,080 --> 01:41:40,360
necessarily be our friend in, in these kinds of decisions. I mean, just, I don't know, I don't

1036
01:41:40,360 --> 01:41:45,640
know anything about it really, but my sense is that evolution has not been our friend in the

1037
01:41:45,640 --> 01:41:52,120
nutrition realm, right? Because we evolved in a, in an environment of scarcity and now we live in an

1038
01:41:52,120 --> 01:41:56,760
environment of plenty and all of the triggers we get from evolutionary triggers we get are the wrong

1039
01:41:56,760 --> 01:42:02,040
ones. And I, you know, probably that's the same with decision-making, right? We, we, we, we evolved

1040
01:42:02,360 --> 01:42:08,280
in settings where we were like facing life and death problems all the time. I mean, I think one

1041
01:42:08,280 --> 01:42:15,400
of the hugest problems that we face mental health wise is this is trauma and, and, and people are

1042
01:42:15,400 --> 01:42:20,600
stuck in, in a trauma mode and reacting to trauma, which evolutionary is probably fantastic because

1043
01:42:20,600 --> 01:42:27,960
the kind of challenges we faced in, and when we were evolving were short-term, incredibly intense

1044
01:42:27,960 --> 01:42:33,320
risks that you needed to marshal everything in your, in your body, in your mind to fight.

1045
01:42:33,960 --> 01:42:39,160
But now, now those aren't typically the kind of risks that we're, we're, or challenges we're

1046
01:42:39,160 --> 01:42:46,280
facing. And so people end up using the wrong set of tools that the body is just not configured well

1047
01:42:46,280 --> 01:42:55,240
for the kind of chronic, you know, chronic settings of, we live in of, of, you know, of too many people,

1048
01:42:55,240 --> 01:43:01,000
too much information, you know, constant fears and threats around itself. Like, I don't know

1049
01:43:01,000 --> 01:43:05,560
much about it, but, but like I, I think there's zero reason to think that evolution is our friend

1050
01:43:05,560 --> 01:43:10,280
when it comes to difficult, complex decisions. It's going to be just didn't, like how complex

1051
01:43:10,280 --> 01:43:15,400
could the decisions have been when we were evolving? I mean, I look back at it, like this

1052
01:43:15,400 --> 01:43:18,040
is maybe off track, but it was something that really struck me. I went to Australia for a while

1053
01:43:18,680 --> 01:43:26,760
and, and, and was talking with some guides as we went around Ayrus Rock, Uduru, and, and talking

1054
01:43:26,760 --> 01:43:37,720
about how basically the Aboriginal life had not changed hardly at all over the course of, of,

1055
01:43:38,440 --> 01:43:44,040
I don't know, 5,000 years or something. And it was interesting to think about, like, you, you don't

1056
01:43:44,040 --> 01:43:50,600
need much innovation at all, but you, you innovate 1% per generation and over 5,000 years, life is

1057
01:43:50,600 --> 01:43:54,120
completely transformed. So it's like, there's literally no innovation going on. I mean, there's

1058
01:43:54,120 --> 01:43:57,720
things like they didn't have the wheel because in the bush, the wheel didn't do you very much good.

1059
01:43:57,720 --> 01:44:02,920
So like, there was interesting to hear about, but it just, it really hit home for me that observation,

1060
01:44:02,920 --> 01:44:10,840
which is modern life and this idea of progress and, and, and innovation and being better off than

1061
01:44:10,840 --> 01:44:17,960
your parents. Like that's a super, super modern idea. Not at all one associated with, with mankind's

1062
01:44:17,960 --> 01:44:26,360
existence over, you know, huge, you know, except for this tiny sliver of, of kind of post-industrial

1063
01:44:26,360 --> 01:44:30,200
life that we're in. It makes it almost impossible to fathom what another hundred years looks like

1064
01:44:32,200 --> 01:44:36,200
for exactly that reason. The non, again, the non-linearity of what the acceleration of

1065
01:44:36,200 --> 01:44:42,120
technology has been like in 100 years and what it, what it looks, looks like going forward.

1066
01:44:42,120 --> 01:44:47,080
Now, speaking of a lack of progress, this is totally off topic, but what you said kind of

1067
01:44:47,080 --> 01:44:52,920
made me think about this. Let's talk about horse racing for a minute. You're, you're kind of a fan.

1068
01:44:53,880 --> 01:45:02,280
Did you get to see secretariat at one point? I did. Yeah. So I was, so when I was in college,

1069
01:45:03,240 --> 01:45:11,960
I wrote my undergraduate thesis on thoroughbred breeding and, and it just happened to be the best

1070
01:45:11,960 --> 01:45:18,760
library was in Lexington, Kentucky. And that's where secretariat was. And, and I was, I was,

1071
01:45:18,760 --> 01:45:22,280
I didn't really understand how the world worked very well. I just figured I could show up at

1072
01:45:22,280 --> 01:45:28,200
Claiborne Farms and they would invite me to go see. So I showed up unannounced at Claiborne Farms

1073
01:45:28,200 --> 01:45:32,600
and I said, I would like to see secretariat. And, and they looked at me like I was crazy.

1074
01:45:33,480 --> 01:45:39,240
And like two seconds later, one of the part owners of secretariat happened to drive up behind me and,

1075
01:45:39,240 --> 01:45:43,560
and like walk up to them and say, Hey, we were just going to go say hi to secretariat.

1076
01:45:44,120 --> 01:45:48,040
And, and for whatever reason, instead of like showing me away, they said, well,

1077
01:45:48,040 --> 01:45:52,200
this is like total clown here thinks he should be secretary too. Do you mind if you text a line?

1078
01:45:52,200 --> 01:45:57,480
Like, okay, why don't you get, so I did, I did get the meet, cause it's funny. I was six when

1079
01:45:57,480 --> 01:46:03,400
secretariat won the triple clown frown. It's one of my sports memory. Absolutely. I remember where

1080
01:46:03,400 --> 01:46:11,240
I was. And, and I remember it in part because for whatever reason in the Belmont, I had chosen

1081
01:46:11,240 --> 01:46:18,440
secretariat as the horse that I thought would win, you know, probably based on heavy guidance from

1082
01:46:19,000 --> 01:46:24,280
adults who had suddenly like led me to think he should win. And then that was the race he won by

1083
01:46:24,360 --> 01:46:34,920
31 or whatever it was. And, and, and I still remember that vividly. And so it was, I'm not,

1084
01:46:34,920 --> 01:46:41,080
I'm not like, I'm not a very good sports fan in the sense that I not very good at remembering names

1085
01:46:41,080 --> 01:46:45,000
of players and whatnot, or heavy heroes, but, but secretary was always one of my heroes. And then he

1086
01:46:45,000 --> 01:46:50,760
died. I don't know when he died. It was, I would have gone there in 1988. Yeah. So he didn't live

1087
01:46:50,840 --> 01:46:58,120
very much longer after that. Yeah. He was only 19 when he died. Why, I mean, you wrote about

1088
01:46:58,120 --> 01:47:02,680
thoroughbred breeding. Why do you think that secretariat was the peak of thoroughbreds and

1089
01:47:02,680 --> 01:47:07,320
they've never gotten note, no horses ever approached secretary its speeds. I mean,

1090
01:47:07,320 --> 01:47:11,320
I think for the listener, just to put in context, what's the triple crown, you've got the Kentucky

1091
01:47:11,320 --> 01:47:17,160
Derby, the Preakness, the Belmont to win all three of those. And the three of those take place over

1092
01:47:17,160 --> 01:47:24,360
what about six weeks, months. Yeah. So to win these three races, which get longer, I mean,

1093
01:47:24,360 --> 01:47:29,000
technically, I guess the it's a mile and a quarter, a mile and three eights, and then a mile and a half.

1094
01:47:29,880 --> 01:47:34,680
So for horses, that's a pretty big difference in distance. So that's speed and endurance.

1095
01:47:35,960 --> 01:47:39,320
In a short period of time. I mean, it's very difficult to win all three of those.

1096
01:47:40,360 --> 01:47:46,120
Secretariat came along and won in 1973, but did, you know, to this day has the record for the

1097
01:47:46,120 --> 01:47:52,600
fastest time in each. I think in the Kentucky Derby, what to me is remarkable is negative

1098
01:47:52,600 --> 01:47:57,960
splitting each quarter mile successively. So each of the five quarters was faster and faster and

1099
01:47:57,960 --> 01:48:03,880
faster and faster and faster. And then in the Belmont, not only running 224, which is like

1100
01:48:03,880 --> 01:48:11,400
six seconds faster than, you know, American Pharaoh won it, but also winning by whatever 30, 31

1101
01:48:11,400 --> 01:48:17,160
lengths. No horses come close to this. Why do you think that is?

1102
01:48:19,080 --> 01:48:28,040
So I don't know if I had it. So there are a couple of things to think about. So I've actually dabbled

1103
01:48:28,040 --> 01:48:37,560
with trying to think about, could you more in a smarter way do breeding of horses. So the way they

1104
01:48:37,560 --> 01:48:44,520
decide what horse to breed to another is not extremely scientific. They kind of look at the

1105
01:48:44,520 --> 01:48:52,280
horse and they have some ideas. And so my hunch is that it's not particularly well done. Although

1106
01:48:52,280 --> 01:48:58,680
just like with humans, there's an enormous amount of assortative mating, right? So it's not like

1107
01:48:58,680 --> 01:49:05,880
they mate the worst mares to the best studs. I mean, they make the best and the best, you know,

1108
01:49:05,880 --> 01:49:11,240
in the same way that the, you know, the best basketball players end up, you know, getting

1109
01:49:11,240 --> 01:49:15,080
married to the, you know, volleyball star. And then who knows what kind of amazing offspring

1110
01:49:15,080 --> 01:49:24,280
they're going to have. I think probably the best explanation for it would be that in contrast to

1111
01:49:25,480 --> 01:49:32,280
human sports, I'm not sure there's been any advances in training techniques for horses

1112
01:49:33,240 --> 01:49:40,840
in the last 40 or 50 years. Now maybe there have been, but they're not obvious to an outside

1113
01:49:40,840 --> 01:49:46,600
observer. And I've always won, like I have this, like I have a few little secret dreams. And one

1114
01:49:46,600 --> 01:49:51,640
of my secret dreams, which sounds like a Disney movie or something, is that I would buy a horse

1115
01:49:51,640 --> 01:50:00,280
an average thoroughbred. And I would train that horse the way Peter Attia trains for ultra marathons.

1116
01:50:00,840 --> 01:50:05,480
And the horse would actually turn out to be really good. I mean, I don't really know much

1117
01:50:05,480 --> 01:50:10,760
about horse training, but I don't think I'm greatly exaggerating when I say that the way

1118
01:50:10,760 --> 01:50:17,560
that horses are currently trained is they stand in their stall for something like 23 and a half

1119
01:50:17,560 --> 01:50:25,160
hours a day. And like roughly once every day or two, they bring them out of the stall and they

1120
01:50:25,160 --> 01:50:29,560
trot around and they run. And sometimes they run fast, but they don't tend to run fast for more

1121
01:50:29,560 --> 01:50:34,040
than like three eighths of a mile. Even though their actual races are going to be a mile, a mile

1122
01:50:34,040 --> 01:50:38,200
and a quarter, they only, like when they train, they only run three eighths of a mile. And I don't,

1123
01:50:38,760 --> 01:50:44,040
and I think these horses spend almost no time swimming. So you might say, look, the legs are

1124
01:50:44,040 --> 01:50:48,520
fragile because the legs are fragile. You can't like run them all the time. But like in my little

1125
01:50:48,520 --> 01:50:54,360
fantasy world, I've got a horse sized pool with a strong current going against it. And my, you know,

1126
01:50:55,080 --> 01:50:59,480
you know, my horse Silver or whatever, you know, whatever the horses that I have in my Disney movie,

1127
01:50:59,720 --> 01:51:05,960
it's swimming in the water like four hours a day. And I've got him hooked up to oxygen monitors. And

1128
01:51:05,960 --> 01:51:12,520
he's like, you know, he's doing that's an interesting point, right? You put some zone

1129
01:51:12,520 --> 01:51:17,080
to training and build some aerobic capacity, Miticon, real efficiency. I mean, that's,

1130
01:51:17,880 --> 01:51:23,320
look, I don't think it's, I mean, I don't know enough about it to say if what you're saying

1131
01:51:23,320 --> 01:51:28,840
is correct or not, that there haven't been advances in 50 years. It's hard to believe there haven't

1132
01:51:28,840 --> 01:51:33,560
been, but I don't know. I mean, it would be interesting. I think though, if you,

1133
01:51:34,200 --> 01:51:40,360
let's forget about the golf project. I think it would be super fun if you and I, because I think

1134
01:51:40,360 --> 01:51:44,760
like, I'm not going to learn, you already know a lot about how humans work. And I'm sure you could

1135
01:51:44,760 --> 01:51:51,560
figure out really quickly how horses work. And it will be, I think it would be so much fun to buy

1136
01:51:51,560 --> 01:51:57,080
some, you know, to claim, okay, because these horses when they run, you can claim them for like

1137
01:51:57,160 --> 01:52:01,800
$5,000, take some total illusion. Well, the problem is for the triple crown, you gotta be a

1138
01:52:01,800 --> 01:52:07,960
three year old. So you can't really do it. You just gotta take some completely un, unspecial horse.

1139
01:52:08,600 --> 01:52:13,480
And, and what we'd really want to do is we'd want to win the triple crown, like four years in a row,

1140
01:52:14,360 --> 01:52:20,200
doing our aerobic training. Look, if it were humans, like imagine, just, just imagine you go back

1141
01:52:20,200 --> 01:52:26,200
to, uh, you know, I don't know, to, to the time when, uh, you know, you know much more than me,

1142
01:52:26,200 --> 01:52:29,560
but like when Roger Bannister was starting the four minute mile. And I think there's still like

1143
01:52:29,560 --> 01:52:35,720
not very good training going on at all at that time. I mean, I may be crazy, but is it not true

1144
01:52:35,720 --> 01:52:40,520
that good high school runners in the U S. Yeah. So I think, I think, I think, I think in the case

1145
01:52:40,520 --> 01:52:47,800
of humans, we can definitely say that the biggest difference between Roger Bannister and a good

1146
01:52:47,800 --> 01:52:53,960
collegiate runner today is knowledge of training much more than equipment or nutrition for sure.

1147
01:52:53,960 --> 01:52:57,960
Because so it would be true. Like if you could magically understand

1148
01:52:59,000 --> 01:53:05,720
fitness and training and take today's fitness and training and take like a run of relatively good

1149
01:53:05,720 --> 01:53:10,520
athlete, but nothing special. I think you could have won the Olympic gold medal in, in, um, a

1150
01:53:10,520 --> 01:53:17,000
range of sports. That would be my conjecture. Take a good athlete and, and, and, and like, like, like,

1151
01:53:17,000 --> 01:53:23,000
uh, like a college, a good college caliber athlete. And with, and the difference between like swimming

1152
01:53:23,000 --> 01:53:28,440
would be probably a good example. Like I think you probably the best, like the, the hundredth of

1153
01:53:28,440 --> 01:53:33,080
the thousandth best swimmer today is probably better than the best swimmer in the world 50

1154
01:53:33,080 --> 01:53:37,800
years ago in terms of times. I don't know. You wouldn't know. Yeah. I'd have to go back. Yeah.

1155
01:53:37,800 --> 01:53:42,280
Yeah. So like, that's what I'm saying. So you take an hour, like a good, but not great horse.

1156
01:53:42,840 --> 01:53:47,320
And it seems like you could, if you could get the same kind of advances in training,

1157
01:53:47,320 --> 01:53:51,720
you would just, I mean, I guess the other thing is how much of an outlier with secretariat, right?

1158
01:53:51,720 --> 01:53:56,200
I mean, I think there's still some debate about was his greatness, how much of it was due to the

1159
01:53:56,200 --> 01:54:03,160
size of his heart. Unfortunately, his heart was not weighed at autopsy, but the same that, that did

1160
01:54:03,160 --> 01:54:08,360
the autopsy on secretariat several years later, did the autopsy on sham, who was the horse that

1161
01:54:08,360 --> 01:54:16,440
finished second, it was basically the second best horse in the world in 1973. And sham had a pretty

1162
01:54:16,440 --> 01:54:23,320
large heart and he said secretariat's heart was at least 15% larger, which basically put secretariat's

1163
01:54:23,320 --> 01:54:30,360
heart at twice the size of a normal heart. And it's interesting that has been linked to an X chromosome,

1164
01:54:31,640 --> 01:54:37,560
which may partially explain why secretariat's immediate male offspring weren't that special

1165
01:54:37,560 --> 01:54:42,840
because his male offspring would have got his Y chromosome. His female would have got the X and

1166
01:54:42,840 --> 01:54:50,120
only if that X made it into a male two generations later would that have been particularly fast horse,

1167
01:54:50,120 --> 01:54:54,360
which I think also gets to your point about the lack of sophistication around breeding.

1168
01:54:55,160 --> 01:55:01,240
I don't know why I find this stuff so interesting. I think there's just something so beautiful about

1169
01:55:02,040 --> 01:55:06,440
these horses, but in particular secretariat, I've always been so enamored with. And in fact, when

1170
01:55:06,520 --> 01:55:14,600
our middle son before he was born, I was really lobbying hard for the middle name secretariat.

1171
01:55:17,080 --> 01:55:21,560
That didn't even get a prayer, but I lobbied really hard for it.

1172
01:55:23,000 --> 01:55:28,840
Maybe if you lived in communist Russia, you would have had more of a better chance with the middle

1173
01:55:28,840 --> 01:55:34,520
name secretariat. Okay, Steve, a couple of last questions for you here, because I've been taking

1174
01:55:34,520 --> 01:55:39,960
up too much of your time. What do you think is the most interesting question

1175
01:55:41,160 --> 01:55:43,960
on your mind that you don't have a clue of what the answer is?

1176
01:55:46,680 --> 01:55:56,840
I don't have a clue. So this is just pure weird, but something I think about. Actually,

1177
01:55:56,840 --> 01:56:03,000
I think of it with you because it's your influence that led me in this direction because you

1178
01:56:03,000 --> 01:56:09,880
introduced me to Sam Harris's work. And there was a few paragraphs in Sam Harris's book,

1179
01:56:11,240 --> 01:56:22,120
Waking Up, which have intrigued me, which I don't really know anything about, but I find them

1180
01:56:22,120 --> 01:56:28,040
intriguing. So this is what I find really interesting. So he talks about how the left

1181
01:56:28,120 --> 01:56:33,320
side of the brain has the ability to speak and the right side of the brain

1182
01:56:34,920 --> 01:56:41,160
doesn't have the capability to express itself through speech. And I'm sure there's a lot more

1183
01:56:41,160 --> 01:56:45,160
to it than that, but it's a starting point. That simple observation, I find interesting because it

1184
01:56:45,160 --> 01:56:51,240
got me thinking about the degree to which our lives are mediated through speech. So obviously,

1185
01:56:51,240 --> 01:56:57,000
when I talk to you, I have to use speech, but my own inner life is incredibly dictated through

1186
01:56:57,000 --> 01:57:02,440
speech. So I'm putting words on everything. As I go through my life, it's not just this idea of

1187
01:57:02,440 --> 01:57:07,800
chatter in the back of our head and like, is there some monkey on my back telling me I'm a

1188
01:57:07,800 --> 01:57:13,000
loser or whatever. That's not really what I'm talking about. It's much more about that in general,

1189
01:57:13,640 --> 01:57:20,200
when I look at a computer, I put the word computer on it. So then it just got me interested in the

1190
01:57:20,200 --> 01:57:28,040
question of, well, he kind of makes a little suggestion that it's possible that you could

1191
01:57:28,040 --> 01:57:34,680
think of the right side of the brain as being held as a slave dominated by the left hand side of the

1192
01:57:34,680 --> 01:57:38,760
brain. Like the right hand side of the brain has all sorts of interesting things it could do,

1193
01:57:39,320 --> 01:57:47,160
but because it can't speak, it's relegated to this awful subsidiary role. And so I just got

1194
01:57:47,240 --> 01:57:54,680
interested in the idea of how could I introduce myself to the right side of my brain? How could

1195
01:57:54,680 --> 01:58:00,840
I actually get to meet that part of my brain? Which I don't even, like I know nothing about how the

1196
01:58:00,840 --> 01:58:07,720
brain works. I don't even know if that makes sense, but like given infinite time, which I don't have,

1197
01:58:08,280 --> 01:58:11,880
it's actually, I think that's one of the things I would pursue. So I've pursued it like in a little

1198
01:58:11,880 --> 01:58:19,960
way, which is I found it was actually quite easy with concentration to train myself to be silent.

1199
01:58:20,920 --> 01:58:22,040
Silent in thought, you mean?

1200
01:58:23,800 --> 01:58:28,840
So what I want to do, so I want to think, I just don't want to think with words. So it's a way,

1201
01:58:28,840 --> 01:58:32,120
so I think it's slightly, like I don't understand meditation and all the different kinds of

1202
01:58:32,120 --> 01:58:35,400
meditation, but I think what I'm trying to do is slightly different than what most people are

1203
01:58:35,400 --> 01:58:40,440
trying to do. I'm not going to quiet my brain. What I'm trying to do is I'm trying to observe

1204
01:58:40,440 --> 01:58:48,040
the world with all of my senses, but observe them without, but free of language. Okay. So that,

1205
01:58:48,040 --> 01:58:55,960
look, it's totally possible as I, I don't know, peel a potato. Like I know how to peel a potato

1206
01:58:55,960 --> 01:59:01,080
without calling a potato, without using the word peel, you know, and almost everything we do in

1207
01:59:01,080 --> 01:59:08,840
life, even I think in some sense, listening, I can kind of, I don't know about listening, but like

1208
01:59:08,840 --> 01:59:13,240
most things I can do on my own, I don't need language, but I constantly have language as my

1209
01:59:13,240 --> 01:59:21,960
partner. And, and so it was an interesting and surprisingly easy task to on demand, be able to

1210
01:59:22,600 --> 01:59:29,880
try to put words aside and be word free. I won't say that I can do it like at will or perfectly,

1211
01:59:29,880 --> 01:59:34,280
but, but I was better at doing that than I thought I'd be. And, and what followed was kind of

1212
01:59:34,280 --> 01:59:39,160
interesting and intriguing and, and, and increased my interest in the question as opposed to

1213
01:59:39,160 --> 01:59:46,920
decrease it, which is, I find that it's very difficult for me to be unhappy or angry

1214
01:59:47,800 --> 01:59:57,160
without words, that words are really critical to feeling victimized, to feeling like,

1215
01:59:57,960 --> 02:00:02,760
like it's not that I don't feel angry, but I feel very, very differently in the absence of words

1216
02:00:02,760 --> 02:00:09,240
and with words. And it's, and I can prostrate. So anyway, that's probably a crazy answer to what

1217
02:00:09,240 --> 02:00:17,560
you just asked, but, but that's, it's something I have a hunch that it might be important.

1218
02:00:19,240 --> 02:00:24,600
I have no clue about how to actually do it well. I'm sure there are probably gurus out there who

1219
02:00:24,600 --> 02:00:27,960
could maybe help me do it, but I've actually never really heard anyone else talk about it

1220
02:00:28,920 --> 02:00:34,760
explicitly in, in that way. But, you know, but so when I'm trying to go to sleep,

1221
02:00:34,760 --> 02:00:39,960
it's kind of thing where when I try to go to sleep at night, I spend maybe five minutes a day

1222
02:00:41,000 --> 02:00:49,000
playing around with this. And, and, and I, I think it's been good for me in a weird way.

1223
02:00:49,800 --> 02:00:56,840
Well, I think that last observation about how the, the reduction of nomenclature around a negative

1224
02:00:56,920 --> 02:01:01,240
emotion or an emotion, let's not that anger is a negative emotion, but a negatively valenced

1225
02:01:01,240 --> 02:01:06,680
emotion at least can reduce its impact. That, that alone is super interesting as an observation.

1226
02:01:07,960 --> 02:01:12,680
The other, the other thing I'd say is that strikes me as something that could be probably augmented

1227
02:01:13,400 --> 02:01:17,400
by medication, right? There, there could be certain plants or drugs or chemicals

1228
02:01:17,400 --> 02:01:24,280
that would probably really augment that state. So we'll leave it at that. That's interesting.

1229
02:01:24,280 --> 02:01:26,360
That's really interesting. Maybe you can help me with some of those.

1230
02:01:29,240 --> 02:01:34,760
You last thing I want to ask you, Steve, you've mentioned numerous times that the random

1231
02:01:35,480 --> 02:01:40,040
controlled experiment, you know, the RCT, the gold standard of medicine is a tool that is

1232
02:01:40,040 --> 02:01:46,200
virtually never available to the economist for obvious reasons. The scale would be enormous.

1233
02:01:46,920 --> 02:01:53,880
If you could put all of that aside for a moment, if you truly had billions of dollars

1234
02:01:54,520 --> 02:02:00,280
and all the time in the world, what experiment would you conduct?

1235
02:02:05,560 --> 02:02:14,920
So it's funny, you know, cause I don't actually have big ideas about economics. I think economics

1236
02:02:14,920 --> 02:02:20,440
isn't where I would go with something like that because in some fundamental way, I think we

1237
02:02:20,440 --> 02:02:28,760
understand what's understandable about the economy. I mean, it isn't, it's not a lack of

1238
02:02:28,760 --> 02:02:33,560
resources that makes it hard to understand the macro economy. It's the fact that we have exactly

1239
02:02:33,560 --> 02:02:38,280
one of them, right? So absent a set of parallel universes, I don't think we're ever going to find

1240
02:02:38,280 --> 02:02:42,760
out through data or experimentation. So look, if you give me a different choice, which is,

1241
02:02:42,760 --> 02:02:48,600
hey, if you had, if you could be God and create enormous number of parallel universes,

1242
02:02:48,600 --> 02:02:52,040
then I would have a bunch of interesting macro experiments that I might want to do.

1243
02:02:52,040 --> 02:02:59,000
I really think, so for me, when I think about big problems, they're much more off and like,

1244
02:02:59,000 --> 02:03:07,640
like I think that they're rarely about randomization. I really think the things I, I would,

1245
02:03:08,760 --> 02:03:12,440
but let me give you an example where maybe there's more you really talking about, but like,

1246
02:03:12,440 --> 02:03:21,160
I think, well, how we really blew it with COVID is that we didn't look, so what have we done? Well,

1247
02:03:21,960 --> 02:03:29,080
we used randomized trials to figure out that we have really good vaccines. Okay. But we didn't

1248
02:03:29,080 --> 02:03:34,440
even, I fought with Montsef Slawey about this, but look, I think we were idiots not to do challenge

1249
02:03:34,440 --> 02:03:40,920
trials where we actually went out and learned much more quickly by giving, you know, giving people

1250
02:03:40,920 --> 02:03:45,640
vaccines and giving them COVID and seeing if it works, you know, like I think we were foolish

1251
02:03:45,640 --> 02:03:54,440
given, given the, the stakes not to do that. But much more fundamentally, I think that we have

1252
02:03:54,440 --> 02:04:01,800
ruled out medical ethics and societal views have ruled out really sensible, like, like when we want

1253
02:04:01,800 --> 02:04:05,640
to know the answers, the best way is a randomized experiment. Okay. So there's so many, and you and

1254
02:04:05,640 --> 02:04:10,200
I have talked about a little bit on my pocket as well. There's so many cases around COVID where we

1255
02:04:10,200 --> 02:04:14,760
just don't know the basic facts about, like, I think you made a big point about the six feet

1256
02:04:14,760 --> 02:04:18,920
thing, like six feet away, like somebody just made that up and we live with it. Or another one that

1257
02:04:18,920 --> 02:04:24,680
just came up as there's a great op-ed piece that said, well, why is everyone still wearing terrible

1258
02:04:24,680 --> 02:04:30,200
masks? Because like at the beginning people, you know, like we're told, well, don't wear a mask

1259
02:04:30,200 --> 02:04:34,440
because a mask won't help you anyway. Which was obviously idiotic because we were really being

1260
02:04:34,440 --> 02:04:37,560
told that because they didn't want us buying up all the good masks because they wanted to give

1261
02:04:37,560 --> 02:04:41,000
them the frontline health professionals. Of course, if they didn't work, why are the frontline

1262
02:04:41,000 --> 02:04:47,880
health professionals wearing them? But I just think there's like the real missed opportunity in COVID

1263
02:04:47,880 --> 02:04:58,120
was not to take the approach that I think is like 100% accepted, which is the best way we are able to

1264
02:04:58,120 --> 02:05:03,320
learn given the constraints of society about COVID is by doing clever little natural experiments

1265
02:05:03,320 --> 02:05:07,960
where we look at some particular airplane and some particular person we know got COVID and then

1266
02:05:07,960 --> 02:05:12,600
we go in and do contact tracing to figure out how bad it was. Look, the best way to figure out whether

1267
02:05:12,600 --> 02:05:16,360
airplanes are dangerous is to put people on airplanes, some who have COVID and some don't,

1268
02:05:16,360 --> 02:05:21,560
and to figure out who gets it. I mean, and people are like, oh, you can't do that. But look, when

1269
02:05:21,560 --> 02:05:26,440
the stakes are high enough and you got, you know, especially with COVID, but this brings it back to

1270
02:05:26,440 --> 02:05:32,840
the field of economics, you would argue if you pay people enough as volunteers, there's going to be a

1271
02:05:32,840 --> 02:05:37,160
subset of people that are going to say, yeah, my risk of getting COVID and something really bad

1272
02:05:37,160 --> 02:05:41,480
happening is sufficiently low enough that I'm willing to be one of the healthy volunteers on

1273
02:05:41,480 --> 02:05:45,880
that airplane who risks getting COVID. And of course, we would argue, well, that might not be

1274
02:05:45,880 --> 02:05:50,520
medically ethical, but you're saying desperate times call for desperate measures. There's a way

1275
02:05:50,520 --> 02:05:56,600
to solve these problems economically. Yeah. And then, you know, Monteslawy who ran Operation

1276
02:05:57,240 --> 02:06:01,080
Warp Speed said, yeah, but that's no good because then you just have the healthy people and that

1277
02:06:01,080 --> 02:06:05,480
might not tell you anything. But look, there are plenty of people who are really, really sick

1278
02:06:05,480 --> 02:06:09,880
and who are worried about their, their dependence and what's going to happen to their family.

1279
02:06:09,880 --> 02:06:13,080
But there are plenty of people who know their life expectancy is four months,

1280
02:06:13,080 --> 02:06:17,880
who are going to be willing to get exposed to COVID if you say, well, take care of your family,

1281
02:06:17,880 --> 02:06:25,480
you know, if you die from. So, like, I think I have a, I know I'm completely out of step with a

1282
02:06:25,480 --> 02:06:32,760
mainstream medical ethics on this, but I think, I think medical ethics gets it really wrong

1283
02:06:32,760 --> 02:06:37,480
because when they think about money, they think about small amounts of money. And it's probably

1284
02:06:37,480 --> 02:06:41,160
wrong. Like if you, if you're willing to offer people a hundred dollars to do this, then the

1285
02:06:41,160 --> 02:06:45,400
only people are going to do it are going to be drug addicts and uneducated people who don't

1286
02:06:45,400 --> 02:06:51,080
understand. But look, if you, when the stakes are as high as COVID, you can offer people $10,000,

1287
02:06:51,080 --> 02:06:56,280
$100,000. I mean, you can offer enough money that everyone's going to be lining up to do it.

1288
02:06:56,280 --> 02:07:02,040
And if everybody's lining up, then I think medical ethics no longer has a real, a real

1289
02:07:02,040 --> 02:07:08,280
stake in this because, you know, it's just a different problem when everyone is volunteering

1290
02:07:08,280 --> 02:07:12,120
to do it. Volunteering because you're paying enough money, but then almost every issue of

1291
02:07:12,600 --> 02:07:16,600
medical ethics fades away when, when you got a surplus of people volunteering.

1292
02:07:17,400 --> 02:07:20,920
You know, I never thought about it that way until you said it, Steve, that is a fantastic

1293
02:07:21,000 --> 02:07:29,080
point. Institutional review boards, IRBs are adamant about making sure honorariums are very

1294
02:07:29,080 --> 02:07:34,600
small in studies to prevent coercion. That's their big stick is you can't overpay people.

1295
02:07:34,600 --> 02:07:41,000
Otherwise it's coercive. But the irony of it is, it is coercive if it's low and it disproportionately

1296
02:07:41,000 --> 02:07:47,480
targets the most vulnerable. Exactly. It's so true. And, and I, I mean, on organ donation,

1297
02:07:47,480 --> 02:07:51,640
this has been something I've harped on for a long time. People, people are really terrified

1298
02:07:51,640 --> 02:07:57,880
of the idea of a market for organs for like live donors and kidneys. And every horror story they

1299
02:07:57,880 --> 02:08:04,360
tell is a horror story that's embedded in the idea that there's going to, it's going to be exploited.

1300
02:08:04,360 --> 02:08:09,560
Look, and that's because the price, the price, the market price would be too low. This is actually

1301
02:08:09,560 --> 02:08:15,480
an odd case in which you want to enforce an actual price, which is way above the market price,

1302
02:08:15,480 --> 02:08:21,320
because the value to our meta, you know, better than me, the amount we spend on dialysis and the

1303
02:08:21,320 --> 02:08:26,760
amount of suffering associated with it. It's like some even like measurable percentage of GDP,

1304
02:08:26,760 --> 02:08:35,960
like one or 2% of GDP or some crazy thing like that. And, and the value of a really good,

1305
02:08:35,960 --> 02:08:41,720
healthy, live kidney to society is enormous. It's, you know, on the order of, I don't know,

1306
02:08:41,720 --> 02:08:48,040
you know, hundreds of thousands of dollars at least. So you could easily pay people a hundred

1307
02:08:48,040 --> 02:08:54,440
thousand dollars for a good kidney. And if you did that, I think you might have, I don't know,

1308
02:08:54,440 --> 02:08:59,240
might have 10 million people who would, 50 million people who would sign up to be kidney donors in

1309
02:08:59,240 --> 02:09:05,080
that world. And in a world in which, you know, Wall Street stockbrokers are signing up to donate

1310
02:09:05,080 --> 02:09:10,040
their kidneys, not donate, I mean, not altruistic, they just want a hundred grand. Look, in that

1311
02:09:10,040 --> 02:09:15,960
world, the whole medical ethics is turned upside down. And the weird thing is I cannot get anyone

1312
02:09:16,680 --> 02:09:21,720
to take that scenario seriously, even though to me, it seems completely and totally obvious that

1313
02:09:22,600 --> 02:09:29,240
if we could do it someplace, if we could get, you know, I don't know, you know, Guatemala or,

1314
02:09:29,880 --> 02:09:34,760
you know, or Singapore, Mozambique or someplace to just do that in one place, I bet it would work

1315
02:09:34,760 --> 02:09:42,840
great. And that one example could lead the rest of the world to follow. But we are so far from any

1316
02:09:42,840 --> 02:09:47,880
place being willing to try it. I mean, the closest is Iran, interestingly, ironically, Iran is one

1317
02:09:47,880 --> 02:09:53,800
place on the planet that's paid for organs, but not the way I would do it. Not that it's the most

1318
02:09:53,800 --> 02:09:59,800
important problem in the world. That is a fan, like along with training that horse, Silver, to win the

1319
02:10:00,360 --> 02:10:07,400
Kentucky Derby. That's another of my fantasies, is a world in which I create this market for

1320
02:10:07,400 --> 02:10:11,640
live donors. All right. So on that note, Steve, we've got three huge problems to follow up on.

1321
02:10:11,640 --> 02:10:19,400
We've got to, in order, figure out how to create a really good scratch five golfer in 12 months.

1322
02:10:20,440 --> 02:10:27,240
We've got to figure out how to take a B player horse as a one-year-old and get a triple crown

1323
02:10:27,240 --> 02:10:33,240
by age three. And we've got to figure out a market for organ donation with a really, really,

1324
02:10:33,240 --> 02:10:38,520
really high price premium paid for kidneys. Let me just add number four is we got to figure out how

1325
02:10:38,520 --> 02:10:43,560
to do this Manhattan project for climate change. That's the most important one. I'm going to

1326
02:10:44,520 --> 02:10:49,240
personally spend more time thinking about that one than the other three, but that's awesome. To

1327
02:10:49,240 --> 02:10:54,200
come out of this podcast with four great problems to work on is more than we bargained for.

1328
02:10:54,840 --> 02:10:59,160
My prediction, the only one we're going to solve is going to be that five handicap golfer. When you

1329
02:10:59,160 --> 02:11:03,560
get burned out and you get tired of doing everything else. The other three, I'm not too optimistic.

1330
02:11:05,240 --> 02:11:07,400
Steve, thanks so much for sitting down today. It was awesome.

1331
02:11:07,400 --> 02:11:09,720
Thank you, Pete. It's awesome. It's always so much fun to talk to.

1332
02:11:11,720 --> 02:11:15,800
Thank you for listening to this week's episode of The Drive. If you're interested in diving deeper

1333
02:11:15,800 --> 02:11:19,960
into any topics we discuss, we've created a membership program that allows us to bring

1334
02:11:19,960 --> 02:11:25,080
you more in-depth, exclusive content without relying on paid ads. It's our goal to ensure

1335
02:11:25,080 --> 02:11:30,360
members get back much more than the price of the subscription. Now to that end, membership benefits

1336
02:11:30,360 --> 02:11:35,720
include a bunch of things. One, totally kick ass comprehensive podcast show notes that detail

1337
02:11:35,720 --> 02:11:41,080
every topic, paper, person, thing we discuss on each episode. The word on the street is nobody's

1338
02:11:41,080 --> 02:11:46,920
show notes rival these. Monthly AMA episodes or ask me anything episodes, hearing these episodes

1339
02:11:46,920 --> 02:11:52,680
completely. Access to our private podcast feed that allows you to hear everything without having

1340
02:11:52,680 --> 02:11:58,040
to listen to spiel's like this. The Qualies, which are a super short podcast that we release

1341
02:11:58,040 --> 02:12:02,440
every Tuesday through Friday, highlighting the best questions, topics, and tactics discussed on

1342
02:12:02,440 --> 02:12:07,720
previous episodes of The Drive. This is a great way to catch up on previous episodes without having

1343
02:12:07,720 --> 02:12:13,320
to go back and necessarily listen to everyone. Steep discounts on products that I believe in,

1344
02:12:13,320 --> 02:12:17,880
but for which I'm not getting paid to endorse and a whole bunch of other benefits that we continue

1345
02:12:17,880 --> 02:12:22,520
to trickle in as time goes on. If you want to learn more and access these member only benefits,

1346
02:12:22,520 --> 02:12:28,760
you can head over to PeterAttiaMD.com forward slash subscribe. You can find me on Twitter,

1347
02:12:28,760 --> 02:12:34,840
Instagram, and Facebook, all with the ID PeterAttiaMD. You can also leave us a review on Apple

1348
02:12:34,840 --> 02:12:40,600
Podcast or whatever podcast player you listen on. This podcast is for general informational

1349
02:12:40,600 --> 02:12:45,000
purposes only and does not constitute the practice of medicine, nursing, or other professional

1350
02:12:45,000 --> 02:12:50,920
healthcare services, including the giving of medical advice. No doctor patient relationship

1351
02:12:50,920 --> 02:12:56,360
is formed. The use of this information and the materials linked to this podcast is at the user's

1352
02:12:56,360 --> 02:13:01,560
own risk. The content on this podcast is not intended to be a substitute for professional

1353
02:13:01,560 --> 02:13:08,440
medical advice, diagnosis, or treatment. Users should not disregard or delay in obtaining medical

1354
02:13:08,440 --> 02:13:13,080
advice from any medical condition they have, and they should seek the assistance of their

1355
02:13:13,080 --> 02:13:18,840
healthcare professionals for any such conditions. Finally, I take conflicts of interest very

1356
02:13:18,840 --> 02:13:24,440
seriously. For all of my disclosures and the companies I invest in or advise, please visit

1357
02:13:24,440 --> 02:13:35,000
PeterAttiaMD.com forward slash about where I keep an up-to-date and active list of such companies.

